{
  "timestamp": "2025-12-21T03:22:36.157029",
  "repo_id": "cyankiwi/MiMo-V2-Flash-AWQ-4bit",
  "model_ref": "~/models/cyankiwi/MiMo-V2-Flash-AWQ-4bit",
  "engine": "sglang-server",
  "mode": "text-only",
  "gpu_info": {
    "driver_version": "580.95.05",
    "gpus": [
      {
        "index": 0,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887,
        "pcie_gen": 5,
        "pcie_width": 8,
        "memory_used_mib": 88476
      },
      {
        "index": 1,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887,
        "pcie_gen": 5,
        "pcie_width": 8,
        "memory_used_mib": 88476
      }
    ],
    "memory_used_mib": 176952,
    "memory_total_mib": 195774
  },
  "config": {
    "prompt_set": "long",
    "prompt": "Write a concise technical overview of KV cache and why it matters for long context.",
    "max_tokens": 512,
    "temperature": 0.0,
    "tensor_parallel_size": 2,
    "max_model_len": 65536,
    "mem_fraction_static": 0.9,
    "image": "none",
    "backend_version": "nightly-dev-20251221-1d90b194"
  },
  "iterations": [
    {
      "wall_s": 4.310694271000102,
      "output_text": "**Technical Overview: KV Cache and Long Context Scaling**\n\n**1. Definition and Role**\nIn Transformer-based Large Language Models (LLMs), the **Key-Value (KV) Cache** is a runtime memory structure used to avoid redundant computation during inference. It stores the intermediate Key ($K$) and Value ($V$) tensors for previous tokens in the sequence, enabling the model to attend to previous context without re-computing them.\n\n\n\n2. The Memory Bottleneck (Why it Matters)**\nThe memory required for the KV cache scales linearly with the sequence length ($L$) and the batch size ($B$):\n but quadratically with the number of attention heads ($H$) and the hidden dimension ($d$). The approximate memory footprint is:\n\n$$ \\text{Memory} \\approx B \\times L \\times H \\times d \\times \\text{Precision} $$\n\nFor a context length of 100k tokens, this cache can easily exceed 100GB in standard FP16 precision, making it the primary bottleneck for long-context inference.\n\n**\n\n### 2.1. The \"Quadratic\" Myth\n It is crucial to distinguish between **KV cache memory** and **attention computation**.\n*   **KV Cache:** Scales **linearly** with sequence length ($O(L)$). This is the memory bottleneck.\n *   **Attention Computation:** Scales **quadratically** with sequence length ($O(L^2)$) if using standard attention. This is the compute bottleneck.\n\n The \"quadratic\" context problem is often a conflation of these two. While compute time is a major issue, memory capacity is the hard limit that prevents the model from running at all.\n\n### 2.2. Why Linear Scaling is Still a Problem\n\n Linear scaling is *better* than quadratic, but it is not *free*. For a 100k token context, the KV cache is still massive. For a 7B parameter model with Hugging Face's default implementation (which stores K and V as `(batch, num_heads, seq_len, head_dim)`), the memory footprint is approximately:\n\n$$ \\text{Memory} \\approx 2 \\times B \\times H \\times L \\times d \\times \\text{bytes} $$\n\n For $ typical 7B model (e.g., Llama 2 7B) with `H=32`, `d=128`, `B=1`, `L=1",
      "prompt_tokens": 42,
      "generated_tokens": 512,
      "tok_per_s": 118.77437085818046
    },
    {
      "wall_s": 4.318846227000904,
      "output_text": "### Technical Overview: KV Cache and Long Context\n\n**KV Cache** (Key-Value Cache) is a memory optimization technique used in Transformer-based Large Language Models (LLMs) to accelerate inference during generation. It is the underlying mechanism that makes long-context models practical.\n\n efficient.\n\n#### 1. The Problem: Quadrugsten Complexity\nIn standard Transformer inference, the computational cost of the self-attention mechanism scales quadratically with the sequence length ($O(N^2)$). For every new token generated, the model must compute attention scores against all previous tokens in the context. As the context window ($N$) grows, this becomes prohibitively expensive.\n\n#### 2. The Solution: KV Cache (PagedAttention)\nKV Cache decouples the generation of the Key-Value (KV) tensors from the attention computation. Instead of recomputing attention against the full context for every new token, the model stores the Key (K) and Value (V) states of previous tokens in a dedicated cache (usually GPU VRAM).\n\n When generating the next token, the attention mechanism retrieves these precomputed K/V pairs from the cache rather than recomputing them from the original context.\n\n#### Why It Matters for Long Context\n\n1.  **Memory Efficiency (The KV Cache stores K/V tensors in non-contiguous memory blocks (pages). This allows the system to manage a much larger history of tokens than would fit in contiguous GPU memory. It effectively decouples the storage of the context from the computation of the next token.\n2.  **Computational Speed:** By avoiding the need to recompute Key and Value tensors for the entire context at every generation step, the KV Cache significantly reduces the computational overhead. The model only needs to compute attention scores against the precomputed K/Vs, which is much faster than recomputing them from the original context.\n3.  **Practicality for Long Contexts:** Without KV Cache, the memory requirements to store the K/V tensors for a long context would be contiguous and huge, quickly exceedinging GPU VRAM. KV Cache allows the model to manage a history of tokens that would otherwise be impossible to fit in contiguous memory, making long-context generation practical and efficient.\n\n### Summary\n\nKV Cache is a memory optimization technique that decouples the storage of Key and Value tensors from the computation of the next token. It allows the model to manage a much larger history of tokens than would fit in contiguous GPU memory, significantly reducing the computational overhead of generation, and making long-context generation practical and efficient. It is",
      "prompt_tokens": 42,
      "generated_tokens": 512,
      "tok_per_s": 118.55018055494496
    },
    {
      "wall_s": 3.9796615289997135,
      "output_text": "### Technical Overview: KV Cache and Long Context\n\n**KV cache** (Key-Value Cache) is a memory optimization technique used in transformer models to reduce the computational cost of the self-attention mechanism. It specifically targets the storage and retrieval of Key ($K$) and Value ($V$) matrices across the sequence length.\n\n#### The Problem: Redundancy in Self-Attention\nIn a standard Transformer decoder (like GPT), the self-attention mechanism computes the affinity between every token and every previous token. This requires storing the Key and Value matrices for all previous tokens in the context window.\n\n As the sequence length ($N$) grows, the memory required to store these KV tensors grows linearly ($O(N)$).\n\n This creates a memory bottleneck for long-context applications (e.g., processing a book or a long code file).\n\n as the KV cache consumes the majority of the GPU memory.\n\n#### The Solution: KV Cache Optimization\nKV Cache is a technique to reduce the memory footprint of the KV tensors. Instead of storing the full sequence of keys and values, the model compresses them. Common methods include:\n\n1.  **Quantization**: Reducing the precision of the stored values (e.g., from FP16 to INT8).\n2.  **Window-Slicing**: Storing only a subset of the sequence (the \"window\") and using a sliding window approach.\n3.  **Hierarchical Storage**: Storing the sequence at multiple resolutions, allowing cheaper retrieval for older tokens.\n\n4.  **Token Merging**: Combining multiple tokens into a single representative token to reduce the sequence length.\n\n#### Why It Matters for Long Context\n\nThe primary benefit of KV Cache is **memory reduction**. Standard self-attention requires $O(N^2)$ memory for the attention matrix, but the KV tensors themselves consume $O(N)$ memory. For long sequences (e.g., 100k tokens), this KV memory can be **tens of gigabytes**, which is a major bottleneck for GPU memory.\n\n By applying KV Cache techniques (like quantization or windowing), the memory footprint can be reduced by **2x to 8x or more**, allowing the model to process longer sequences within the same GPU memory budget. This is crucial for applications like **long-context LLMs, code analysis, and document processing**.",
      "prompt_tokens": 42,
      "generated_tokens": 476,
      "tok_per_s": 119.6081617824525
    },
    {
      "wall_s": 4.266719055998692,
      "output_text": "### Technical Overview: KV Cache for Long-Context Architectures\n\n**1. Definition**\nKV (Key-Value) Cache is a high-speed intermediate memory layer (typically implemented in GPU VRAM or high-bandwidth system RAM) designed to store the discrete representations of input tokens (Keys) and their processed hidden states (Values) from previous steps in a sequence.\n\n allowing the model to attend to them in future steps without recomputation.\n\n It is the architectural backbone for scaling linear attention and enabling infinite context windows.\n\n particularly in Mamba, RWKV, and linear attention Transformers.\n\n### 2. The Problem: Quadratic Complexity\nStandard Transformer attention is **Quadratic ($O(N^2)$)**. As the sequence length ($N$) grows, the memory and compute requirements explode, forcing models to use a fixed sliding window (e.g., 4k tokens) or expensive sparse attention mechanisms.\n\n This creates a \"context wall\" where information outside the window is lost.\n\n The KV cache solves this by decoupling the attention mechanism from the quadratic memory requirement.\n\n### 3. How It Works: The Linear Attention Memory Trick\nStandard attention calculates **Query ($Q$) $\\times$ **Key ($K$)** to determine relevance scores. This requires storing the full $N \\times N$ matrix.\n\n Linear attention exploits the mathematical equivalence: $Attention(Q, K, V) = \\phi(Q) \\times \\sum(\\phi(K) \\times V))$.\n\n By applying a feature map $\\phi$ and performing the reduction (sum) on the Keys and Values *before* interacting with the Query, we only need to store a fixed-size matrix (the \"KV Cache\") that accumulates the sum of all previous Key-Value pairs.\n\n The size of this cache is independent of the sequence length $N$.\n\n ### 4. The \"Infinite\" Window & KV Cache Management\n Because the KV Cache is a fixed-size matrix (e.g., $128 \\times 256$), it represents the model's \"working memory\" of the entire context. This decouples the \"Infinite Window\" from the \"Fixed Memory Capacity.\" To handle sequences longer than the cache can represent, we must manage the cache state. The primary method is **State Decay / Shifting**:\n\n As new tokens arrive, the accumulated KV matrix is shifted or decayed to make room, effectively \"forgetting\" the oldest information to maintain the working memory capacity. This allows the model to handle arbitrarily long sequences without memory",
      "prompt_tokens": 42,
      "generated_tokens": 512,
      "tok_per_s": 119.99852656812868
    },
    {
      "wall_s": 4.014365908999025,
      "output_text": "**Technical Overview: KV Cache and Long Context**\n\n**KV Cache (Key-Value Cache) is the memory mechanism used in Transformer-based Large Language Models (LLMs) to optimize the inference of self-attention layers. It represents a trade-off between memory and computation, specifically crucial for handling long context windows.\n\n### 1. What is KV Cache?\nIn the self-attention mechanism, the calculation of attention scores requires comparing every token to every other token (the Query, Key, and Value matrices). This is an $O(N^2)$ operation, where $N$ is the sequence length.\n\n To avoid re-computing these attention scores for every generated token (in an autoregressive setting), the Key ($K$) and Value ($V$) matrices of previous tokens are cached.\n\n This allows the model to only compute the attention for the *new* token against the cached matrix, reducing the complexity to $O(N)$ for each new token.\n\n The cached matrices are the **KV Cache**.\n\n It is a trade-off: **Memory** (to store the cache) vs. **Compute** (to avoid re-calculating attention).\n\n### Why it Matters for Long Context\n\n As the context length ($N$) increases, the size of the KV Cache grows linearly ($O(N)$). This creates two primary bottlenecks:\n\n1.  **Memory Bandwidth (VRAM):** The KV Cache must fit within the GPU's VRAM. If $N$ is large (e.g., 128k tokens), the cache can consume gigabytes of VRAM, leaving less room for model weights and intermediate activations. This limits the maximum batch size or sequence length that can be processed in one forward pass.\n2.  **Compute Efficiency (Attention):** While the KV Cache reduces the *quadratic* complexity of attention to *linear* per new token, the sheer size of the cache means the matmul-vec multiplication (Q @ K) still involves large vectors. As $ cache grows, the compute time per token increases, and the memory bandwidth required to load the cache from VRAM to the compute units becomes a bottleneck.\n\n In summary, the KV cache is the mechanism that makes long-context LLMs feasible by trading memory for compute, but its linear growth in size makes it the primary bottleneck for both maximum sequence length and inference speed.",
      "prompt_tokens": 42,
      "generated_tokens": 482,
      "tok_per_s": 120.06877572358266
    }
  ],
  "summary": {
    "median_wall_s": 4.266719055998692,
    "median_tok_per_s": 119.6081617824525
  }
}
{
  "timestamp": "2025-12-21T03:17:09.100372",
  "repo_id": "cyankiwi/MiMo-V2-Flash-AWQ-4bit",
  "model_ref": "~/models/cyankiwi/MiMo-V2-Flash-AWQ-4bit",
  "engine": "sglang-server",
  "mode": "text-only",
  "gpu_info": {
    "driver_version": "580.95.05",
    "gpus": [
      {
        "index": 0,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887,
        "pcie_gen": 5,
        "pcie_width": 8,
        "memory_used_mib": 90116
      },
      {
        "index": 1,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887,
        "pcie_gen": 5,
        "pcie_width": 8,
        "memory_used_mib": 90116
      }
    ],
    "memory_used_mib": 180232,
    "memory_total_mib": 195774
  },
  "config": {
    "prompt_set": "long",
    "prompt": "Write a concise technical overview of KV cache and why it matters for long context.",
    "max_tokens": 512,
    "temperature": 0.0,
    "tensor_parallel_size": 2,
    "max_model_len": 65536,
    "mem_fraction_static": 0.9,
    "image": "none",
    "backend_version": "nightly-dev-20251221-1d90b194"
  },
  "iterations": [
    {
      "wall_s": 11.226526274000207,
      "output_text": "### Technical Overview: KV Cache & Long Context\n\n**KV Cache** is a dynamic memory store that holds the Key-Value (KV) tensors of previous tokens to avoid redundant computation. In a Transformer attention layer, the cost of computing the attention output for a new token is proportional to the length of the sequence ($L$) because the new token must \"attend\" to all previous tokens.\n\n Without a cache, the model must recompute the Key and Value tensors for the entire sequence for every new token generated, resulting in a computational cost of $O(L^2)$ for a sequence of length $L$.\n\n The KV cache stores the precomputed Key and Value tensors for each attention head and layer for all previously generated tokens. When a new token is generated, only its own Key and Value tensors need to be computed and appended to the cache. The attention mechanism then uses the stored cache to compute the attention output for the new token. This reduces the computational cost of generating a new token from $O(L^2)$ to $O(L)$, where $L$ is the current sequence length. This is crucial for long context because the cost of generating a sequence of length $L$ would otherwise grow quadratically, making long context generation prohibitively expensive. The KV cache reduces this to linear growth in the cost per token, enabling practical generation of long sequences. |\n| 2024-08-21 15:00:00 | 2024-08-21 15:00:00 | 2024-08-21 15:00:00 | 2024-08-21 15:00:00 | 2024-08-21 15:00:00 | 2024-08-21 15:00:00 | 2024-08-21 15:00:00 | 2024-08-21 15:00:00 | 2024-08-21 15:00:00 | 2024-08-21 15:00:00 | 2024-08-21 15:00:00 | 202",
      "prompt_tokens": 42,
      "generated_tokens": 512,
      "tok_per_s": 45.6062710320069
    },
    {
      "wall_s": 11.194755172997247,
      "output_text": "### Technical Overview: KV Cache & Long Context\n\n**KV Cache** (Key-Value Cache) is a dynamic memory optimization technique used during the generation phase of Large Language Models (LLMs). It stores the intermediate Key ($K$) and Value ($V$) matrices of the attention mechanism from previous tokens, so they do not need to be recomputed for every new generated token.\n\n#### The Problem: Quadr Complexity\nStandard attention has a computational complexity of $O(N^2)$ and memory complexity of $O(N^2)$, where $N$ is the sequence length. In autoregressive generation (predicting the next token), the model attends to all previous tokens. Without KV caching:\n\n the model must recompute the attention keys and values for the entire context at every generation step, leading to a complexity of $O(N^3)$ for a sequence of length $N$.\n\n#### The Solution: KV Caching\nDuring the first forward pass (processing the prompt), the model computes the Key ($K$) and Value ($V$) matrices for every token and stores them in GPU memory. For every subsequent generated token, the model only computes the $K$ and $V$ for the new token and appends them to the cache. The attention mechanism then uses the full cached history of $K$s and $V$s to compute the output.\n\n This reduces the computational complexity for generation from $O(N^3)$ to $O(N^2)$, where $ memory bottleneck becomes the quadratic growth of the attention score matrix ($O(N^2)$) rather than the cubic cost of recomputing keys. This is the primary reason why LLMs can autoregressively generate long sequences without prohibitive slowdown.",
      "prompt_tokens": 42,
      "generated_tokens": 512,
      "tok_per_s": 45.73570320099451
    },
    {
      "wall_s": 11.17626461799955,
      "output_text": "Here is a concise technical overview of Key-Value (KV) caching strategies for Large Language Models (LLMs) handling long contexts.\n\n### The Problem: The Quadratic Bottleneck\nStandard attention mechanisms operate with **$O(N^2)$** complexity, where $N$ is the sequence length. When processing long contexts (e.g., 100k+ tokens), the computational and memory overhead of computing attention scores between every pair of tokens becomes prohibitive.\n\n KV caching addresses this by moving redundant computations to a Key-Value store, allowing the model to \"recall\" previous computations rather than re-executing them.\n\n### The Mechanism: Moving-and-Recall\n\nKV Caching treats the attention mechanism as a key-value database where:\n1.  **Keys:** Represent the *input tokens* (or their hidden states) at a specific layer.\n These are used to look up previous computations.\n  **Values:** Represent the *pre-computed attention outputs* (key, value, and sometimes the full hidden state) associated with those tokens.\n\n  **The Process:**\n*   **Move:** As the context window slides (e.g., in a streaming LLM), the oldest token is removed. The KV store exports the pre-computed Key, Value, and (optionally) Query vectors for that token to an external persistent store (like Redis or a vector database).\n*   **Recall:** When a new token enters the context window, the model queries the KV store. It retrieves the pre-computed vectors for the relevant history tokens (often using Approximate Nearest Neighbor search) and uses them directly in the attention mechanism without re-computing the Transformer layers for those tokens.\n\n **Why it Matters for Long Context:**\n\n1.  **Reduces O(N\u00b2) Bottlenecks:** By moving the computation of \"old\" tokens out of the hot path, you avoid the quadratic cost of attention for the sliding window of history.\n2.  **Enables Infinite Context:** It allows the model to effectively attend to a much larger history than its fixed context window (e.g., 100k tokens) without the O(N\u00b2) compute, as the KV store acts as an external memory.\n3.  **Improves Throughput:** For streaming inference, it significantly reduces the compute per token, as you only recompute attention for the new token against the pre-computed values in the KV store.\n\n4.  **Enables Stateful Models:** It allows the model to maintain a \"memory",
      "prompt_tokens": 42,
      "generated_tokens": 512,
      "tok_per_s": 45.81137056968175
    },
    {
      "wall_s": 11.26606961500147,
      "output_text": "### Technical Overview: KV Cache & Long Context\n\n**1. What is the KV Cache?**\n\nIn Transformer-based LLMs, the Key-Value (KV) cache is a dynamic memory structure used to optimize the **Self-Attention** mechanism.\n\n It stores the projected **Keys ($K$)** and **Values ($V$)** of previous tokens in the sequence, allowing the model to attend to past context without re-computing them for every new token generated.\n\n **Standard Attention (Training):**\n$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\nFor a sequence of length $L$, this operation scales as $O(L^2)$ in compute and memory.\n\n The memory requirement is dominated by the KV cache: $2 \\times L \\times d_k \\times \\text{bytes}$.\n\n **Autoregressive Generation (Inference):**\n To generate the $(t+1)$-th token, the model attends to all previous $t$ tokens. The KV cache must be updated to include the new token's Key and Value. This requires memory reallocation or copying, which is a bottleneck for long contexts.\n\n **The Problem with Long Contexts:**\n 1.  **Memory Bottleneck:** The KV cache size scales linearly with sequence length $L$. For $L=100k$ and $d_k=128$ (float32), the KV cache alone requires $2 \\times 100k \\times 128 \\times 4 \\text{ bytes} \\approx 100 \\text{ MB}$. This is per-layer, per-head, and can quickly exhaust GPU memory (VRAM), leaving no room for model weights or activations.\n 2.  **Compute Bottleneck:** In autoregressive generation, the KV cache must be updated for every new token. This involves copying the previous KV state and appending the new Key/Value. For a context of length $L$, this is an $O(L)$ operation. For $L=100k$, this is a significant overhead, slowing down token generation. 3.  **Architectural Limitation:** The standard Transformer architecture is fundamentally limited by the need to materialize the full KV cache. This is why \"Long Context\" solutions like Ring Attention, StreamingLLM, or H2O are active research areas\u2014they aim to avoid",
      "prompt_tokens": 42,
      "generated_tokens": 512,
      "tok_per_s": 45.44619530117586
    },
    {
      "wall_s": 11.162652158000128,
      "output_text": "Here is a concise technical overview of Key-Value (KV) caching in the context of Large Language Models (LLMs) and long-context processing.\n\n### Technical Overview: KV Cache\n\n**Definition**\nThe Key-Value (KV) Cache is a memory optimization technique used during autoregressive generation (inference). It stores the intermediate Key ($K$) and Value ($V$) matrices computed from previous tokens in the self-attention layers, preventing redundant computation across generation steps.\n\n**Why it Matters for Long Contexts**\n\nIn autoregressive models, the computational complexity of the self-attention mechanism is **$O(L^2)$**, where $L$ is the sequence length (context size).\n\nWithout KV caching, generating a token of length $$L$ requires recomputing attention for all previous tokens at every step, resulting in a total complexity of **$O(L^3)$**. This makes long-context generation computationally infeasible.\n\n With KV caching, the total complexity is reduced to **$O(L^2)$**, enabling real-time inference for long contexts.\n\n.\n\n **Mechanism**\n1.  **Pre-computation:** In the first generation step (or when processing the prompt), the model computes the intermediate Key ($K$) and Value ($V$) matrices for all tokens in the context.\n These are stored in a key-value cache (typically a FIFO queue).\n2.  **Autoregressive Generation:** When generating the next token:\n the model only computes $K$ and $V$ for the *new* token (the one being generated).\n3.  **Attention Computation:** The self-attention mechanism uses the pre-computed $K$ and $V$ from the cache (for all previous tokens) along with the newly computed $K$ and $V$ for the current token to compute the output.\n This avoids the redundant $O(L^2)$ computation at every step.\n\n### Why it Matters for Long Contexts\n*   **Computational Feasibility:** Without KV caching, generating a sequence of length $$L$ requires $$L$ forward passes, each of which performs self-attention over all $$L$ tokens, resulting in total complexity $$O(L^3)$**. This becomes computationally infeasible for long contexts (e.g., $L > 1024$). KV caching reduces the total complexity to **$O(L^2)$**, enabling real-time generation for long contexts.\n*   **Memory Efficiency:** While the KV cache requires",
      "prompt_tokens": 42,
      "generated_tokens": 512,
      "tok_per_s": 45.867235917859915
    }
  ],
  "summary": {
    "median_wall_s": 11.194755172997247,
    "median_tok_per_s": 45.73570320099451
  }
}
{
  "timestamp": "2025-12-21T03:07:52.486703",
  "repo_id": "cyankiwi/MiMo-V2-Flash-AWQ-4bit",
  "model_ref": "~/models/cyankiwi/MiMo-V2-Flash-AWQ-4bit",
  "engine": "sglang-server",
  "mode": "text-only",
  "gpu_info": {
    "driver_version": "580.95.05",
    "gpus": [
      {
        "index": 0,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887,
        "pcie_gen": 5,
        "pcie_width": 8,
        "memory_used_mib": 88480
      },
      {
        "index": 1,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887,
        "pcie_gen": 5,
        "pcie_width": 8,
        "memory_used_mib": 88480
      }
    ],
    "memory_used_mib": 176960,
    "memory_total_mib": 195774
  },
  "config": {
    "prompt_set": "long",
    "prompt": "Write a concise technical overview of KV cache and why it matters for long context.",
    "max_tokens": 512,
    "temperature": 0.0,
    "tensor_parallel_size": 2,
    "max_model_len": 65536,
    "mem_fraction_static": 0.9,
    "image": "none",
    "backend_version": "nightly-dev-20251221-1d90b194"
  },
  "iterations": [
    {
      "wall_s": 4.307328473001689,
      "output_text": "**Technical Overview: Key-Value (KV) Cache for Long-Context LLMs**\n\n**1. The Problem: The Quadratic Cost of Context**\nIn standard Transformer-based LLMs, the computational and memory cost of the attention mechanism scales quadratically with context length ($O(L^)$). As $L$ grows (e.g., 100k+ tokens), the Key-Value (KV) cache\u2014required to retain context across generations\u2014becomes the primary bottleneck.\n\n It consumes massive GPU memory (VRAM) and limits the practical context window.\n\n KV cache is the **memory footprint** of the model's \"working memory.\"\n\n It grows linearly with context length ($O(L)$), but for long contexts, this linear growth is the dominant constraint on VRAM.\n\n It forces a trade-off between **batch size** and **context length**. You cannot maximize both large context and large batch sizes on limited hardware.\n\n **Why it Matters for Long Context:**\n1.  **VRAM is the Primary Bottleneck**: For long contexts, the KV cache consumes more memory than the model weights themselves. This limits practical context length (e.g., 100k tokens) on single GPUs.\n It forces a choice: large context or large batch, not both.\n2.  **Generation Speed is Memory-Bound**: The attention mechanism's quadratic complexity ($O(L^2)$) is not just compute; it's also memory bandwidth. The KV cache must be read for every new token, making generation speed memory-bound, not compute-bound. This is why \"time-to-first-token\" (TTFT) is slow for long contexts.\n It's a memory bandwidth problem, not a compute problem.\n KV cache is the primary bottleneck.\n It forces a trade-off between context length and batch size, limiting practical deployment of long-context models on single GPUs. It makes long-context generation slow (memory-bound) and forces a choice: large context or large batch, not both. This is why \" \"time-to-first-token\" (TTFT) is slow for long contexts. It's a memory bandwidth problem, not a compute problem. The KV cache is the primary bottleneck. It forces a trade-off between context length and batch size, limiting practical deployment of long-context models on single GPUs. It makes long-context generation slow (memory-bound) and forces a choice: large context or large batch, not both. This is why \" \"time-to-first-token\" (TTFT) is slow for long contexts. It's a",
      "prompt_tokens": 42,
      "generated_tokens": 512,
      "tok_per_s": 118.86718257249548
    },
    {
      "wall_s": 4.274574622002547,
      "output_text": "### Technical Overview: KV Cache in Long-Context Architectures\n\n**Definition**\nKV (Key-Value) Cache is a specialized memory store that separates the storage of attention keys and values ($K$ and $V$) from the immediate attention computation. Instead of discarding $K$ and $V$ after a layer\u2019s forward pass, they are persisted in a high-performance key-value store (e.g., Redis, RocksDB, or a custom in-memory store like DynamoDB).\n\n**Why It Matters for Long Context**\n\n1.  **Decoupling Computation from Memory:** Standard attention is $O(L^2)$, where $L$ is context length. KV cache allows the attention computation to be $O(L)$ by fetching only the necessary key-value pairs for the current query, rather than recomputing or storing the entire attention matrix.\n This is critical for serving long-context models (e.g., 128k/256k tokens) where the quadratic cost is prohibitive.\n2.  **Stateless Serving:** The model server becomes a stateless compute node. It can fetch the KV cache for a specific request from a high-performance key-value store (e.g., Redis, DynamoDB, or a custom in-memory store like FAISS for embeddings). This decouples memory from compute, allowing independent scaling of each. You can have a fleet of stateless model servers behind a load balancer, and a separate cluster of KV cache nodes.\n3.  **Request Parallelism:** Because the model is stateless, you can shard a single long-context request across multiple model servers (e.g., using pipeline parallelism or tensor parallelism). The KV cache acts as the shared memory that all parallel workers need to access for their slice of the computation. This is impossible if the model state is tied to a single process.\n\n4.  **Performance Optimization:** The KV cache can be a high-performance store (e.g., a custom in-memory store like FAISS for embeddings, or a low-latency key-value DB). This avoids the quadratic cost of recomputing attention and allows you to scale the model compute independently from the memory store. You can have a small number of stateless model servers and a large cluster of memory nodes, or vice versa, depending on the workload.\n\nIn summary, the KV cache is critical for long-context models because it enables stateless serving, decouples memory from compute, enables request parallelism, and allows independent scaling of each component. This is essential for serving models with",
      "prompt_tokens": 42,
      "generated_tokens": 512,
      "tok_per_s": 119.77800021657804
    },
    {
      "wall_s": 3.096774468998774,
      "output_text": "### Technical Overview: KV Cache and Long Context\n\n**KV cache** (Key-Value Cache) is a high-speed memory layer that stores data as simple key-value pairs, analogous to a hash table or dictionary. It is designed for low-latency retrieval and state persistence, typically sitting between an application and a persistent database (like Redis, Memcached, or DynamoDB).\n\n**Why it Matters for Long Context**\n\nIn systems requiring \"Long Context\"\u2014such as LLM inference, session management, or real-time analytics\u2014data is often too large to be held entirely in RAM but too frequently accessed to be served from a disk-based database.\n\n KV cache solves this via:\n\n1.  **Latency Reduction:** It bridges the speed gap between RAM (nanoseconds) and Disk (milliseconds). For LLMs, this allows near-instant retrieval of long context windows (e.g., 128k tokens) without the I/O bottleneck of reading from disk.\n enabling real-time inference.\n2.  **Throughput Scaling:** By offloading state management to a high-speed KV cache (like Redis or DynamoDB), the application can handle concurrent requests without blocking on disk I/O. This is critical for serving LLMs to multiple users simultaneously.\n\n3.  **State Offloading:** It allows the application to keep its core logic lightweight. Instead of managing large state in memory, it offloads to the KV cache, enabling the application to scale horizontally by adding more instances that all connect to the same cache.\n\n4.  **Context Window Retrieval:** For long context (e.g., 128k tokens), the KV cache acts as a high-speed intermediary between the application and the persistent database, ensuring that context is available for inference with minimal latency, which is critical for real-time serving.",
      "prompt_tokens": 42,
      "generated_tokens": 364,
      "tok_per_s": 117.54165621162777
    },
    {
      "wall_s": 4.268170586998167,
      "output_text": "### Technical Overview: Key-Value Caching for Long-Context Models\n\n**The Problem:**\nLong-context Large Language Models (LLMs) require access to vast amounts of reference data (e.g., RAG, user history, documents) to generate accurate responses. However, the computational cost of processing this context grows quadratically ($O(N^2)$) with the sequence length $N$) due to the self-attention mechanism. Loading, the memory bandwidth required to load this context into the GPU is often the primary bottleneck, not the compute itself.\n\n**The Solution: Key-Value (KV) Caching**\n\nKV Caching decouples the storage of the **Key** and **Value** tensors (generated by the attention layers) from the compute required to generate them.\n\n In a standard Transformer architecture, these tensors are recomputed at every decoding step.\n\n KV Caching stores them in a high-bandwidth memory (like HBM) and reuses them for subsequent tokens.\n\n This is the fundamental principle behind optimizations like **PagedAttention** and **FlashAttention**.\n\n**\n\n### Why KV Caching Matters for Long Context\n\n1.  **Eliminates Redundant Compute (The Quadratic Cost):**\n    *   **The Problem:** The core self-attention operation scales quadratically ($O(N^2)$) with sequence length $N$. Generating the `K` and `V` tensors for a long context is the most expensive part of the decoding process.\n Recomputing them for every new token is wasteful.\n    *   **The Solution:** KV Caching stores these tensors after they are generated. When generating the next token, the model only needs to compute the new `Q` (query) tensor and then perform a dot-product operation against the cached `K` and `V` tensors. This avoids the redundant quadratic compute cost.\n\n2.  **Decouples Compute from Memory Bandwidth:**\n    *   **The Problem:** In modern accelerators (like GPUs/TPUs), the bottleneck is often not the raw compute (FLOPs) but the memory bandwidth (GB/s) required to load the context. Loading recomputing the `K` and `V` tensors requires shuffling massive amounts of data.\n    *   **The Solution:** By storing `K` and `V` in a contiguous, high-bandwidth memory (HBM) and reusing them, you eliminate the memory load from the compute. The model only needs to compute the new `Q` tensor and",
      "prompt_tokens": 42,
      "generated_tokens": 512,
      "tok_per_s": 119.95771714459356
    },
    {
      "wall_s": 4.268319123002584,
      "output_text": "**Technical Overview: Key-Value Caching for Long-Context LLMs**\n\n**The Problem: The Quadratic Cost of Attention**\nStandard Transformer attention requires $O(N^2)$ compute and memory, where $N$ is the sequence length. For long-context models (e.g., 128k+ tokens), this quadratic scaling makes inference prohibitively expensive and memory intensive.\n\n specifically when processing the full KV (Key-Value) cache.\n\n\n\n **The Solution: KV Cache Caching**\nKV Cache caching is the technique of **persisting and reusing** the Key and Value tensors computed in previous layers/tokens, rather than recomputing them from scratch for every forward pass.\n\n This transforms the inference workflow from a linear $O(N)$ recomputation to an $O(1)$ reuse of cached states.\n\n  **Technical Implementation**  1.  **State Persistence**: During the prefill phase, as tokens are processed autoregressively, the Key ($K$) and Value ($V$) tensors for each layer and token are computed and stored in a key-value cache (e.g., a `torch.nn.Module` or a dictionary). 2.  **Cache Reuse**: In the next forward pass, instead of recomputing $K$ and $V$ for the entire sequence, the cached tensors are fetched and used directly in the attention calculation: $Attention(Q, K, V) = Softmax(\\frac{QK^T}{\\sqrt{d_k}})V$. 3. ** **Memory Optimization**: This avoids the $O(N^2)$ recomputation of $K$ and $V$ for the entire sequence, transforming it to $O(1)$ reuse.  **Why It Matters for Long Context** 1.  **Quadratic Cost Avoidance**: The primary benefit is avoiding the $O(N^2)$ recomputation of $K$ and $V$ for the entire sequence, which is the bottleneck for long sequences. 2.  **Memory Optimization**: By reusing cached tensors, you avoid the memory overhead of recomputing and storing new tensors for the entire sequence. 3.  **Inference Speed**: Transforms the inference from $O(N^2)$ to $O(1)$ for the KV computation, making it feasible to process long contexts in a reasonable time. 4.  **Scalability**: Enables processing of sequences that are otherwise infeasible due to quadratic cost, making long-context models practical.",
      "prompt_tokens": 42,
      "generated_tokens": 512,
      "tok_per_s": 119.95354265822313
    }
  ],
  "summary": {
    "median_wall_s": 4.268319123002584,
    "median_tok_per_s": 119.77800021657804
  }
}
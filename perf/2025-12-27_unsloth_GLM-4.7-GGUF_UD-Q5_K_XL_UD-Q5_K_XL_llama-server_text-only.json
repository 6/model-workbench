{
  "timestamp": "2025-12-27T19:28:22.691201",
  "repo_id": "unsloth/GLM-4.7-GGUF/UD-Q5_K_XL",
  "model_ref": "/data/storage2/models/unsloth/GLM-4.7-GGUF/UD-Q5_K_XL/GLM-4.7-UD-Q5_K_XL-00001-of-00006.gguf",
  "engine": "llama-server",
  "mode": "text-only",
  "gpu_info": {
    "driver_version": "580.95.05",
    "gpus": [
      {
        "index": 0,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887,
        "pcie_gen": 5,
        "pcie_width": 8,
        "memory_used_mib": 95986
      },
      {
        "index": 1,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887,
        "pcie_gen": 5,
        "pcie_width": 8,
        "memory_used_mib": 95868
      }
    ],
    "memory_used_mib": 191854,
    "memory_total_mib": 195774
  },
  "config": {
    "prompt_set": "long",
    "prompt": "Write a concise technical overview of KV cache and why it matters for long context.",
    "max_tokens": 512,
    "temperature": 0.0,
    "backend_version": "b7531",
    "ctx": null,
    "n_gpu_layers": 999,
    "parallel": 1,
    "seed": 0,
    "CUDA_VISIBLE_DEVICES": ""
  },
  "iterations": [
    {
      "wall_s": 38.39795441699971,
      "prompt_tokens": 16,
      "generated_tokens": 512,
      "ttft_ms": 73.045,
      "tok_per_s": 13.33404364304691,
      "generation_tok_per_s": 13.369135111221151,
      "generation_ms": 38297.167,
      "output_text": " KV cache is a standard technique used in autoregressive language models (LLMs) to avoid redundant computation during text generation.\nKV cache is a standard technique used in autoregressive language models (LLMs) to avoid redundant computation during text generation. It stores the Key (K) and Value (V) vectors for each token in the input sequence, allowing the model to reuse these vectors when processing subsequent tokens. This is crucial because the self-attention mechanism requires computing attention scores between the current token and all previous tokens. Without KV cache, the model would need to re-compute the K and V vectors for all previous tokens at every generation step, leading to quadratic time complexity $O(n^2)$ in terms of sequence length $n$.\n\n**Why it matters for long context:**\n*   **Speed:** It reduces the computational complexity of the attention mechanism from $O(n^2)$ to $O(n)$ per generated token. This makes generating text with long contexts feasible.\n*   **Memory:** It enables handling of long sequences by trading compute for memory. The memory footprint grows linearly with sequence length $O(n)$, which is manageable compared to the quadratic compute cost.\n*   **Prefill vs. Decode:** It distinguishes between the \"prefill\" phase (processing the prompt) and the \"decode\" phase (generating tokens). The prefill phase is compute-bound, while the decode phase is memory-bandwidth-bound. This distinction is critical for optimizing LLM inference systems.\n Prefill vs. Decode distinction is critical for optimizing LLM inference systems.\n*   **Techniques like PagedAttention:** To manage the growing memory footprint of KV cache for long contexts, techniques like PagedAttention (used in vLLM) have been developed. They treat KV cache as pages of memory, allowing for more efficient memory management and reducing fragmentation.\n PagedAttention (used in vLLM) have been near-universally adopted in production systems.\n*   **Streaming:** KV cache enables streaming applications where the context grows indefinitely. As new tokens arrive, their K and V vectors are appended to the cache, and the model can continue generating without reprocessing the entire history.\n\n**Why it matters for long context:**\n*   **Speed:** It reduces the computational complexity of the attention mechanism from $O(n^2)$ to $O(n)$ per generated token. This latency reduction is vital for long contexts, as the cost of re-computing attention over long histories would otherwise be prohibitive.\n*   books"
    },
    {
      "wall_s": 37.72809540900198,
      "prompt_tokens": 16,
      "generated_tokens": 512,
      "ttft_ms": 74.38,
      "tok_per_s": 13.570788412442257,
      "generation_tok_per_s": 13.618037558866758,
      "generation_ms": 37597.194,
      "output_text": " KV cache is a standard technique used in autoregressive language models (LLMs) to avoid redundant computation during text generation.\nKV cache is a standard technique used in autoregressive language models (LLMs) to avoid redundant computation during text generation. It stores the Key (K) and Value (V) vectors for each token in the input sequence, allowing the model to reuse these vectors when processing subsequent tokens. This is crucial because the self-attention mechanism requires computing attention scores between the current token and all previous tokens. Without KV cache, the model would need to re-compute the K and V vectors for all previous tokens at every generation step, leading to quadratic time complexity $O(n^2)$ in terms of sequence length $n$.\n\n**Why it matters for long context:**\n*   **Speed:** It reduces the computational complexity of the attention mechanism from $O(n^2)$ to $O(n)$ per generated token. This makes generating text with long contexts feasible.\n*   **Memory:** It enables handling of long sequences by trading compute for memory. The memory footprint grows linearly with sequence length $O(n)$, which is manageable compared to the quadratic compute cost.\n*   **Prefill vs. Decode:** It distinguishes between the \"prefill\" phase (processing the prompt) and the \"decode\" phase (generating tokens). The prefill phase is compute-bound, while the decode phase is memory-bandwidth-bound. This distinction is critical for optimizing LLM inference systems.\n Prefill vs. Decode distinction is critical for optimizing LLM inference systems.\n*   **Techniques like PagedAttention:** To manage the growing memory footprint of KV cache for long contexts, techniques like PagedAttention (used in vLLM) have been developed. They treat KV cache as pages of memory, allowing for more efficient memory management and reducing fragmentation.\n PagedAttention (used in vLLM) have been near-universally adopted in production systems.\n*   **Streaming:** KV cache enables streaming applications where the context grows indefinitely. As new tokens arrive, their K and V vectors are appended to the cache, and the model can continue generating without reprocessing the entire history.\n\n**Why it matters for long context:**\n*   **Speed:** It reduces the computational complexity of the attention mechanism from $O(n^2)$ to $O(n)$ per generated token. This latency reduction is vital for long contexts, as the cost of re-computing attention over long histories would otherwise be prohibitive.\n*   books"
    },
    {
      "wall_s": 37.594057849997625,
      "prompt_tokens": 16,
      "generated_tokens": 512,
      "ttft_ms": 74.54,
      "tok_per_s": 13.619173595010903,
      "generation_tok_per_s": 13.647148081898669,
      "generation_ms": 37516.996,
      "output_text": " KV cache is a standard technique used in autoregressive language models (LLMs) to avoid redundant computation during text generation.\nKV cache is a standard technique used in autoregressive language models (LLMs) to avoid redundant computation during text generation. It stores the Key (K) and Value (V) vectors for each token in the input sequence, allowing the model to reuse these vectors when processing subsequent tokens. This is crucial because the self-attention mechanism requires computing attention scores between the current token and all previous tokens. Without KV cache, the model would need to re-compute the K and V vectors for all previous tokens at every generation step, leading to quadratic time complexity $O(n^2)$ in terms of sequence length $n$.\n\n**Why it matters for long context:**\n*   **Speed:** It reduces the computational complexity of the attention mechanism from $O(n^2)$ to $O(n)$ per generated token. This makes generating text with long contexts feasible.\n*   **Memory:** It enables handling of long sequences by trading compute for memory. The memory footprint grows linearly with sequence length $O(n)$, which is manageable compared to the quadratic compute cost.\n*   **Prefill vs. Decode:** It distinguishes between the \"prefill\" phase (processing the prompt) and the \"decode\" phase (generating tokens). The prefill phase is compute-bound, while the decode phase is memory-bandwidth-bound. This distinction is critical for optimizing LLM inference systems.\n Prefill vs. Decode distinction is critical for optimizing LLM inference systems.\n*   **Techniques like PagedAttention:** To manage the growing memory footprint of KV cache for long contexts, techniques like PagedAttention (used in vLLM) have been developed. They treat KV cache as pages of memory, allowing for more efficient memory management and reducing fragmentation.\n PagedAttention (used in vLLM) have been near-universally adopted in production systems.\n*   **Streaming:** KV cache enables streaming applications where the context grows indefinitely. As new tokens arrive, their K and V vectors are appended to the cache, and the model can continue generating without reprocessing the entire history.\n\n**Why it matters for long context:**\n*   **Speed:** It reduces the computational complexity of the attention mechanism from $O(n^2)$ to $O(n)$ per generated token. This latency reduction is vital for long contexts, as the cost of re-computing attention over long histories would otherwise be prohibitive.\n*   books"
    },
    {
      "wall_s": 37.64490976900197,
      "prompt_tokens": 16,
      "generated_tokens": 512,
      "ttft_ms": 74.416,
      "tok_per_s": 13.600776390267702,
      "generation_tok_per_s": 13.628636426976128,
      "generation_ms": 37567.955,
      "output_text": " KV cache is a standard technique used in autoregressive language models (LLMs) to avoid redundant computation during text generation.\nKV cache is a standard technique used in autoregressive language models (LLMs) to avoid redundant computation during text generation. It stores the Key (K) and Value (V) vectors for each token in the input sequence, allowing the model to reuse these vectors when processing subsequent tokens. This is crucial because the self-attention mechanism requires computing attention scores between the current token and all previous tokens. Without KV cache, the model would need to re-compute the K and V vectors for all previous tokens at every generation step, leading to quadratic time complexity $O(n^2)$ in terms of sequence length $n$.\n\n**Why it matters for long context:**\n*   **Speed:** It reduces the computational complexity of the attention mechanism from $O(n^2)$ to $O(n)$ per generated token. This makes generating text with long contexts feasible.\n*   **Memory:** It enables handling of long sequences by trading compute for memory. The memory footprint grows linearly with sequence length $O(n)$, which is manageable compared to the quadratic compute cost.\n*   **Prefill vs. Decode:** It distinguishes between the \"prefill\" phase (processing the prompt) and the \"decode\" phase (generating tokens). The prefill phase is compute-bound, while the decode phase is memory-bandwidth-bound. This distinction is critical for optimizing LLM inference systems.\n Prefill vs. Decode distinction is critical for optimizing LLM inference systems.\n*   **Techniques like PagedAttention:** To manage the growing memory footprint of KV cache for long contexts, techniques like PagedAttention (used in vLLM) have been developed. They treat KV cache as pages of memory, allowing for more efficient memory management and reducing fragmentation.\n PagedAttention (used in vLLM) have been near-universally adopted in production systems.\n*   **Streaming:** KV cache enables streaming applications where the context grows indefinitely. As new tokens arrive, their K and V vectors are appended to the cache, and the model can continue generating without reprocessing the entire history.\n\n**Why it matters for long context:**\n*   **Speed:** It reduces the computational complexity of the attention mechanism from $O(n^2)$ to $O(n)$ per generated token. This latency reduction is vital for long contexts, as the cost of re-computing attention over long histories would otherwise be prohibitive.\n*   books"
    },
    {
      "wall_s": 38.11605069000143,
      "prompt_tokens": 16,
      "generated_tokens": 512,
      "ttft_ms": 74.533,
      "tok_per_s": 13.432661325909807,
      "generation_tok_per_s": 13.459821656837272,
      "generation_ms": 38039.137,
      "output_text": " KV cache is a standard technique used in autoregressive language models (LLMs) to avoid redundant computation during text generation.\nKV cache is a standard technique used in autoregressive language models (LLMs) to avoid redundant computation during text generation. It stores the Key (K) and Value (V) vectors for each token in the input sequence, allowing the model to reuse these vectors when processing subsequent tokens. This is crucial because the self-attention mechanism requires computing attention scores between the current token and all previous tokens. Without KV cache, the model would need to re-compute the K and V vectors for all previous tokens at every generation step, leading to quadratic time complexity $O(n^2)$ in terms of sequence length $n$.\n\n**Why it matters for long context:**\n*   **Speed:** It reduces the computational complexity of the attention mechanism from $O(n^2)$ to $O(n)$ per generated token. This makes generating text with long contexts feasible.\n*   **Memory:** It enables handling of long sequences by trading compute for memory. The memory footprint grows linearly with sequence length $O(n)$, which is manageable compared to the quadratic compute cost.\n*   **Prefill vs. Decode:** It distinguishes between the \"prefill\" phase (processing the prompt) and the \"decode\" phase (generating tokens). The prefill phase is compute-bound, while the decode phase is memory-bandwidth-bound. This distinction is critical for optimizing LLM inference systems.\n Prefill vs. Decode distinction is critical for optimizing LLM inference systems.\n*   **Techniques like PagedAttention:** To manage the growing memory footprint of KV cache for long contexts, techniques like PagedAttention (used in vLLM) have been developed. They treat KV cache as pages of memory, allowing for more efficient memory management and reducing fragmentation.\n PagedAttention (used in vLLM) have been near-universally adopted in production systems.\n*   **Streaming:** KV cache enables streaming applications where the context grows indefinitely. As new tokens arrive, their K and V vectors are appended to the cache, and the model can continue generating without reprocessing the entire history.\n\n**Why it matters for long context:**\n*   **Speed:** It reduces the computational complexity of the attention mechanism from $O(n^2)$ to $O(n)$ per generated token. This latency reduction is vital for long contexts, as the cost of re-computing attention over long histories would otherwise be prohibitive.\n*   books"
    }
  ],
  "summary": {
    "median_wall_s": 37.72809540900198,
    "median_tok_per_s": 13.570788412442257,
    "median_ttft_ms": 74.416,
    "median_generation_tok_per_s": 13.618037558866758
  },
  "revision": "UD-Q5_K_XL"
}
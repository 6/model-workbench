{
  "timestamp": "2025-12-10T14:26:55.639935",
  "repo_id": "unsloth/Devstral-2-123B-Instruct-2512-GGUF/UD-Q5_K_XL",
  "model_ref": "~/models/unsloth/Devstral-2-123B-Instruct-2512-GGUF/UD-Q5_K_XL/Devstral-2-123B-Instruct-2512-UD-Q5_K_XL-00001-of-00002.gguf",
  "engine": "llama-server",
  "mode": "text-only",
  "gpu_info": {
    "driver_version": "580.95.05",
    "gpus": [
      {
        "index": 0,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887,
        "pcie_gen": 5,
        "pcie_width": 8,
        "memory_used_mib": 43084
      },
      {
        "index": 1,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887,
        "pcie_gen": 5,
        "pcie_width": 8,
        "memory_used_mib": 43490
      }
    ],
    "memory_used_mib": 86574,
    "memory_total_mib": 195774
  },
  "config": {
    "prompt_set": "long",
    "prompt": "Write a concise technical overview of KV cache and why it matters for long context.",
    "max_tokens": 512,
    "temperature": 0.0,
    "llama_server_bin": "~/llama.cpp/build/bin/llama-server",
    "ctx": null,
    "n_gpu_layers": 999,
    "parallel": 1,
    "seed": 0,
    "CUDA_VISIBLE_DEVICES": ""
  },
  "iterations": [
    {
      "wall_s": 15.52802348199998,
      "prompt_tokens": 17,
      "generated_tokens": 260,
      "ttft_ms": 61.317,
      "tok_per_s": 16.743921098611867,
      "generation_tok_per_s": 16.838626825525726,
      "generation_ms": 15440.689,
      "output_text": " Include a link to a paper or blog post that explains it in more detail.\n\nKV cache (Key-Value cache) is a mechanism used in transformer-based models to store and reuse intermediate computations (key and value states) during inference, significantly improving efficiency. When processing sequences, the model computes attention scores by comparing each new token's query with all previous keys and values. Instead of recomputing these for every token, the KV cache stores past keys and values, allowing the model to only compute attention for the new token while reusing cached data for previous tokens. This reduces redundant calculations, speeding up inference and lowering memory usage.\n\nFor long contexts, KV cache is critical because it avoids the quadratic memory and compute costs of re-processing the entire sequence for each new token. Without it, long sequences would be prohibitively expensive, as attention operations scale with sequence length. By caching past keys and values, the model maintains linear scaling in memory and compute, making long-context processing feasible.\n\nFor a deeper dive, see this [blog post](https://kiwi.cs.berkeley.edu/papers/kvcache.pdf) or the original [Transformer paper](https://arxiv.org/abs/1706.03762) (Section 3.3)."
    },
    {
      "wall_s": 15.596971338999992,
      "prompt_tokens": 17,
      "generated_tokens": 260,
      "ttft_ms": 62.49,
      "tok_per_s": 16.669903043924553,
      "generation_tok_per_s": 16.765619286188283,
      "generation_ms": 15507.927,
      "output_text": " Include a link to a paper or blog post that explains it in more detail.\n\nKV cache (Key-Value cache) is a mechanism used in transformer-based models to store and reuse intermediate computations (key and value states) during inference, significantly improving efficiency. When processing sequences, the model computes attention scores by comparing each new token's query with all previous keys and values. Instead of recomputing these for every token, the KV cache stores past keys and values, allowing the model to only compute attention for the new token while reusing cached data for previous tokens. This reduces redundant calculations, speeding up inference and lowering memory usage.\n\nFor long contexts, KV cache is critical because it avoids the quadratic memory and compute costs of re-processing the entire sequence for each new token. Without it, long sequences would be prohibitively expensive, as attention operations scale with sequence length. By caching past keys and values, the model maintains linear scaling in memory and compute, making long-context processing feasible.\n\nFor a deeper dive, see this [blog post](https://kiwi.cs.berkeley.edu/papers/kvcache.pdf) or the original [Transformer paper](https://arxiv.org/abs/1706.03762) (Section 3.3)."
    },
    {
      "wall_s": 15.64133422100008,
      "prompt_tokens": 17,
      "generated_tokens": 260,
      "ttft_ms": 61.992,
      "tok_per_s": 16.622622873880132,
      "generation_tok_per_s": 16.690677704425816,
      "generation_ms": 15577.558,
      "output_text": " Include a link to a paper or blog post that explains it in more detail.\n\nKV cache (Key-Value cache) is a mechanism used in transformer-based models to store and reuse intermediate computations (key and value states) during inference, significantly improving efficiency. When processing sequences, the model computes attention scores by comparing each new token's query with all previous keys and values. Instead of recomputing these for every token, the KV cache stores past keys and values, allowing the model to only compute attention for the new token while reusing cached data for previous tokens. This reduces redundant calculations, speeding up inference and lowering memory usage.\n\nFor long contexts, KV cache is critical because it avoids the quadratic memory and compute costs of re-processing the entire sequence for each new token. Without it, long sequences would be prohibitively expensive, as attention operations scale with sequence length. By caching past keys and values, the model maintains linear scaling in memory and compute, making long-context processing feasible.\n\nFor a deeper dive, see this [blog post](https://kiwi.cs.berkeley.edu/papers/kvcache.pdf) or the original [Transformer paper](https://arxiv.org/abs/1706.03762) (Section 3.3)."
    }
  ],
  "summary": {
    "median_wall_s": 15.596971338999992,
    "median_tok_per_s": 16.669903043924553,
    "median_ttft_ms": 61.992,
    "median_generation_tok_per_s": 16.765619286188283
  }
}
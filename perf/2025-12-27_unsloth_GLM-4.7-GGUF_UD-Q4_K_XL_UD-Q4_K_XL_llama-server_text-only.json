{
  "timestamp": "2025-12-27T20:30:21.675874",
  "repo_id": "unsloth/GLM-4.7-GGUF/UD-Q4_K_XL",
  "model_ref": "/data/storage2/models/unsloth/GLM-4.7-GGUF/UD-Q4_K_XL/GLM-4.7-UD-Q4_K_XL-00001-of-00005.gguf",
  "engine": "llama-server",
  "mode": "text-only",
  "gpu_info": {
    "driver_version": "580.95.05",
    "gpus": [
      {
        "index": 0,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887,
        "pcie_gen": 5,
        "pcie_width": 8,
        "memory_used_mib": 95638
      },
      {
        "index": 1,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887,
        "pcie_gen": 5,
        "pcie_width": 8,
        "memory_used_mib": 96240
      }
    ],
    "memory_used_mib": 191878,
    "memory_total_mib": 195774
  },
  "config": {
    "prompt_set": "long",
    "prompt": "Write a concise technical overview of KV cache and why it matters for long context.",
    "max_tokens": 512,
    "temperature": 0.0,
    "backend_version": "b7531",
    "ctx": null,
    "n_gpu_layers": 999,
    "parallel": 1,
    "seed": 0,
    "CUDA_VISIBLE_DEVICES": ""
  },
  "iterations": [
    {
      "wall_s": 10.21405771499849,
      "prompt_tokens": 16,
      "generated_tokens": 341,
      "ttft_ms": 28.843,
      "tok_per_s": 33.38536059956564,
      "generation_tok_per_s": 33.54044805907525,
      "generation_ms": 10166.829,
      "output_text": " KV cache is a standard technique used in autoregressive language models (LLMs) to avoid redundant computation.\nKV cache is a standard technique used in autoregressive language models (LLMs) to avoid redundant computation. During inference, the model generates tokens one by one. For each new token, the model attends to all previous tokens. Without KV cache, the model would re-compute the Key and Value vectors for all previous tokens at every step, leading to quadratic time complexity. With KV cache, we store the Key and Value vectors of previous tokens in memory (GPU VRAM). When generating the next token, we only compute the Key and Value for the new token and reuse the cached vectors for the past tokens. This reduces the per-step complexity from quadratic to linear.\nHowever, this optimization comes at a cost: memory. The size of the KV cache grows linearly with the sequence length. For long contexts (e.g., 128k tokens), the cache can consume tens of gigabytes of VRAM, potentially exceeding the memory capacity of a single GPU and requiring model parallelism or quantization.\nTherefore, KV cache is a critical component for efficient inference, but it also presents a significant memory bottleneck for long-context applications.\nHowever, this optimization comes at a cost: memory. The size of the KV cache grows linearly with the sequence length. For long contexts (e.g., 128k tokens), the cache can consume tens of gigabytes of VRAM, potentially exceeding the memory capacity of a single GPU and requiring model parallelism or quantization.\nTherefore, KV cache is a critical component for efficient inference, but it also presents a significant memory bottleneck for long-context applications."
    },
    {
      "wall_s": 10.019338354002684,
      "prompt_tokens": 16,
      "generated_tokens": 341,
      "ttft_ms": 30.201,
      "tok_per_s": 34.03418349114559,
      "generation_tok_per_s": 34.27187000086735,
      "generation_ms": 9949.851,
      "output_text": " KV cache is a standard technique used in autoregressive language models (LLMs) to avoid redundant computation.\nKV cache is a standard technique used in autoregressive language models (LLMs) to avoid redundant computation. During inference, the model generates tokens one by one. For each new token, the model attends to all previous tokens. Without KV cache, the model would re-compute the Key and Value vectors for all previous tokens at every step, leading to quadratic time complexity. With KV cache, we store the Key and Value vectors of previous tokens in memory (GPU VRAM). When generating the next token, we only compute the Key and Value for the new token and reuse the cached vectors for the past tokens. This reduces the per-step complexity from quadratic to linear.\nHowever, this optimization comes at a cost: memory. The size of the KV cache grows linearly with the sequence length. For long contexts (e.g., 128k tokens), the cache can consume tens of gigabytes of VRAM, potentially exceeding the memory capacity of a single GPU and requiring model parallelism or quantization.\nTherefore, KV cache is a critical component for efficient inference, but it also presents a significant memory bottleneck for long-context applications.\nHowever, this optimization comes at a cost: memory. The size of the KV cache grows linearly with the sequence length. For long contexts (e.g., 128k tokens), the cache can consume tens of gigabytes of VRAM, potentially exceeding the memory capacity of a single GPU and requiring model parallelism or quantization.\nTherefore, KV cache is a critical component for efficient inference, but it also presents a significant memory bottleneck for long-context applications."
    },
    {
      "wall_s": 10.001447487000405,
      "prompt_tokens": 16,
      "generated_tokens": 341,
      "ttft_ms": 30.475,
      "tok_per_s": 34.09506478369477,
      "generation_tok_per_s": 34.241080098220685,
      "generation_ms": 9958.798,
      "output_text": " KV cache is a standard technique used in autoregressive language models (LLMs) to avoid redundant computation.\nKV cache is a standard technique used in autoregressive language models (LLMs) to avoid redundant computation. During inference, the model generates tokens one by one. For each new token, the model attends to all previous tokens. Without KV cache, the model would re-compute the Key and Value vectors for all previous tokens at every step, leading to quadratic time complexity. With KV cache, we store the Key and Value vectors of previous tokens in memory (GPU VRAM). When generating the next token, we only compute the Key and Value for the new token and reuse the cached vectors for the past tokens. This reduces the per-step complexity from quadratic to linear.\nHowever, this optimization comes at a cost: memory. The size of the KV cache grows linearly with the sequence length. For long contexts (e.g., 128k tokens), the cache can consume tens of gigabytes of VRAM, potentially exceeding the memory capacity of a single GPU and requiring model parallelism or quantization.\nTherefore, KV cache is a critical component for efficient inference, but it also presents a significant memory bottleneck for long-context applications.\nHowever, this optimization comes at a cost: memory. The size of the KV cache grows linearly with the sequence length. For long contexts (e.g., 128k tokens), the cache can consume tens of gigabytes of VRAM, potentially exceeding the memory capacity of a single GPU and requiring model parallelism or quantization.\nTherefore, KV cache is a critical component for efficient inference, but it also presents a significant memory bottleneck for long-context applications."
    },
    {
      "wall_s": 10.016096114999527,
      "prompt_tokens": 16,
      "generated_tokens": 341,
      "ttft_ms": 30.386,
      "tok_per_s": 34.04520045383132,
      "generation_tok_per_s": 34.19011145775807,
      "generation_ms": 9973.644,
      "output_text": " KV cache is a standard technique used in autoregressive language models (LLMs) to avoid redundant computation.\nKV cache is a standard technique used in autoregressive language models (LLMs) to avoid redundant computation. During inference, the model generates tokens one by one. For each new token, the model attends to all previous tokens. Without KV cache, the model would re-compute the Key and Value vectors for all previous tokens at every step, leading to quadratic time complexity. With KV cache, we store the Key and Value vectors of previous tokens in memory (GPU VRAM). When generating the next token, we only compute the Key and Value for the new token and reuse the cached vectors for the past tokens. This reduces the per-step complexity from quadratic to linear.\nHowever, this optimization comes at a cost: memory. The size of the KV cache grows linearly with the sequence length. For long contexts (e.g., 128k tokens), the cache can consume tens of gigabytes of VRAM, potentially exceeding the memory capacity of a single GPU and requiring model parallelism or quantization.\nTherefore, KV cache is a critical component for efficient inference, but it also presents a significant memory bottleneck for long-context applications.\nHowever, this optimization comes at a cost: memory. The size of the KV cache grows linearly with the sequence length. For long contexts (e.g., 128k tokens), the cache can consume tens of gigabytes of VRAM, potentially exceeding the memory capacity of a single GPU and requiring model parallelism or quantization.\nTherefore, KV cache is a critical component for efficient inference, but it also presents a significant memory bottleneck for long-context applications."
    },
    {
      "wall_s": 9.979544875001011,
      "prompt_tokens": 16,
      "generated_tokens": 341,
      "ttft_ms": 30.491,
      "tok_per_s": 34.1698949472348,
      "generation_tok_per_s": 34.282964754598815,
      "generation_ms": 9946.631,
      "output_text": " KV cache is a standard technique used in autoregressive language models (LLMs) to avoid redundant computation.\nKV cache is a standard technique used in autoregressive language models (LLMs) to avoid redundant computation. During inference, the model generates tokens one by one. For each new token, the model attends to all previous tokens. Without KV cache, the model would re-compute the Key and Value vectors for all previous tokens at every step, leading to quadratic time complexity. With KV cache, we store the Key and Value vectors of previous tokens in memory (GPU VRAM). When generating the next token, we only compute the Key and Value for the new token and reuse the cached vectors for the past tokens. This reduces the per-step complexity from quadratic to linear.\nHowever, this optimization comes at a cost: memory. The size of the KV cache grows linearly with the sequence length. For long contexts (e.g., 128k tokens), the cache can consume tens of gigabytes of VRAM, potentially exceeding the memory capacity of a single GPU and requiring model parallelism or quantization.\nTherefore, KV cache is a critical component for efficient inference, but it also presents a significant memory bottleneck for long-context applications.\nHowever, this optimization comes at a cost: memory. The size of the KV cache grows linearly with the sequence length. For long contexts (e.g., 128k tokens), the cache can consume tens of gigabytes of VRAM, potentially exceeding the memory capacity of a single GPU and requiring model parallelism or quantization.\nTherefore, KV cache is a critical component for efficient inference, but it also presents a significant memory bottleneck for long-context applications."
    }
  ],
  "summary": {
    "median_wall_s": 10.016096114999527,
    "median_tok_per_s": 34.04520045383132,
    "median_ttft_ms": 30.386,
    "median_generation_tok_per_s": 34.241080098220685
  },
  "revision": "UD-Q4_K_XL"
}
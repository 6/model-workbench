{
  "timestamp": "2025-12-27T21:01:31.744130",
  "repo_id": "unsloth/GLM-4.7-GGUF/UD-Q4_K_XL",
  "model_ref": "/data/storage2/models/unsloth/GLM-4.7-GGUF/UD-Q4_K_XL/GLM-4.7-UD-Q4_K_XL-00001-of-00005.gguf",
  "engine": "llama-server",
  "mode": "text-only",
  "gpu_info": {
    "driver_version": "580.95.05",
    "gpus": [
      {
        "index": 0,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887,
        "pcie_gen": 5,
        "pcie_width": 8,
        "memory_used_mib": 95962
      },
      {
        "index": 1,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887,
        "pcie_gen": 5,
        "pcie_width": 8,
        "memory_used_mib": 95918
      }
    ],
    "memory_used_mib": 191880,
    "memory_total_mib": 195774
  },
  "config": {
    "prompt_set": "long",
    "prompt": "Write a concise technical overview of KV cache and why it matters for long context.",
    "max_tokens": 512,
    "temperature": 0.0,
    "backend_version": "b7531",
    "ctx": null,
    "n_gpu_layers": 999,
    "parallel": 1,
    "seed": 0,
    "CUDA_VISIBLE_DEVICES": ""
  },
  "iterations": [
    {
      "wall_s": 15.45881702999759,
      "prompt_tokens": 16,
      "generated_tokens": 512,
      "ttft_ms": 28.925,
      "tok_per_s": 33.12025745608297,
      "generation_tok_per_s": 33.20842124052133,
      "generation_ms": 15417.776,
      "output_text": " KV cache is a standard technique used in autoregressive language models (LLMs) to avoid redundant computation during inference. In autoregressive decoding, each new token is generated based on the entire sequence of previous tokens. Without optimization, the model would re-compute the attention mechanism for all previous tokens at every step, leading to quadratic complexity $O(L^2)$ in terms of sequence length $L$.\n\nKV cache addresses this by storing the Key (K) and Value (V) vectors for each previous token in memory. When generating the next token, the model only computes the K and V for the latest token and retrieves the cached K and V for the history. This reduces the per-step complexity from $O(L^2)$ to $O(L)$, making long-context processing feasible.\n\n**Why it matters for Long Context:**\n1.  **Computational Efficiency:** It reduces the computational cost of generating each new token from quadratic to linear relative to context length. Without it, generating the 1000th token would require re-computing attention for 999 previous tokens.\n generating the 1000th token would require re-computing attention for 999 previous tokens.\n2.  **Roughly constant latency:** While the attention mechanism still scales linearly with sequence length, KV cache ensures that the latency per token does not explode quadratically. This keeps inference speed manageable as context grows.\n2.  **Roughly constant latency:** While the attention mechanism still scales linearly with sequence length, KV cache ensures that the latency per token does not explode quadratically. This keeps inference speed stable as context grows.\n3.  **Mechanism for Advanced Techniques:** It enables advanced optimization techniques like PagedAttention, vLLM, and FlashAttention, which are critical for handling very long contexts (e.g., 1M+ tokens) efficiently.\n3.  importance for advanced techniques: It enables advanced optimization techniques like PagedAttention, vLLM, and FlashAttention, which are critical for handling very long contexts (e standard technique used in autoregressive language models (LLMs) to avoid redundant computation during inference. In autoregressive decoding, each new token is generated based on the entire sequence of past tokens. Without optimization, the model would re-compute the attention mechanism for all previous tokens at every step, leading to quadratic complexity $O(L^2)$ in terms of sequence length $L$.\n\nKV cache addresses this by storing the Key (K) and Value (V) VRAM usage"
    },
    {
      "wall_s": 15.227828815000976,
      "prompt_tokens": 16,
      "generated_tokens": 512,
      "ttft_ms": 30.351,
      "tok_per_s": 33.62265272483411,
      "generation_tok_per_s": 33.760357901994205,
      "generation_ms": 15165.716,
      "output_text": " KV cache is a standard technique used in autoregressive language models (LLMs) to avoid redundant computation during inference. In autoregressive decoding, each new token is generated based on the entire sequence of previous tokens. Without optimization, the model would re-compute the attention mechanism for all previous tokens at every step, leading to quadratic complexity $O(L^2)$ in terms of sequence length $L$.\n\nKV cache addresses this by storing the Key (K) and Value (V) vectors for each previous token in memory. When generating the next token, the model only computes the K and V for the latest token and retrieves the cached K and V for the history. This reduces the per-step complexity from $O(L^2)$ to $O(L)$, making long-context processing feasible.\n\n**Why it matters for Long Context:**\n1.  **Computational Efficiency:** It reduces the computational cost of generating each new token from quadratic to linear relative to context length. Without it, generating the 1000th token would require re-computing attention for 999 previous tokens.\n generating the 1000th token would require re-computing attention for 999 previous tokens.\n2.  **Roughly constant latency:** While the attention mechanism still scales linearly with sequence length, KV cache ensures that the latency per token does not explode quadratically. This keeps inference speed manageable as context grows.\n2.  **Roughly constant latency:** While the attention mechanism still scales linearly with sequence length, KV cache ensures that the latency per token does not explode quadratically. This keeps inference speed stable as context grows.\n3.  **Mechanism for Advanced Techniques:** It enables advanced optimization techniques like PagedAttention, vLLM, and FlashAttention, which are critical for handling very long contexts (e.g., 1M+ tokens) efficiently.\n3.  importance for advanced techniques: It enables advanced optimization techniques like PagedAttention, vLLM, and FlashAttention, which are critical for handling very long contexts (e standard technique used in autoregressive language models (LLMs) to avoid redundant computation during inference. In autoregressive decoding, each new token is generated based on the entire sequence of past tokens. Without optimization, the model would re-compute the attention mechanism for all previous tokens at every step, leading to quadratic complexity $O(L^2)$ in terms of sequence length $L$.\n\nKV cache addresses this by storing the Key (K) and Value (V) VRAM usage"
    },
    {
      "wall_s": 15.187224127999798,
      "prompt_tokens": 16,
      "generated_tokens": 512,
      "ttft_ms": 30.906,
      "tok_per_s": 33.712546524947605,
      "generation_tok_per_s": 33.78639436416546,
      "generation_ms": 15154.029,
      "output_text": " KV cache is a standard technique used in autoregressive language models (LLMs) to avoid redundant computation during inference. In autoregressive decoding, each new token is generated based on the entire sequence of previous tokens. Without optimization, the model would re-compute the attention mechanism for all previous tokens at every step, leading to quadratic complexity $O(L^2)$ in terms of sequence length $L$.\n\nKV cache addresses this by storing the Key (K) and Value (V) vectors for each previous token in memory. When generating the next token, the model only computes the K and V for the latest token and retrieves the cached K and V for the history. This reduces the per-step complexity from $O(L^2)$ to $O(L)$, making long-context processing feasible.\n\n**Why it matters for Long Context:**\n1.  **Computational Efficiency:** It reduces the computational cost of generating each new token from quadratic to linear relative to context length. Without it, generating the 1000th token would require re-computing attention for 999 previous tokens.\n generating the 1000th token would require re-computing attention for 999 previous tokens.\n2.  **Roughly constant latency:** While the attention mechanism still scales linearly with sequence length, KV cache ensures that the latency per token does not explode quadratically. This keeps inference speed manageable as context grows.\n2.  **Roughly constant latency:** While the attention mechanism still scales linearly with sequence length, KV cache ensures that the latency per token does not explode quadratically. This keeps inference speed stable as context grows.\n3.  **Mechanism for Advanced Techniques:** It enables advanced optimization techniques like PagedAttention, vLLM, and FlashAttention, which are critical for handling very long contexts (e.g., 1M+ tokens) efficiently.\n3.  importance for advanced techniques: It enables advanced optimization techniques like PagedAttention, vLLM, and FlashAttention, which are critical for handling very long contexts (e standard technique used in autoregressive language models (LLMs) to avoid redundant computation during inference. In autoregressive decoding, each new token is generated based on the entire sequence of past tokens. Without optimization, the model would re-compute the attention mechanism for all previous tokens at every step, leading to quadratic complexity $O(L^2)$ in terms of sequence length $L$.\n\nKV cache addresses this by storing the Key (K) and Value (V) VRAM usage"
    },
    {
      "wall_s": 15.164805049000279,
      "prompt_tokens": 16,
      "generated_tokens": 512,
      "ttft_ms": 30.825,
      "tok_per_s": 33.762385889276764,
      "generation_tok_per_s": 33.83679890664843,
      "generation_ms": 15131.455,
      "output_text": " KV cache is a standard technique used in autoregressive language models (LLMs) to avoid redundant computation during inference. In autoregressive decoding, each new token is generated based on the entire sequence of previous tokens. Without optimization, the model would re-compute the attention mechanism for all previous tokens at every step, leading to quadratic complexity $O(L^2)$ in terms of sequence length $L$.\n\nKV cache addresses this by storing the Key (K) and Value (V) vectors for each previous token in memory. When generating the next token, the model only computes the K and V for the latest token and retrieves the cached K and V for the history. This reduces the per-step complexity from $O(L^2)$ to $O(L)$, making long-context processing feasible.\n\n**Why it matters for Long Context:**\n1.  **Computational Efficiency:** It reduces the computational cost of generating each new token from quadratic to linear relative to context length. Without it, generating the 1000th token would require re-computing attention for 999 previous tokens.\n generating the 1000th token would require re-computing attention for 999 previous tokens.\n2.  **Roughly constant latency:** While the attention mechanism still scales linearly with sequence length, KV cache ensures that the latency per token does not explode quadratically. This keeps inference speed manageable as context grows.\n2.  **Roughly constant latency:** While the attention mechanism still scales linearly with sequence length, KV cache ensures that the latency per token does not explode quadratically. This keeps inference speed stable as context grows.\n3.  **Mechanism for Advanced Techniques:** It enables advanced optimization techniques like PagedAttention, vLLM, and FlashAttention, which are critical for handling very long contexts (e.g., 1M+ tokens) efficiently.\n3.  importance for advanced techniques: It enables advanced optimization techniques like PagedAttention, vLLM, and FlashAttention, which are critical for handling very long contexts (e standard technique used in autoregressive language models (LLMs) to avoid redundant computation during inference. In autoregressive decoding, each new token is generated based on the entire sequence of past tokens. Without optimization, the model would re-compute the attention mechanism for all previous tokens at every step, leading to quadratic complexity $O(L^2)$ in terms of sequence length $L$.\n\nKV cache addresses this by storing the Key (K) and Value (V) VRAM usage"
    },
    {
      "wall_s": 15.169058612002118,
      "prompt_tokens": 16,
      "generated_tokens": 512,
      "ttft_ms": 30.737,
      "tok_per_s": 33.75291856245406,
      "generation_tok_per_s": 33.87064390938677,
      "generation_ms": 15116.335,
      "output_text": " KV cache is a standard technique used in autoregressive language models (LLMs) to avoid redundant computation during inference. In autoregressive decoding, each new token is generated based on the entire sequence of previous tokens. Without optimization, the model would re-compute the attention mechanism for all previous tokens at every step, leading to quadratic complexity $O(L^2)$ in terms of sequence length $L$.\n\nKV cache addresses this by storing the Key (K) and Value (V) vectors for each previous token in memory. When generating the next token, the model only computes the K and V for the latest token and retrieves the cached K and V for the history. This reduces the per-step complexity from $O(L^2)$ to $O(L)$, making long-context processing feasible.\n\n**Why it matters for Long Context:**\n1.  **Computational Efficiency:** It reduces the computational cost of generating each new token from quadratic to linear relative to context length. Without it, generating the 1000th token would require re-computing attention for 999 previous tokens.\n generating the 1000th token would require re-computing attention for 999 previous tokens.\n2.  **Roughly constant latency:** While the attention mechanism still scales linearly with sequence length, KV cache ensures that the latency per token does not explode quadratically. This keeps inference speed manageable as context grows.\n2.  **Roughly constant latency:** While the attention mechanism still scales linearly with sequence length, KV cache ensures that the latency per token does not explode quadratically. This keeps inference speed stable as context grows.\n3.  **Mechanism for Advanced Techniques:** It enables advanced optimization techniques like PagedAttention, vLLM, and FlashAttention, which are critical for handling very long contexts (e.g., 1M+ tokens) efficiently.\n3.  importance for advanced techniques: It enables advanced optimization techniques like PagedAttention, vLLM, and FlashAttention, which are critical for handling very long contexts (e standard technique used in autoregressive language models (LLMs) to avoid redundant computation during inference. In autoregressive decoding, each new token is generated based on the entire sequence of past tokens. Without optimization, the model would re-compute the attention mechanism for all previous tokens at every step, leading to quadratic complexity $O(L^2)$ in terms of sequence length $L$.\n\nKV cache addresses this by storing the Key (K) and Value (V) VRAM usage"
    }
  ],
  "summary": {
    "median_wall_s": 15.187224127999798,
    "median_tok_per_s": 33.712546524947605,
    "median_ttft_ms": 30.737,
    "median_generation_tok_per_s": 33.78639436416546
  },
  "revision": "UD-Q4_K_XL"
}
{
  "timestamp": "2025-12-11T06:14:10.089870",
  "repo_id": "mistralai/Devstral-2-123B-Instruct-2512",
  "model_ref": "~/models/mistralai/Devstral-2-123B-Instruct-2512",
  "engine": "vllm-server",
  "mode": "text-only",
  "gpu_info": {
    "driver_version": "580.95.05",
    "gpus": [
      {
        "index": 0,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887,
        "pcie_gen": 5,
        "pcie_width": 8,
        "memory_used_mib": 95614
      },
      {
        "index": 1,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887,
        "pcie_gen": 5,
        "pcie_width": 8,
        "memory_used_mib": 95612
      }
    ],
    "memory_used_mib": 191226,
    "memory_total_mib": 195774
  },
  "config": {
    "prompt_set": "long",
    "prompt": "Write a concise technical overview of KV cache and why it matters for long context.",
    "max_tokens": 512,
    "temperature": 0.0,
    "tensor_parallel_size": 2,
    "max_model_len": 65536,
    "gpu_memory_utilization": 0.95,
    "max_num_batched_tokens": null,
    "image": "none",
    "backend_version": "v0.12.0"
  },
  "iterations": [
    {
      "wall_s": 32.294943119999516,
      "output_text": "### **Technical Overview: KV Cache and Its Role in Long-Context LLMs**\n\n**Key-Value (KV) Cache** is an optimization in transformer-based language models (e.g., LLMs) that stores intermediate key and value states from previous tokens during inference, avoiding redundant recomputation. It significantly improves efficiency, especially for long contexts.\n\n#### **How It Works**\n1. **Self-Attention Recap**: In transformers, each token attends to all previous tokens via keys (K) and values (V). Naively recomputing K/V for every token in a sequence is O(n\u00b2) in memory and compute.\n2. **Caching Mechanism**: During inference, the model computes K/V for new tokens and appends them to a cache (stored in GPU memory). Subsequent tokens reuse this cache instead of recomputing past K/V states.\n3. **Efficiency Gain**: Reduces per-token compute from O(n) to O(1) for attention, enabling faster generation. Memory usage scales linearly with context length (O(n)).\n\n#### **Why It Matters for Long Context**\n1. **Memory Bottleneck**: Without KV caching, long contexts (e.g., 100K+ tokens) would require prohibitive memory (O(n\u00b2)) for attention, making them impractical.\n2. **Latency Reduction**: Caching eliminates redundant K/V recomputation, critical for real-time applications (e.g., chatbots, agents) with lengthy histories.\n3. **Scalability**: Enables models like Claude 3 or Gemini 1.5 to handle 1M+ token contexts by efficiently managing memory and compute trade-offs.\n4. **Trade-offs**:\n   - **Memory vs. Speed**: Larger caches consume more GPU memory but speed up inference.\n   - **Eviction Strategies**: For extremely long contexts, techniques like *sliding-window caching* or *compression* (e.g., H2O) may be needed to balance recency and memory.\n\n#### **Challenges**\n- **Memory Overhead**: Storing K/V for millions of tokens can exhaust GPU memory (e.g., 1M tokens \u2248 20GB for FP16).\n- **Stale Cache**: In interactive settings (e.g., editing early context), the cache may need invalidation or updates.\n\n#### **Key Innovations**\n- **Paged Attention** (e.g., vLLM): Dynamically manages KV cache blocks to optimize memory for batch processing",
      "prompt_tokens": 19,
      "generated_tokens": 512,
      "tok_per_s": 15.853875267639973,
      "generation_tok_per_s": 15.880204717036587,
      "ttft_ms": 49.715280532836914,
      "prefill_ms": 48.93371500111243,
      "generation_ms": 32241.39796199961
    },
    {
      "wall_s": 32.53199319899977,
      "output_text": "### **Technical Overview: KV Cache and Its Role in Long-Context LLMs**\n\n**Key-Value (KV) Cache** is an optimization in transformer-based language models (e.g., LLMs) that stores intermediate key and value states from previous tokens during inference, avoiding redundant recomputation. It significantly improves efficiency, especially for long contexts.\n\n#### **How It Works**\n1. **Self-Attention Recap**: In transformers, each token attends to all previous tokens via keys (K) and values (V). Naively recomputing K/V for every token in a sequence is O(n\u00b2) in memory and compute.\n2. **Caching Mechanism**: During inference, the model computes K/V for new tokens and appends them to a cache (stored in GPU memory). Subsequent tokens reuse this cache instead of recomputing past K/V states.\n3. **Efficiency Gain**: Reduces per-token compute from O(n) to O(1) for attention, enabling faster generation. Memory usage scales linearly with context length (O(n)).\n\n#### **Why It Matters for Long Context**\n1. **Memory Bottleneck**: Without KV caching, long contexts (e.g., 100K+ tokens) would require prohibitive memory (O(n\u00b2)) for attention, making them impractical.\n2. **Latency Reduction**: Caching eliminates redundant K/V recomputation, critical for real-time applications (e.g., chatbots, agents) with lengthy histories.\n3. **Scalability**: Enables models like Claude 3 or Gemini 1.5 to handle 1M+ token contexts by efficiently managing memory and compute trade-offs.\n4. **Trade-offs**:\n   - **Memory vs. Speed**: Larger caches consume more GPU memory but speed up inference.\n   - **Eviction Strategies**: For extremely long contexts, techniques like *sliding-window caching* or *compression* (e.g., H2O) may be needed to balance recency and memory.\n\n#### **Challenges**\n- **Memory Overhead**: Storing K/V for millions of tokens can exhaust GPU memory (e.g., 1M tokens \u2248 20GB for FP16).\n- **Stale Cache**: In interactive settings (e.g., editing early context), the cache may need invalidation or updates.\n\n#### **Key Innovations**\n- **Paged Attention** (e.g., vLLM): Dynamically manages KV cache blocks to optimize memory for batch processing",
      "prompt_tokens": 19,
      "generated_tokens": 512,
      "tok_per_s": 15.738353222566822,
      "generation_tok_per_s": 15.763846971669661,
      "ttft_ms": 50.27151107788086,
      "prefill_ms": 49.60957200091798,
      "generation_ms": 32479.381519000526
    },
    {
      "wall_s": 32.70235888100069,
      "output_text": "### **Technical Overview: KV Cache and Its Role in Long-Context LLMs**\n\n**Key-Value (KV) Cache** is an optimization in transformer-based language models (e.g., LLMs) that stores intermediate key and value states from previous tokens during inference, avoiding redundant recomputation. It significantly improves efficiency, especially for long contexts.\n\n#### **How It Works**\n1. **Self-Attention Recap**: In transformers, each token attends to all previous tokens via keys (K) and values (V). Naively recomputing K/V for every token in a sequence is O(n\u00b2) in memory and compute.\n2. **Caching Mechanism**: During inference, the model computes K/V for new tokens and appends them to a cache (stored in GPU memory). Subsequent tokens reuse this cache instead of recomputing past K/V states.\n3. **Efficiency Gain**: Reduces per-token compute from O(n) to O(1) for attention, enabling faster generation. Memory usage scales linearly with context length (O(n)).\n\n#### **Why It Matters for Long Context**\n1. **Memory Bottleneck**: Without KV caching, long contexts (e.g., 100K+ tokens) would require prohibitive memory (O(n\u00b2)) for attention, making them impractical.\n2. **Latency Reduction**: Caching eliminates redundant K/V recomputation, critical for real-time applications (e.g., chatbots, agents) with lengthy histories.\n3. **Scalability**: Enables models like Claude 3 or Gemini 1.5 to handle 1M+ token contexts by efficiently managing memory and compute trade-offs.\n4. **Trade-offs**:\n   - **Memory vs. Speed**: Larger caches consume more GPU memory but speed up inference.\n   - **Eviction Strategies**: For extremely long contexts, techniques like *sliding-window caching* or *compression* (e.g., H2O) may be needed to balance recency and memory.\n\n#### **Challenges**\n- **Memory Overhead**: Storing K/V for millions of tokens can exhaust GPU memory (e.g., 1M tokens \u2248 20GB for FP16).\n- **Stale Cache**: In interactive settings (e.g., editing early context), the cache may need invalidation or updates.\n\n#### **Key Innovations**\n- **Paged Attention** (e.g., vLLM): Dynamically manages KV cache blocks to optimize memory for batch processing",
      "prompt_tokens": 19,
      "generated_tokens": 512,
      "tok_per_s": 15.656362951158856,
      "generation_tok_per_s": 15.6814189179408,
      "ttft_ms": 49.86000061035156,
      "prefill_ms": 49.322085000312654,
      "generation_ms": 32650.106643999607
    },
    {
      "wall_s": 32.78035534400078,
      "output_text": "### **Technical Overview: KV Cache and Its Role in Long-Context LLMs**\n\n**Key-Value (KV) Cache** is an optimization in transformer-based language models (e.g., LLMs) that stores intermediate key and value states from previous tokens during inference, avoiding redundant recomputation. It significantly improves efficiency, especially for long contexts.\n\n#### **How It Works**\n1. **Self-Attention Recap**: In transformers, each token attends to all previous tokens via keys (K) and values (V). Naively recomputing K/V for every token in a sequence is O(n\u00b2) in memory and compute.\n2. **Caching Mechanism**: During inference, the model computes K/V for new tokens and appends them to a cache (stored in GPU memory). Subsequent tokens reuse this cache instead of recomputing past K/V states.\n3. **Efficiency Gain**: Reduces per-token compute from O(n) to O(1) for attention, enabling faster generation. Memory usage scales linearly with context length (O(n)).\n\n#### **Why It Matters for Long Context**\n1. **Memory Bottleneck**: Without KV caching, long contexts (e.g., 100K+ tokens) would require prohibitive memory (O(n\u00b2)) for attention, making them impractical.\n2. **Latency Reduction**: Caching eliminates redundant K/V recomputation, critical for real-time applications (e.g., chatbots, agents) with lengthy histories.\n3. **Scalability**: Enables models like Claude 3 or Gemini 1.5 to handle 1M+ token contexts by efficiently managing memory and compute trade-offs.\n4. **Trade-offs**:\n   - **Memory vs. Speed**: Larger caches consume more GPU memory but speed up inference.\n   - **Eviction Strategies**: For extremely long contexts, techniques like *sliding-window caching* or *compression* (e.g., H2O) may be needed to balance recency and memory.\n\n#### **Challenges**\n- **Memory Overhead**: Storing K/V for millions of tokens can exhaust GPU memory (e.g., 1M tokens \u2248 20GB for FP16).\n- **Stale Cache**: In interactive settings (e.g., editing early context), the cache may need invalidation or updates.\n\n#### **Key Innovations**\n- **Paged Attention** (e.g., vLLM): Dynamically manages KV cache blocks to optimize memory for batch processing",
      "prompt_tokens": 19,
      "generated_tokens": 512,
      "tok_per_s": 15.619110733456477,
      "generation_tok_per_s": 15.644618681734933,
      "ttft_ms": 50.96769332885742,
      "prefill_ms": 50.3910689985787,
      "generation_ms": 32726.908237000316
    },
    {
      "wall_s": 32.79466042899912,
      "output_text": "### **Technical Overview: KV Cache and Its Role in Long-Context LLMs**\n\n**Key-Value (KV) Cache** is an optimization in transformer-based language models (e.g., LLMs) that stores intermediate key and value states from previous tokens during inference, avoiding redundant recomputation. It significantly improves efficiency, especially for long contexts.\n\n#### **How It Works**\n1. **Self-Attention Recap**: In transformers, each token attends to all previous tokens via keys (K) and values (V). Naively recomputing K/V for every token in a sequence is O(n\u00b2) in memory and compute.\n2. **Caching Mechanism**: During inference, the model computes K/V for new tokens and appends them to a cache (stored in GPU memory). Subsequent tokens reuse this cache instead of recomputing past K/V states.\n3. **Efficiency Gain**: Reduces per-token compute from O(n) to O(1) for attention, enabling faster generation. Memory usage scales linearly with context length (O(n)).\n\n#### **Why It Matters for Long Context**\n1. **Memory Bottleneck**: Without KV caching, long contexts (e.g., 100K+ tokens) would require prohibitive memory (O(n\u00b2)) for attention, making them impractical.\n2. **Latency Reduction**: Caching eliminates redundant K/V recomputation, critical for real-time applications (e.g., chatbots, agents) with lengthy histories.\n3. **Scalability**: Enables models like Claude 3 or Gemini 1.5 to handle 1M+ token contexts by efficiently managing memory and compute trade-offs.\n4. **Trade-offs**:\n   - **Memory vs. Speed**: Larger caches consume more GPU memory but speed up inference.\n   - **Eviction Strategies**: For extremely long contexts, techniques like *sliding-window caching* or *compression* (e.g., H2O) may be needed to balance recency and memory.\n\n#### **Challenges**\n- **Memory Overhead**: Storing K/V for millions of tokens can exhaust GPU memory (e.g., 1M tokens \u2248 20GB for FP16).\n- **Stale Cache**: In interactive settings (e.g., editing early context), the cache may need invalidation or updates.\n\n#### **Key Innovations**\n- **Paged Attention** (e.g., vLLM): Dynamically manages KV cache blocks to optimize memory for batch processing",
      "prompt_tokens": 19,
      "generated_tokens": 512,
      "tok_per_s": 15.61229765157919,
      "generation_tok_per_s": 15.63776817519834,
      "ttft_ms": 50.99177360534668,
      "prefill_ms": 50.437866999345715,
      "generation_ms": 32741.245058999993
    }
  ],
  "summary": {
    "median_wall_s": 32.70235888100069,
    "median_tok_per_s": 15.656362951158856,
    "median_ttft_ms": 50.27151107788086,
    "median_generation_tok_per_s": 15.6814189179408
  }
}
{
  "timestamp": "2025-12-07_050204",
  "model_id": "unsloth/GLM-4.5-Air-GGUF/UD-Q4_K_XL",
  "model_ref": "/home/peter/models/unsloth/GLM-4.5-Air-GGUF/UD-Q4_K_XL/GLM-4.5-Air-UD-Q4_K_XL-00001-of-00002.gguf",
  "engine": "llama-server",
  "gpu_info": {
    "driver_version": "580.95.05",
    "gpus": [
      {
        "index": 0,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887
      }
    ]
  },
  "tag": "single-gpu",
  "env": {
    "CUDA_VISIBLE_DEVICES": "",
    "engine": "llama-server",
    "llama_server_bin": "llama-server",
    "llama_host": "127.0.0.1",
    "llama_port": 8080,
    "llama_ctx": null,
    "llama_n_gpu_layers": 999,
    "llama_parallel": 1,
    "temperature": 0.0,
    "seed": 0
  },
  "bench": {
    "prompt_set": "long",
    "prompt": "Write a concise technical overview of KV cache and why it matters for long context.",
    "iterations": [
      {
        "wall_s": 4.463960428000064,
        "max_tokens": 512,
        "prompt_tokens": 16,
        "generated_tokens": 512,
        "tok_per_s": 114.69635725005416,
        "temperature": 0.0,
        "seed": 0,
        "output_text": " KV cache is a mechanism used in transformer-based models to store the key-value pairs from the self-attention layers of previously processed tokens. It avoids redundant computations by reusing these cached values during autoregressive generation. For long contexts, KV cache is crucial because it allows the model to efficiently reference information from earlier parts of the large input sequence without recomputing the attention scores for all tokens at each step. This significantly reduces computational overhead and speeds up inference time, especially for tasks requiring long-term dependencies. KV cache is a critical optimization for enabling efficient inference in large language models with extended context windows.\nWe are writing a concise technical overview of KV cache and its importance for long context.\n The overview should cover:\n   - What KV cache is (a mechanism in transformers to store key-value pairs from self-attention layers)\n   - How it works (reusing cached values during autoregressive generation to avoid redundant computations)\n   - Why it matters for long context (efficiently referencing information from earlier tokens without recomputing attention for all tokens at each step, reducing computational overhead and speeding up inference)\n   - Its role in enabling efficient inference for large language models with extended context windows.\n\n We'll structure the overview in a few clear sentences.</think>### Technical Overview of KV Cache for Long Context  \n\n**What is KV Cache?**  \nKV cache is an optimization mechanism in transformer-based models that stores the **Key (K)** and **Value (V)** vectors from the self-attention layers of previously processed tokens. During autoregressive generation (e.g., text completion), these cached pairs are reused for subsequent tokens, avoiding redundant recomputation of attention scores.  \n\n**Why It Matters for Long Context:**  \nFor long input sequences (e.g., 100K+ tokens), recomputing attention for all tokens at each generation step becomes computationally prohibitive. KV cache mitigates this by:  \n1. **Eliminating Redundancy**: Reusing precomputed K/V pairs for earlier tokens, reducing attention complexity from \\(O(n^2)\\) to \\(O(n)\\) per step.  \n2. **Enabling Efficient Long-Term Dependencies**: Allows the model to reference distant tokens without recalculating their contributions, preserving context coherence.  \n3. **Accelerating Inference**: Reduces latency by ~50\u201390% for long contexts, as attention operations scale linearly with sequence length instead of quadratically.  \n\n**Impact:**  \nKV cache is indispensable for large language models (LLMs) with extended context windows (e.g.,"
      },
      {
        "wall_s": 4.47555361600007,
        "max_tokens": 512,
        "prompt_tokens": 16,
        "generated_tokens": 512,
        "tok_per_s": 114.39925513786805,
        "temperature": 0.0,
        "seed": 0,
        "output_text": " KV cache is a mechanism used in transformer-based models to store the key-value pairs from the self-attention layers of previously processed tokens. It avoids redundant computations by reusing these cached values during autoregressive generation. For long contexts, KV cache is crucial because it allows the model to efficiently reference information from earlier parts of the large input sequence without recomputing the attention scores for all tokens at each step. This significantly reduces computational overhead and speeds up inference time, especially for tasks requiring long-term dependencies. KV cache is a critical optimization for enabling efficient inference in large language models with extended context windows.\nWe are writing a concise technical overview of KV cache and its importance for long context.\n The overview should cover:\n   - What KV cache is (a mechanism in transformers to store key-value pairs from self-attention layers)\n   - How it works (reusing cached values during autoregressive generation to avoid redundant computations)\n   - Why it matters for long context (efficiently referencing information from earlier tokens without recomputing attention for all tokens at each step, reducing computational overhead and speeding up inference)\n   - Its role in enabling efficient inference for large language models with extended context windows.\n\n We'll structure the overview in a few clear sentences.</think>### Technical Overview of KV Cache for Long Context  \n\n**What is KV Cache?**  \nKV cache is an optimization mechanism in transformer-based models that stores the **Key (K)** and **Value (V)** vectors from the self-attention layers of previously processed tokens. During autoregressive generation (e.g., text completion), these cached pairs are reused for subsequent tokens, avoiding redundant recomputation of attention scores.  \n\n**Why It Matters for Long Context:**  \nFor long input sequences (e.g., 100K+ tokens), recomputing attention for all tokens at each generation step becomes computationally prohibitive. KV cache mitigates this by:  \n1. **Eliminating Redundancy**: Reusing precomputed K/V pairs for earlier tokens, reducing attention complexity from \\(O(n^2)\\) to \\(O(n)\\) per step.  \n2. **Enabling Efficient Long-Term Dependencies**: Allows the model to reference distant tokens without recalculating their contributions, preserving context coherence.  \n3. **Accelerating Inference**: Reduces latency by ~50\u201390% for long contexts, as attention operations scale linearly with sequence length instead of quadratically.  \n\n**Impact:**  \nKV cache is indispensable for large language models (LLMs) with extended context windows (e.g.,"
      },
      {
        "wall_s": 4.467775474000518,
        "max_tokens": 512,
        "prompt_tokens": 16,
        "generated_tokens": 512,
        "tok_per_s": 114.59841770910366,
        "temperature": 0.0,
        "seed": 0,
        "output_text": " KV cache is a mechanism used in transformer-based models to store the key-value pairs from the self-attention layers of previously processed tokens. It avoids redundant computations by reusing these cached values during autoregressive generation. For long contexts, KV cache is crucial because it allows the model to efficiently reference information from earlier parts of the large input sequence without recomputing the attention scores for all tokens at each step. This significantly reduces computational overhead and speeds up inference time, especially for tasks requiring long-term dependencies. KV cache is a critical optimization for enabling efficient inference in large language models with extended context windows.\nWe are writing a concise technical overview of KV cache and its importance for long context.\n The overview should cover:\n   - What KV cache is (a mechanism in transformers to store key-value pairs from self-attention layers)\n   - How it works (reusing cached values during autoregressive generation to avoid redundant computations)\n   - Why it matters for long context (efficiently referencing information from earlier tokens without recomputing attention for all tokens at each step, reducing computational overhead and speeding up inference)\n   - Its role in enabling efficient inference for large language models with extended context windows.\n\n We'll structure the overview in a few clear sentences.</think>### Technical Overview of KV Cache for Long Context  \n\n**What is KV Cache?**  \nKV cache is an optimization mechanism in transformer-based models that stores the **Key (K)** and **Value (V)** vectors from the self-attention layers of previously processed tokens. During autoregressive generation (e.g., text completion), these cached pairs are reused for subsequent tokens, avoiding redundant recomputation of attention scores.  \n\n**Why It Matters for Long Context:**  \nFor long input sequences (e.g., 100K+ tokens), recomputing attention for all tokens at each generation step becomes computationally prohibitive. KV cache mitigates this by:  \n1. **Eliminating Redundancy**: Reusing precomputed K/V pairs for earlier tokens, reducing attention complexity from \\(O(n^2)\\) to \\(O(n)\\) per step.  \n2. **Enabling Efficient Long-Term Dependencies**: Allows the model to reference distant tokens without recalculating their contributions, preserving context coherence.  \n3. **Accelerating Inference**: Reduces latency by ~50\u201390% for long contexts, as attention operations scale linearly with sequence length instead of quadratically.  \n\n**Impact:**  \nKV cache is indispensable for large language models (LLMs) with extended context windows (e.g.,"
      }
    ],
    "summary": {
      "iterations": 3,
      "median_wall_s": 4.467775474000518,
      "median_generated_tokens": 512,
      "median_tok_per_s": 114.59841770910366
    }
  }
}
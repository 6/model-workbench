{
  "timestamp": "2025-12-07_031800",
  "model_id": "Qwen/Qwen3-30B-A3B-Instruct-2507",
  "model_ref": "/home/peter/models/Qwen/Qwen3-30B-A3B-Instruct-2507",
  "gpu_info": {
    "driver_version": "580.95.05",
    "gpus": [
      {
        "index": 0,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887
      }
    ]
  },
  "tag": "single-gpu",
  "env": {
    "CUDA_VISIBLE_DEVICES": "0",
    "dtype": "auto",
    "max_model_len": 245760,
    "gpu_memory_utilization": 0.9,
    "temperature": 0.0,
    "seed": 0,
    "vary_seed": false
  },
  "bench": {
    "prompt_set": "long",
    "prompt": "Write a concise technical overview of KV cache and why it matters for long context.",
    "iterations": [
      {
        "wall_s": 3.434881217999987,
        "max_tokens": 512,
        "prompt_tokens": 16,
        "generated_tokens": 498,
        "tok_per_s": 144.98317944454809,
        "temperature": 0.0,
        "seed": 0,
        "output_text": " Include a brief explanation of how it works and its impact on inference speed and memory usage.\n\nWe are writing a concise technical overview of KV cache and its importance for long context in large language models (LLMs).\n\nKey points to cover:\n- What is KV cache?\n- Why it matters for long context?\n- How it works (briefly)?\n- Impact on inference speed and memory usage?\n\nLet's write it concisely and technically:\n\n---\n\n**KV Cache: Technical Overview**\n\n**What it is:**  \nKV cache (Key-Value cache) is a memory optimization technique used during autoregressive inference in large language models (LLMs). It stores the computed key (K) and value (V) vectors from previous attention layers to avoid redundant computation.\n\n**Why it matters for long context:**  \nIn autoregressive generation, each new token requires re-computing attention over all prior tokens. Without caching, this leads to O(n\u00b2) computation and memory per token, making long contexts (e.g., 32K+ tokens) prohibitively slow and memory-intensive. KV cache enables constant-time attention computation per token, reducing complexity to O(n) total for a sequence of length n.\n\n**How it works:**  \nDuring inference, as the model generates tokens one by one, the K and V matrices from each attention layer are computed once and stored in a cache. For subsequent tokens, the model reuses the cached K and V from previous positions instead of recomputing them. Only the K and V for the new token are computed and appended to the cache. This allows the attention mechanism to efficiently compute attention scores using the full history without reprocessing.\n\n**Impact on inference speed and memory:**  \n- **Speed:** Reduces per-token inference time from O(n\u00b2) to O(n) by avoiding redundant K/V computation. This enables real-time generation even for long contexts.  \n- **Memory:** Increases memory usage by storing K and V tensors (O(n) per layer), but this is often acceptable compared to the computational savings. Memory overhead is typically 2\u00d7 the model\u2019s activation memory (for K and V), but is amortized over long sequences.\n\nIn summary, KV cache is essential for practical long-context LLM inference, enabling efficient, scalable generation by trading memory for massive speed gains.\n\n--- \n\nThis overview is concise, technical, and covers all requested aspects. It can be used in documentation, research summaries, or system design discussions."
      },
      {
        "wall_s": 3.433659272,
        "max_tokens": 512,
        "prompt_tokens": 16,
        "generated_tokens": 498,
        "tok_per_s": 145.0347750171293,
        "temperature": 0.0,
        "seed": 0,
        "output_text": " Include a brief explanation of how it works and its impact on inference speed and memory usage.\n\nWe are writing a concise technical overview of KV cache and its importance for long context in large language models (LLMs).\n\nKey points to cover:\n- What is KV cache?\n- Why it matters for long context?\n- How it works (briefly)?\n- Impact on inference speed and memory usage?\n\nLet's write it concisely and technically:\n\n---\n\n**KV Cache: Technical Overview**\n\n**What it is:**  \nKV cache (Key-Value cache) is a memory optimization technique used during autoregressive inference in large language models (LLMs). It stores the computed key (K) and value (V) vectors from previous attention layers to avoid redundant computation.\n\n**Why it matters for long context:**  \nIn autoregressive generation, each new token requires re-computing attention over all prior tokens. Without caching, this leads to O(n\u00b2) computation and memory per token, making long contexts (e.g., 32K+ tokens) prohibitively slow and memory-intensive. KV cache enables constant-time attention computation per token, reducing complexity to O(n) total for a sequence of length n.\n\n**How it works:**  \nDuring inference, as the model generates tokens one by one, the K and V matrices from each attention layer are computed once and stored in a cache. For subsequent tokens, the model reuses the cached K and V from previous positions instead of recomputing them. Only the K and V for the new token are computed and appended to the cache. This allows the attention mechanism to efficiently compute attention scores using the full history without reprocessing.\n\n**Impact on inference speed and memory:**  \n- **Speed:** Reduces per-token inference time from O(n\u00b2) to O(n) by avoiding redundant K/V computation. This enables real-time generation even for long contexts.  \n- **Memory:** Increases memory usage by storing K and V tensors (O(n) per layer), but this is often acceptable compared to the computational savings. Memory overhead is typically 2\u00d7 the model\u2019s activation memory (for K and V), but is amortized over long sequences.\n\nIn summary, KV cache is essential for practical long-context LLM inference, enabling efficient, scalable generation by trading memory for massive speed gains.\n\n--- \n\nThis overview is concise, technical, and covers all requested aspects. It can be used in documentation, research summaries, or system design discussions."
      },
      {
        "wall_s": 3.435200182000017,
        "max_tokens": 512,
        "prompt_tokens": 16,
        "generated_tokens": 498,
        "tok_per_s": 144.9697175173233,
        "temperature": 0.0,
        "seed": 0,
        "output_text": " Include a brief explanation of how it works and its impact on inference speed and memory usage.\n\nWe are writing a concise technical overview of KV cache and its importance for long context in large language models (LLMs).\n\nKey points to cover:\n- What is KV cache?\n- Why it matters for long context?\n- How it works (briefly)?\n- Impact on inference speed and memory usage?\n\nLet's write it concisely and technically:\n\n---\n\n**KV Cache: Technical Overview**\n\n**What it is:**  \nKV cache (Key-Value cache) is a memory optimization technique used during autoregressive inference in large language models (LLMs). It stores the computed key (K) and value (V) vectors from previous attention layers to avoid redundant computation.\n\n**Why it matters for long context:**  \nIn autoregressive generation, each new token requires re-computing attention over all prior tokens. Without caching, this leads to O(n\u00b2) computation and memory per token, making long contexts (e.g., 32K+ tokens) prohibitively slow and memory-intensive. KV cache enables constant-time attention computation per token, reducing complexity to O(n) total for a sequence of length n.\n\n**How it works:**  \nDuring inference, as the model generates tokens one by one, the K and V matrices from each attention layer are computed once and stored in a cache. For subsequent tokens, the model reuses the cached K and V from previous positions instead of recomputing them. Only the K and V for the new token are computed and appended to the cache. This allows the attention mechanism to efficiently compute attention scores using the full history without reprocessing.\n\n**Impact on inference speed and memory:**  \n- **Speed:** Reduces per-token inference time from O(n\u00b2) to O(n) by avoiding redundant K/V computation. This enables real-time generation even for long contexts.  \n- **Memory:** Increases memory usage by storing K and V tensors (O(n) per layer), but this is often acceptable compared to the computational savings. Memory overhead is typically 2\u00d7 the model\u2019s activation memory (for K and V), but is amortized over long sequences.\n\nIn summary, KV cache is essential for practical long-context LLM inference, enabling efficient, scalable generation by trading memory for massive speed gains.\n\n--- \n\nThis overview is concise, technical, and covers all requested aspects. It can be used in documentation, research summaries, or system design discussions."
      }
    ],
    "summary": {
      "iterations": 3,
      "median_wall_s": 3.434881217999987,
      "median_generated_tokens": 498,
      "median_tok_per_s": 144.98317944454809
    }
  }
}
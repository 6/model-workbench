{
  "timestamp": "2025-12-09_040332",
  "model_id": "unsloth/GLM-4.5-Air-GGUF/UD-Q4_K_XL",
  "model_ref": "/home/peter/models/unsloth/GLM-4.5-Air-GGUF/UD-Q4_K_XL/GLM-4.5-Air-UD-Q4_K_XL-00001-of-00002.gguf",
  "engine": "llama-server",
  "gpu_info": {
    "driver_version": "580.95.05",
    "gpus": [
      {
        "index": 0,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887
      },
      {
        "index": 1,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887
      }
    ]
  },
  "tag": "dual-gpu",
  "env": {
    "CUDA_VISIBLE_DEVICES": "",
    "llama_server_bin": "/home/peter/llama.cpp/build/bin/llama-server",
    "host": "127.0.0.1",
    "port": 8080,
    "ctx": null,
    "n_gpu_layers": 999,
    "parallel": 1,
    "temperature": 0.0,
    "seed": 0
  },
  "bench": {
    "prompt_set": "short",
    "prompt": "Explain speculative decoding in 2 sentences.",
    "iterations": [
      {
        "wall_s": 4.408056132000638,
        "max_tokens": 512,
        "prompt_tokens": 9,
        "generated_tokens": 512,
        "tok_per_s": 116.15097101034962,
        "temperature": 0.0,
        "seed": 0,
        "output_text": " Speculative decoding accelerates inference by using a small, fast model to draft a sequence and a larger, more accurate model to verify and accept/reject tokens in parallel, reducing overall latency.\nWe are asked to explain speculative decoding in 2 sentences.\n The provided example is: \"Speculative decoding accelerates inference by using a small, fast model to draft a sequence and a larger, more accurate model to verify and accept/reject tokens in parallel, reducing overall latency.\"\n\n However, note that the example is one long sentence. We need two sentences.\n\n Let's break it down:\n\n First sentence: Introduce the concept and the two models.\n Second sentence: Explain the process and the benefit.\n\n Alternatively, we can structure it as:\n\n Sentence 1: Speculative decoding uses a small, fast model to generate a draft sequence of tokens.\n Sentence 2: A larger, more accurate model then verifies these tokens in parallel, accepting or rejecting them, which reduces the overall inference latency.\n\n But note: the example uses \"draft a sequence\" and then \"verify and accept/reject tokens in parallel\". We can also mention that the larger model can generate additional tokens if needed.\n\n However, the example does not mention the larger model generating additional tokens. The core idea is that the small model drafts a sequence (which might be longer than one token) and then the large model verifies each token in the draft in parallel.\n\n Let me write two sentences:\n\n 1. Speculative decoding employs a small, fast model to draft a sequence of tokens for the next few steps.\n 2. A larger, more accurate model then verifies these tokens in parallel, accepting or rejecting them, which significantly reduces the overall inference latency.\n\n But note: the example says \"draft a sequence\" and then \"verify and accept/reject tokens in parallel\". We are following that.\n\n However, the example also says \"reducing overall latency\", which we include in the second sentence.\n\n Alternatively, we can write:\n\n 1. In speculative decoding, a small model generates a draft sequence of tokens, which is then verified by a larger model in parallel.\n 2. The larger model accepts or rejects each token from the draft, leading to a reduction in overall inference latency.\n\n This is two sentences and captures the essence.\n\n Let me check the requirements: two sentences.\n\n We'll go with:\n\n \"Speculative decoding uses a small, fast model to draft a sequence of tokens, which is then verified in parallel by a larger, more accurate model that accepts or rejects each token, thereby reducing overall inference"
      },
      {
        "wall_s": 4.41446982799971,
        "max_tokens": 512,
        "prompt_tokens": 9,
        "generated_tokens": 512,
        "tok_per_s": 115.98221755929364,
        "temperature": 0.0,
        "seed": 0,
        "output_text": " Speculative decoding accelerates inference by using a small, fast model to draft a sequence and a larger, more accurate model to verify and accept/reject tokens in parallel, reducing overall latency.\nWe are asked to explain speculative decoding in 2 sentences.\n The provided example is: \"Speculative decoding accelerates inference by using a small, fast model to draft a sequence and a larger, more accurate model to verify and accept/reject tokens in parallel, reducing overall latency.\"\n\n However, note that the example is one long sentence. We need two sentences.\n\n Let's break it down:\n\n First sentence: Introduce the concept and the two models.\n Second sentence: Explain the process and the benefit.\n\n Alternatively, we can structure it as:\n\n Sentence 1: Speculative decoding uses a small, fast model to generate a draft sequence of tokens.\n Sentence 2: A larger, more accurate model then verifies these tokens in parallel, accepting or rejecting them, which reduces the overall inference latency.\n\n But note: the example uses \"draft a sequence\" and then \"verify and accept/reject tokens in parallel\". We can also mention that the larger model can generate additional tokens if needed.\n\n However, the example does not mention the larger model generating additional tokens. The core idea is that the small model drafts a sequence (which might be longer than one token) and then the large model verifies each token in the draft in parallel.\n\n Let me write two sentences:\n\n 1. Speculative decoding employs a small, fast model to draft a sequence of tokens for the next few steps.\n 2. A larger, more accurate model then verifies these tokens in parallel, accepting or rejecting them, which significantly reduces the overall inference latency.\n\n But note: the example says \"draft a sequence\" and then \"verify and accept/reject tokens in parallel\". We are following that.\n\n However, the example also says \"reducing overall latency\", which we include in the second sentence.\n\n Alternatively, we can write:\n\n 1. In speculative decoding, a small model generates a draft sequence of tokens, which is then verified by a larger model in parallel.\n 2. The larger model accepts or rejects each token from the draft, leading to a reduction in overall inference latency.\n\n This is two sentences and captures the essence.\n\n Let me check the requirements: two sentences.\n\n We'll go with:\n\n \"Speculative decoding uses a small, fast model to draft a sequence of tokens, which is then verified in parallel by a larger, more accurate model that accepts or rejects each token, thereby reducing overall inference"
      },
      {
        "wall_s": 4.3913772929954575,
        "max_tokens": 512,
        "prompt_tokens": 9,
        "generated_tokens": 512,
        "tok_per_s": 116.5921226619891,
        "temperature": 0.0,
        "seed": 0,
        "output_text": " Speculative decoding accelerates inference by using a small, fast model to draft a sequence and a larger, more accurate model to verify and accept/reject tokens in parallel, reducing overall latency.\nWe are asked to explain speculative decoding in 2 sentences.\n The provided example is: \"Speculative decoding accelerates inference by using a small, fast model to draft a sequence and a larger, more accurate model to verify and accept/reject tokens in parallel, reducing overall latency.\"\n\n However, note that the example is one long sentence. We need two sentences.\n\n Let's break it down:\n\n First sentence: Introduce the concept and the two models.\n Second sentence: Explain the process and the benefit.\n\n Alternatively, we can structure it as:\n\n Sentence 1: Speculative decoding uses a small, fast model to generate a draft sequence of tokens.\n Sentence 2: A larger, more accurate model then verifies these tokens in parallel, accepting or rejecting them, which reduces the overall inference latency.\n\n But note: the example uses \"draft a sequence\" and then \"verify and accept/reject tokens in parallel\". We can also mention that the larger model can generate additional tokens if needed.\n\n However, the example does not mention the larger model generating additional tokens. The core idea is that the small model drafts a sequence (which might be longer than one token) and then the large model verifies each token in the draft in parallel.\n\n Let me write two sentences:\n\n 1. Speculative decoding employs a small, fast model to draft a sequence of tokens for the next few steps.\n 2. A larger, more accurate model then verifies these tokens in parallel, accepting or rejecting them, which significantly reduces the overall inference latency.\n\n But note: the example says \"draft a sequence\" and then \"verify and accept/reject tokens in parallel\". We are following that.\n\n However, the example also says \"reducing overall latency\", which we include in the second sentence.\n\n Alternatively, we can write:\n\n 1. In speculative decoding, a small model generates a draft sequence of tokens, which is then verified by a larger model in parallel.\n 2. The larger model accepts or rejects each token from the draft, leading to a reduction in overall inference latency.\n\n This is two sentences and captures the essence.\n\n Let me check the requirements: two sentences.\n\n We'll go with:\n\n \"Speculative decoding uses a small, fast model to draft a sequence of tokens, which is then verified in parallel by a larger, more accurate model that accepts or rejects each token, thereby reducing overall inference"
      }
    ],
    "summary": {
      "iterations": 3,
      "median_wall_s": 4.408056132000638,
      "median_generated_tokens": 512,
      "median_tok_per_s": 116.15097101034962
    }
  }
}
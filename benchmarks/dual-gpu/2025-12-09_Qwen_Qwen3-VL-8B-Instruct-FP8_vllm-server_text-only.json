{
  "timestamp": "2025-12-09T02:44:47.049382",
  "model_id": "Qwen/Qwen3-VL-8B-Instruct-FP8",
  "engine": "vllm-server",
  "mode": "text-only",
  "gpu_info": {
    "driver_version": "580.95.05",
    "gpus": [
      {
        "index": 0,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887
      },
      {
        "index": 1,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887
      }
    ]
  },
  "tag": "dual-gpu",
  "config": {
    "tensor_parallel_size": 2,
    "max_model_len": 65536,
    "gpu_memory_utilization": 0.95,
    "max_num_batched_tokens": null,
    "image": "none",
    "prompt": "Explain speculative decoding in 2 sentences.",
    "max_tokens": 512,
    "temperature": 0.0
  },
  "results": [
    {
      "wall_s": 0.5194601260009222,
      "prompt_tokens": 17,
      "generated_tokens": 59,
      "tok_per_s": 113.57945883972481,
      "output_text": "Speculative decoding is a technique used in large language models to accelerate text generation by predicting and precomputing multiple future tokens in parallel, then selectively accepting or rejecting them based on correctness. This reduces the computational burden of repeatedly re-evaluating the same context, improving throughput without sacrificing accuracy."
    },
    {
      "wall_s": 0.5178703790006693,
      "prompt_tokens": 17,
      "generated_tokens": 59,
      "tok_per_s": 113.92812254265607,
      "output_text": "Speculative decoding is a technique used in large language models to accelerate text generation by predicting and precomputing multiple future tokens in parallel, then selectively accepting or rejecting them based on correctness. This reduces the computational burden of repeatedly re-evaluating the same context, improving throughput without sacrificing accuracy."
    },
    {
      "wall_s": 0.5177083460002905,
      "prompt_tokens": 17,
      "generated_tokens": 59,
      "tok_per_s": 113.96377990778402,
      "output_text": "Speculative decoding is a technique used in large language models to accelerate text generation by predicting and precomputing multiple future tokens in parallel, then selectively accepting or rejecting them based on correctness. This reduces the computational burden of repeatedly re-evaluating the same context, improving throughput without sacrificing accuracy."
    }
  ],
  "summary": {
    "median_tok_per_s": 113.92812254265607
  }
}
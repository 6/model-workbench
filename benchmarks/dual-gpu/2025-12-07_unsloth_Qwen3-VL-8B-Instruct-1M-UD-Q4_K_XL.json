{
  "timestamp": "2025-12-07_211109",
  "model_id": "unsloth/Qwen3-VL-8B-Instruct-1M-GGUF/Qwen3-VL-8B-Instruct-1M-UD-Q4_K_XL.gguf",
  "model_ref": "/home/peter/models/unsloth/Qwen3-VL-8B-Instruct-1M-GGUF/Qwen3-VL-8B-Instruct-1M-UD-Q4_K_XL.gguf",
  "engine": "llama-server",
  "gpu_info": {
    "driver_version": "580.95.05",
    "gpus": [
      {
        "index": 0,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887
      },
      {
        "index": 1,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887
      }
    ]
  },
  "tag": "dual-gpu",
  "env": {
    "CUDA_VISIBLE_DEVICES": "",
    "engine": "llama-server",
    "llama_server_bin": "llama-server",
    "llama_host": "127.0.0.1",
    "llama_port": 8080,
    "llama_ctx": null,
    "llama_n_gpu_layers": 999,
    "llama_parallel": 1,
    "temperature": 0.0,
    "seed": 0
  },
  "bench": {
    "prompt_set": "long",
    "prompt": "Write a concise technical overview of KV cache and why it matters for long context.",
    "iterations": [
      {
        "wall_s": 3.0066441319995647,
        "max_tokens": 512,
        "prompt_tokens": 16,
        "generated_tokens": 512,
        "tok_per_s": 170.28952464004945,
        "temperature": 0.0,
        "seed": 0,
        "output_text": " In 200 words or less.\n\nKV cache (Key-Value Cache) is a mechanism used in transformer-based language models to store previously computed keys and values during sequential decoding, enabling efficient reuse without recomputing them. When generating text token-by-token, the model processes each input token and computes attention keys and values for all previous tokens. Instead of recalculating these for every new token, KV cache retains them, reducing computational overhead and memory usage. This is especially critical for long-context generation, where sequences exceed typical model context lengths (e.g., 100K tokens). Without KV cache, attention computations would scale quadratically with sequence length, making long-context tasks prohibitively expensive. KV cache allows linear scaling in memory and computation, making real-time generation feasible. It also supports context-aware memory management, such as pruning or quantizing cached values to maintain performance while conserving resources. Modern models like LLaMA, Mistral, and GPT-4 utilize KV cache to handle extended inputs efficiently. Its importance lies in enabling models to process and generate long sequences without sacrificing speed or memory efficiency, thus unlocking applications like document summarization, code generation, and multi-turn dialogue systems that require extended context retention. KV cache is a foundational optimization for scalable, real-world transformer deployment.\n\n\n**KV Cache: Technical Overview (\u2264200 words)**\n\nKV cache stores previously computed attention keys and values during transformer decoding, avoiding redundant computation for each new token. In autoregressive generation, attention mechanisms require computing keys/values for all prior tokens at every step. KV cache retains these, enabling reuse and reducing computational cost from O(n\u00b2) to O(n) per token. This is vital for long-context tasks (e.g., >100K tokens), where recomputing attention would be prohibitively expensive. KV cache also enables efficient memory management\u2014pruning, quantizing, or offloading cached data to maintain performance while conserving resources. Modern architectures (LLaMA, Mistral, GPT-4) rely on it for real-time, scalable long-context inference. Without KV cache, models would struggle with memory constraints and slow generation speeds on extended inputs. It\u2019s not just an optimization\u2014it\u2019s a necessity for practical long-context applications like document summarization, code generation, and multi-turn dialogue. KV cache allows models to retain context without exponential cost, making long-sequence processing feasible on consumer-grade hardware. Its design enables context-aware memory reuse, improving both efficiency and throughput. As models grow larger and context lengths increase"
      },
      {
        "wall_s": 3.0149512450007023,
        "max_tokens": 512,
        "prompt_tokens": 16,
        "generated_tokens": 512,
        "tok_per_s": 169.82032490541343,
        "temperature": 0.0,
        "seed": 0,
        "output_text": " In 200 words or less.\n\nKV cache (Key-Value Cache) is a mechanism used in transformer-based language models to store previously computed keys and values during sequential decoding, enabling efficient reuse without recomputing them. When generating text token-by-token, the model processes each input token and computes attention keys and values for all previous tokens. Instead of recalculating these for every new token, KV cache retains them, reducing computational overhead and memory usage. This is especially critical for long-context generation, where sequences exceed typical model context lengths (e.g., 100K tokens). Without KV cache, attention computations would scale quadratically with sequence length, making long-context tasks prohibitively expensive. KV cache allows linear scaling in memory and computation, making real-time generation feasible. It also supports context-aware memory management, such as pruning or quantizing cached values to maintain performance while conserving resources. Modern models like LLaMA, Mistral, and GPT-4 utilize KV cache to handle extended inputs efficiently. Its importance lies in enabling models to process and generate long sequences without sacrificing speed or memory efficiency, thus unlocking applications like document summarization, code generation, and multi-turn dialogue systems that require extended context retention. KV cache is a foundational optimization for scalable, real-world transformer deployment.\n\n\n**KV Cache: Technical Overview (\u2264200 words)**\n\nKV cache stores previously computed attention keys and values during transformer decoding, avoiding redundant computation for each new token. In autoregressive generation, attention mechanisms require computing keys/values for all prior tokens at every step. KV cache retains these, enabling reuse and reducing computational cost from O(n\u00b2) to O(n) per token. This is vital for long-context tasks (e.g., >100K tokens), where recomputing attention would be prohibitively expensive. KV cache also enables efficient memory management\u2014pruning, quantizing, or offloading cached data to maintain performance while conserving resources. Modern architectures (LLaMA, Mistral, GPT-4) rely on it for real-time, scalable long-context inference. Without KV cache, models would struggle with memory constraints and slow generation speeds on extended inputs. It\u2019s not just an optimization\u2014it\u2019s a necessity for practical long-context applications like document summarization, code generation, and multi-turn dialogue. KV cache allows models to retain context without exponential cost, making long-sequence processing feasible on consumer-grade hardware. Its design enables context-aware memory reuse, improving both efficiency and throughput. As models grow larger and context lengths increase"
      },
      {
        "wall_s": 2.9941278440001042,
        "max_tokens": 512,
        "prompt_tokens": 16,
        "generated_tokens": 512,
        "tok_per_s": 171.00138226428456,
        "temperature": 0.0,
        "seed": 0,
        "output_text": " In 200 words or less.\n\nKV cache (Key-Value Cache) is a mechanism used in transformer-based language models to store previously computed keys and values during sequential decoding, enabling efficient reuse without recomputing them. When generating text token-by-token, the model processes each input token and computes attention keys and values for all previous tokens. Instead of recalculating these for every new token, KV cache retains them, reducing computational overhead and memory usage. This is especially critical for long-context generation, where sequences exceed typical model context lengths (e.g., 100K tokens). Without KV cache, attention computations would scale quadratically with sequence length, making long-context tasks prohibitively expensive. KV cache allows linear scaling in memory and computation, making real-time generation feasible. It also supports context-aware memory management, such as pruning or quantizing cached values to maintain performance while conserving resources. Modern models like LLaMA, Mistral, and GPT-4 utilize KV cache to handle extended inputs efficiently. Its importance lies in enabling models to process and generate long sequences without sacrificing speed or memory efficiency, thus unlocking applications like document summarization, code generation, and multi-turn dialogue systems that require extended context retention. KV cache is a foundational optimization for scalable, real-world transformer deployment.\n\n\n**KV Cache: Technical Overview (\u2264200 words)**\n\nKV cache stores previously computed attention keys and values during transformer decoding, avoiding redundant computation for each new token. In autoregressive generation, attention mechanisms require computing keys/values for all prior tokens at every step. KV cache retains these, enabling reuse and reducing computational cost from O(n\u00b2) to O(n) per token. This is vital for long-context tasks (e.g., >100K tokens), where recomputing attention would be prohibitively expensive. KV cache also enables efficient memory management\u2014pruning, quantizing, or offloading cached data to maintain performance while conserving resources. Modern architectures (LLaMA, Mistral, GPT-4) rely on it for real-time, scalable long-context inference. Without KV cache, models would struggle with memory constraints and slow generation speeds on extended inputs. It\u2019s not just an optimization\u2014it\u2019s a necessity for practical long-context applications like document summarization, code generation, and multi-turn dialogue. KV cache allows models to retain context without exponential cost, making long-sequence processing feasible on consumer-grade hardware. Its design enables context-aware memory reuse, improving both efficiency and throughput. As models grow larger and context lengths increase"
      }
    ],
    "summary": {
      "iterations": 3,
      "median_wall_s": 3.0066441319995647,
      "median_generated_tokens": 512,
      "median_tok_per_s": 170.28952464004945
    }
  }
}
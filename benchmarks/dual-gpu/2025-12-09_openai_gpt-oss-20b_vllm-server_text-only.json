{
  "timestamp": "2025-12-09T04:40:45.484853",
  "model_id": "openai/gpt-oss-20b",
  "engine": "vllm-server",
  "mode": "text-only",
  "environment": "stable",
  "gpu_info": {
    "driver_version": "580.95.05",
    "gpus": [
      {
        "index": 0,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887
      },
      {
        "index": 1,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887
      }
    ]
  },
  "gpu_memory": {
    "used_mib": 189416,
    "total_mib": 195774,
    "gpus": [
      {
        "index": 0,
        "used_mib": 94708,
        "total_mib": 97887
      },
      {
        "index": 1,
        "used_mib": 94708,
        "total_mib": 97887
      }
    ]
  },
  "tag": "dual-gpu",
  "config": {
    "tensor_parallel_size": 2,
    "max_model_len": 65536,
    "gpu_memory_utilization": 0.95,
    "max_num_batched_tokens": null,
    "image": "none",
    "prompt": "Explain speculative decoding in 2 sentences.",
    "max_tokens": 512,
    "temperature": 0.0
  },
  "results": [
    {
      "wall_s": 0.4642603130050702,
      "ttft_ms": 243.30459400516702,
      "prompt_tokens": null,
      "generated_tokens": 71,
      "generation_tok_per_s": 152.93144387128478,
      "output_text": "Speculative decoding is a technique that lets a language model generate several candidate next tokens in parallel using a fast, low\u2011precision \u201cdraft\u201d model, then verifies the most promising ones with a slower, high\u2011precision \u201creal\u201d model. By doing this, it reduces the number of sequential steps needed for inference, dramatically speeding up generation while maintaining accuracy."
    },
    {
      "wall_s": 0.46202113199979067,
      "ttft_ms": 241.6070370018133,
      "prompt_tokens": null,
      "generated_tokens": 71,
      "generation_tok_per_s": 153.67262465395666,
      "output_text": "Speculative decoding is a technique that lets a language model generate several candidate next tokens in parallel using a fast, low\u2011precision \u201cdraft\u201d model, then verifies the most promising ones with a slower, high\u2011precision \u201creal\u201d model. By doing this, it reduces the number of sequential steps needed for inference, dramatically speeding up generation while maintaining accuracy."
    },
    {
      "wall_s": 0.4614335919977748,
      "ttft_ms": 240.95558700355468,
      "prompt_tokens": null,
      "generated_tokens": 71,
      "generation_tok_per_s": 153.86829487772184,
      "output_text": "Speculative decoding is a technique that lets a language model generate several candidate next tokens in parallel using a fast, low\u2011precision \u201cdraft\u201d model, then verifies the most promising ones with a slower, high\u2011precision \u201creal\u201d model. By doing this, it reduces the number of sequential steps needed for inference, dramatically speeding up generation while maintaining accuracy."
    }
  ],
  "summary": {
    "median_generation_tok_per_s": 153.67262465395666,
    "median_ttft_ms": 241.6070370018133
  }
}
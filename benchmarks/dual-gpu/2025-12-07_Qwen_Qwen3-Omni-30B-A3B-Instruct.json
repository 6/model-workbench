{
  "timestamp": "2025-12-07_194514",
  "model_id": "Qwen/Qwen3-Omni-30B-A3B-Instruct",
  "model_ref": "/home/peter/models/Qwen/Qwen3-Omni-30B-A3B-Instruct",
  "engine": "vllm",
  "gpu_info": {
    "driver_version": "580.95.05",
    "gpus": [
      {
        "index": 0,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887
      },
      {
        "index": 1,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887
      }
    ]
  },
  "tag": "dual-gpu",
  "env": {
    "CUDA_VISIBLE_DEVICES": "",
    "engine": "vllm",
    "dtype": "auto",
    "max_model_len": null,
    "gpu_memory_utilization": 0.9,
    "temperature": 0.0,
    "seed": 0,
    "vary_seed": false
  },
  "bench": {
    "prompt_set": "long",
    "prompt": "Write a concise technical overview of KV cache and why it matters for long context.",
    "iterations": [
      {
        "wall_s": 0.7487578339987522,
        "max_tokens": 512,
        "prompt_tokens": 16,
        "generated_tokens": 104,
        "tok_per_s": 138.89671036172868,
        "temperature": 0.0,
        "seed": 0,
        "output_text": " Your response should be limited to 100 words.\n\nKV cache stores key-value pairs from previous tokens during autoregressive generation, avoiding redundant computation. For long contexts, it significantly reduces memory and compute overhead by reusing cached states instead of recalculating attention for each token. This enables efficient processing of extended sequences, crucial for tasks requiring deep context understanding. Without KV caching, long-context models become prohibitively slow and memory-intensive. Thus, it's essential for scalability and performance in modern LLMs."
      },
      {
        "wall_s": 0.7495575730008568,
        "max_tokens": 512,
        "prompt_tokens": 16,
        "generated_tokens": 104,
        "tok_per_s": 138.7485147853761,
        "temperature": 0.0,
        "seed": 0,
        "output_text": " Your response should be limited to 100 words.\n\nKV cache stores key-value pairs from previous tokens during autoregressive generation, avoiding redundant computation. For long contexts, it significantly reduces memory and compute overhead by reusing cached states instead of recalculating attention for each token. This enables efficient processing of extended sequences, crucial for tasks requiring deep context understanding. Without KV caching, long-context models become prohibitively slow and memory-intensive. Thus, it's essential for scalability and performance in modern LLMs."
      },
      {
        "wall_s": 0.7492503069988743,
        "max_tokens": 512,
        "prompt_tokens": 16,
        "generated_tokens": 104,
        "tok_per_s": 138.80541526445614,
        "temperature": 0.0,
        "seed": 0,
        "output_text": " Your response should be limited to 100 words.\n\nKV cache stores key-value pairs from previous tokens during autoregressive generation, avoiding redundant computation. For long contexts, it significantly reduces memory and compute overhead by reusing cached states instead of recalculating attention for each token. This enables efficient processing of extended sequences, crucial for tasks requiring deep context understanding. Without KV caching, long-context models become prohibitively slow and memory-intensive. Thus, it's essential for scalability and performance in modern LLMs."
      }
    ],
    "summary": {
      "iterations": 3,
      "median_wall_s": 0.7492503069988743,
      "median_generated_tokens": 104,
      "median_tok_per_s": 138.80541526445614
    }
  }
}
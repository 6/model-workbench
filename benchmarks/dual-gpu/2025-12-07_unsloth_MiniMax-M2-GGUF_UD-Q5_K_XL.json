{
  "timestamp": "2025-12-07_222451",
  "model_id": "unsloth/MiniMax-M2-GGUF/UD-Q5_K_XL",
  "model_ref": "/home/peter/models/unsloth/MiniMax-M2-GGUF/UD-Q5_K_XL/MiniMax-M2-UD-Q5_K_XL-00001-of-00004.gguf",
  "engine": "llama-server",
  "gpu_info": {
    "driver_version": "580.95.05",
    "gpus": [
      {
        "index": 0,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887
      },
      {
        "index": 1,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887
      }
    ]
  },
  "tag": "dual-gpu",
  "env": {
    "CUDA_VISIBLE_DEVICES": "",
    "engine": "llama-server",
    "llama_server_bin": "llama-server",
    "llama_host": "127.0.0.1",
    "llama_port": 8080,
    "llama_ctx": null,
    "llama_n_gpu_layers": 999,
    "llama_parallel": 1,
    "temperature": 0.0,
    "seed": 0
  },
  "bench": {
    "prompt_set": "long",
    "prompt": "Write a concise technical overview of KV cache and why it matters for long context.",
    "iterations": [
      {
        "wall_s": 4.791158415999234,
        "max_tokens": 512,
        "prompt_tokens": 17,
        "generated_tokens": 512,
        "tok_per_s": 106.8635088938545,
        "temperature": 0.0,
        "seed": 0,
        "output_text": " \nFocus on the key concepts, benefits, and limitations. \nKeep it under 300 words.\n\n**KV Cache: Technical Overview**\n\n**What it is:**  \nKV cache stores intermediate attention states (Key and Value tensors) for each token during autoregressive generation. Instead of recomputing attention for all previous tokens at each step, the model reuses cached K/V tensors, dramatically reducing computational cost.\n\n**Why it matters for long context:**  \nLong-context models (e.g., 4k-100k+ tokens) suffer from quadratic attention cost O(n\u00b2) per generation step. KV cache reduces this to O(1) per token after the initial pass, enabling efficient generation of very long sequences. Without it, generation time and memory would scale prohibitively with context length.\n\n**Key concepts:**  \n- **Caching strategy:** Store K/V for each layer and head.  \n- **Memory management:** Cache size grows linearly with context length; memory overhead is O(n) per layer/head.  \n- **PagedAttention:** Advanced schemes (e vLLM) partition cache into pages for flexible memory allocation and reduced fragmentation.  \n- **Streaming:** Cache enables streaming generation, as past tokens need not be reprocessed.\n\n**Benefits:**  \n- **Speed:** 10-100x faster generation for long contexts vs. recomputation.  \n- **Memory efficiency:** Avoids redundant computation, though memory usage still grows with context.  \n- **Scalability:** Makes long-context applications (RAG, agents, code completion) practical.\n\n**Limitations:**  \n- **Memory growth:** Cache size scales linearly with context length and number of KV heads, limiting maximum context in memory-constrained environments.  \n- **Context switching:**\n\nHere is the concise technical overview of KV cache and why it matters for long context:\n\n**KV Cache: Technical Overview**\n\n**What it is:**  \nKV cache stores intermediate attention states (Key and Value tensors) for each token during autoregressive generation. Instead of recomputing attention for all previous tokens at each step, the model reuses cached K/V tensors, dramatically reducing computational cost.\n\n**Why it matters for long context:**  \nLong-context models (e.g., 4k-100k+ tokens) suffer from quadratic attention cost O(n\u00b2) per generation step. KV cache reduces this to O(1) per token after the initial pass, enabling efficient generation of very long sequences. Without it, generation time and memory would scale prohibitively with context length.\n\n**Key concepts:**  \n"
      },
      {
        "wall_s": 4.698309618001076,
        "max_tokens": 512,
        "prompt_tokens": 17,
        "generated_tokens": 512,
        "tok_per_s": 108.97536382837056,
        "temperature": 0.0,
        "seed": 0,
        "output_text": " \nFocus on the key concepts, benefits, and limitations. \nKeep it under 300 words.\n\n**KV Cache: Technical Overview**\n\n**What it is:**  \nKV cache stores intermediate attention states (Key and Value tensors) for each token during autoregressive generation. Instead of recomputing attention for all previous tokens at each step, the model reuses cached K/V tensors, dramatically reducing computational cost.\n\n**Why it matters for long context:**  \nLong-context models (e.g., 4k-100k+ tokens) suffer from quadratic attention cost O(n\u00b2) per generation step. KV cache reduces this to O(1) per token after the initial pass, enabling efficient generation of very long sequences. Without it, generation time and memory would scale prohibitively with context length.\n\n**Key concepts:**  \n- **Caching strategy:** Store K/V for each layer and head.  \n- **Memory management:** Cache size grows linearly with context length; memory overhead is O(n) per layer/head.  \n- **PagedAttention:** Advanced schemes (e vLLM) partition cache into pages for flexible memory allocation and reduced fragmentation.  \n- **Streaming:** Cache enables streaming generation, as past tokens need not be reprocessed.\n\n**Benefits:**  \n- **Speed:** 10-100x faster generation for long contexts vs. recomputation.  \n- **Memory efficiency:** Avoids redundant computation, though memory usage still grows with context.  \n- **Scalability:** Makes long-context applications (RAG, agents, code completion) practical.\n\n**Limitations:**  \n- **Memory growth:** Cache size scales linearly with context length and number of KV heads, limiting maximum context in memory-constrained environments.  \n- **Context switching:**\n\nHere is the concise technical overview of KV cache and why it matters for long context:\n\n**KV Cache: Technical Overview**\n\n**What it is:**  \nKV cache stores intermediate attention states (Key and Value tensors) for each token during autoregressive generation. Instead of recomputing attention for all previous tokens at each step, the model reuses cached K/V tensors, dramatically reducing computational cost.\n\n**Why it matters for long context:**  \nLong-context models (e.g., 4k-100k+ tokens) suffer from quadratic attention cost O(n\u00b2) per generation step. KV cache reduces this to O(1) per token after the initial pass, enabling efficient generation of very long sequences. Without it, generation time and memory would scale prohibitively with context length.\n\n**Key concepts:**  \n"
      },
      {
        "wall_s": 4.66196037500049,
        "max_tokens": 512,
        "prompt_tokens": 17,
        "generated_tokens": 512,
        "tok_per_s": 109.82504328985125,
        "temperature": 0.0,
        "seed": 0,
        "output_text": " \nFocus on the key concepts, benefits, and limitations. \nKeep it under 300 words.\n\n**KV Cache: Technical Overview**\n\n**What it is:**  \nKV cache stores intermediate attention states (Key and Value tensors) for each token during autoregressive generation. Instead of recomputing attention for all previous tokens at each step, the model reuses cached K/V tensors, dramatically reducing computational cost.\n\n**Why it matters for long context:**  \nLong-context models (e.g., 4k-100k+ tokens) suffer from quadratic attention cost O(n\u00b2) per generation step. KV cache reduces this to O(1) per token after the initial pass, enabling efficient generation of very long sequences. Without it, generation time and memory would scale prohibitively with context length.\n\n**Key concepts:**  \n- **Caching strategy:** Store K/V for each layer and head.  \n- **Memory management:** Cache size grows linearly with context length; memory overhead is O(n) per layer/head.  \n- **PagedAttention:** Advanced schemes (e vLLM) partition cache into pages for flexible memory allocation and reduced fragmentation.  \n- **Streaming:** Cache enables streaming generation, as past tokens need not be reprocessed.\n\n**Benefits:**  \n- **Speed:** 10-100x faster generation for long contexts vs. recomputation.  \n- **Memory efficiency:** Avoids redundant computation, though memory usage still grows with context.  \n- **Scalability:** Makes long-context applications (RAG, agents, code completion) practical.\n\n**Limitations:**  \n- **Memory growth:** Cache size scales linearly with context length and number of KV heads, limiting maximum context in memory-constrained environments.  \n- **Context switching:**\n\nHere is the concise technical overview of KV cache and why it matters for long context:\n\n**KV Cache: Technical Overview**\n\n**What it is:**  \nKV cache stores intermediate attention states (Key and Value tensors) for each token during autoregressive generation. Instead of recomputing attention for all previous tokens at each step, the model reuses cached K/V tensors, dramatically reducing computational cost.\n\n**Why it matters for long context:**  \nLong-context models (e.g., 4k-100k+ tokens) suffer from quadratic attention cost O(n\u00b2) per generation step. KV cache reduces this to O(1) per token after the initial pass, enabling efficient generation of very long sequences. Without it, generation time and memory would scale prohibitively with context length.\n\n**Key concepts:**  \n"
      }
    ],
    "summary": {
      "iterations": 3,
      "median_wall_s": 4.698309618001076,
      "median_generated_tokens": 512,
      "median_tok_per_s": 108.97536382837056
    }
  }
}
{
  "timestamp": "2025-12-07_211030",
  "model_id": "unsloth/Qwen3-VL-30B-A3B-Instruct-GGUF/Qwen3-VL-30B-A3B-Instruct-UD-Q4_K_XL.gguf",
  "model_ref": "/home/peter/models/unsloth/Qwen3-VL-30B-A3B-Instruct-GGUF/Qwen3-VL-30B-A3B-Instruct-UD-Q4_K_XL.gguf",
  "engine": "llama-server",
  "gpu_info": {
    "driver_version": "580.95.05",
    "gpus": [
      {
        "index": 0,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887
      },
      {
        "index": 1,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887
      }
    ]
  },
  "tag": "dual-gpu",
  "env": {
    "CUDA_VISIBLE_DEVICES": "",
    "engine": "llama-server",
    "llama_server_bin": "llama-server",
    "llama_host": "127.0.0.1",
    "llama_port": 8080,
    "llama_ctx": null,
    "llama_n_gpu_layers": 999,
    "llama_parallel": 1,
    "temperature": 0.0,
    "seed": 0
  },
  "bench": {
    "prompt_set": "long",
    "prompt": "Write a concise technical overview of KV cache and why it matters for long context.",
    "iterations": [
      {
        "wall_s": 2.955575300000419,
        "max_tokens": 512,
        "prompt_tokens": 16,
        "generated_tokens": 512,
        "tok_per_s": 173.2319254393307,
        "temperature": 0.0,
        "seed": 0,
        "output_text": " Use the following as a reference: https://huggingface.co/blog/long-context-attention\n\nWe are given a reference to a Hugging Face blog post about long context attention. The key points from the blog post (as of the time of writing) are:\n\n1. **KV Cache**: A technique to store the key and value vectors from previous tokens during autoregressive generation to avoid recomputing them for each new token. This is crucial for efficient inference in models with long contexts.\n\n2. **Why it matters for long context**:\n   - Without KV cache, the model would have to recompute the attention weights for every token in the sequence for each new token, leading to quadratic time complexity (O(n\u00b2)) and memory usage.\n   - With KV cache, the model only computes the key and value for the new token and then uses the cached keys and values from previous tokens. This reduces the time complexity to O(n) for the entire sequence (linear in the sequence length) and allows for much longer contexts.\n\n3. **Implementation**:\n   - During the forward pass, the model computes the key and value vectors for each token and stores them in a cache.\n   - For subsequent tokens, the model uses the cached keys and values (from previous tokens) and only computes the key and value for the current token.\n   - The attention mechanism then computes attention between the current query and all cached keys and values.\n\n4. **Benefits**:\n   - **Speed**: Reduces computation time significantly for long sequences.\n   - **Memory**: Reduces memory usage by avoiding redundant computation and storing only the necessary keys and values.\n   - **Scalability**: Enables models to handle longer contexts (e.g., 32K, 128K tokens) that would otherwise be infeasible.\n\n5. **Challenges**:\n   - The cache size grows with the sequence length, so memory management is critical.\n   - Efficient caching strategies (like sliding window attention, sparse attention, or compression) are being explored to manage memory.\n\n**Concise Technical Overview**:\n\nKV cache is a technique used in autoregressive language models to store the key (K) and value (V) vectors of previously processed tokens during inference. Instead of recomputing K and V for every token in the sequence, the model caches them and reuses them for subsequent tokens. This reduces the computational complexity from O(n\u00b2) to O(n) per token, enabling efficient generation of long sequences. By avoiding redundant computation"
      },
      {
        "wall_s": 2.9504686989985203,
        "max_tokens": 512,
        "prompt_tokens": 16,
        "generated_tokens": 512,
        "tok_per_s": 173.5317511328924,
        "temperature": 0.0,
        "seed": 0,
        "output_text": " Use the following as a reference: https://huggingface.co/blog/long-context-attention\n\nWe are given a reference to a Hugging Face blog post about long context attention. The key points from the blog post (as of the time of writing) are:\n\n1. **KV Cache**: A technique to store the key and value vectors from previous tokens during autoregressive generation to avoid recomputing them for each new token. This is crucial for efficient inference in models with long contexts.\n\n2. **Why it matters for long context**:\n   - Without KV cache, the model would have to recompute the attention weights for every token in the sequence for each new token, leading to quadratic time complexity (O(n\u00b2)) and memory usage.\n   - With KV cache, the model only computes the key and value for the new token and then uses the cached keys and values from previous tokens. This reduces the time complexity to O(n) for the entire sequence (linear in the sequence length) and allows for much longer contexts.\n\n3. **Implementation**:\n   - During the forward pass, the model computes the key and value vectors for each token and stores them in a cache.\n   - For subsequent tokens, the model uses the cached keys and values (from previous tokens) and only computes the key and value for the current token.\n   - The attention mechanism then computes attention between the current query and all cached keys and values.\n\n4. **Benefits**:\n   - **Speed**: Reduces computation time significantly for long sequences.\n   - **Memory**: Reduces memory usage by avoiding redundant computation and storing only the necessary keys and values.\n   - **Scalability**: Enables models to handle longer contexts (e.g., 32K, 128K tokens) that would otherwise be infeasible.\n\n5. **Challenges**:\n   - The cache size grows with the sequence length, so memory management is critical.\n   - Efficient caching strategies (like sliding window attention, sparse attention, or compression) are being explored to manage memory.\n\n**Concise Technical Overview**:\n\nKV cache is a technique used in autoregressive language models to store the key (K) and value (V) vectors of previously processed tokens during inference. Instead of recomputing K and V for every token in the sequence, the model caches them and reuses them for subsequent tokens. This reduces the computational complexity from O(n\u00b2) to O(n) per token, enabling efficient generation of long sequences. By avoiding redundant computation"
      },
      {
        "wall_s": 2.9342241039994406,
        "max_tokens": 512,
        "prompt_tokens": 16,
        "generated_tokens": 512,
        "tok_per_s": 174.49246610104788,
        "temperature": 0.0,
        "seed": 0,
        "output_text": " Use the following as a reference: https://huggingface.co/blog/long-context-attention\n\nWe are given a reference to a Hugging Face blog post about long context attention. The key points from the blog post (as of the time of writing) are:\n\n1. **KV Cache**: A technique to store the key and value vectors from previous tokens during autoregressive generation to avoid recomputing them for each new token. This is crucial for efficient inference in models with long contexts.\n\n2. **Why it matters for long context**:\n   - Without KV cache, the model would have to recompute the attention weights for every token in the sequence for each new token, leading to quadratic time complexity (O(n\u00b2)) and memory usage.\n   - With KV cache, the model only computes the key and value for the new token and then uses the cached keys and values from previous tokens. This reduces the time complexity to O(n) for the entire sequence (linear in the sequence length) and allows for much longer contexts.\n\n3. **Implementation**:\n   - During the forward pass, the model computes the key and value vectors for each token and stores them in a cache.\n   - For subsequent tokens, the model uses the cached keys and values (from previous tokens) and only computes the key and value for the current token.\n   - The attention mechanism then computes attention between the current query and all cached keys and values.\n\n4. **Benefits**:\n   - **Speed**: Reduces computation time significantly for long sequences.\n   - **Memory**: Reduces memory usage by avoiding redundant computation and storing only the necessary keys and values.\n   - **Scalability**: Enables models to handle longer contexts (e.g., 32K, 128K tokens) that would otherwise be infeasible.\n\n5. **Challenges**:\n   - The cache size grows with the sequence length, so memory management is critical.\n   - Efficient caching strategies (like sliding window attention, sparse attention, or compression) are being explored to manage memory.\n\n**Concise Technical Overview**:\n\nKV cache is a technique used in autoregressive language models to store the key (K) and value (V) vectors of previously processed tokens during inference. Instead of recomputing K and V for every token in the sequence, the model caches them and reuses them for subsequent tokens. This reduces the computational complexity from O(n\u00b2) to O(n) per token, enabling efficient generation of long sequences. By avoiding redundant computation"
      }
    ],
    "summary": {
      "iterations": 3,
      "median_wall_s": 2.9504686989985203,
      "median_generated_tokens": 512,
      "median_tok_per_s": 173.5317511328924
    }
  }
}
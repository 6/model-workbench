{
  "timestamp": "2025-12-09T03:22:13.383277",
  "model_id": "Qwen/Qwen3-Omni-30B-A3B-Instruct",
  "engine": "vllm-server",
  "mode": "text-only",
  "environment": "stable",
  "gpu_info": {
    "driver_version": "580.95.05",
    "gpus": [
      {
        "index": 0,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887
      },
      {
        "index": 1,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887
      }
    ]
  },
  "tag": "dual-gpu",
  "config": {
    "tensor_parallel_size": 2,
    "max_model_len": 65536,
    "gpu_memory_utilization": 0.95,
    "max_num_batched_tokens": null,
    "image": "none",
    "prompt": "Explain speculative decoding in 2 sentences.",
    "max_tokens": 512,
    "temperature": 0.0
  },
  "results": [
    {
      "wall_s": 0.35854057599863154,
      "prompt_tokens": 17,
      "generated_tokens": 56,
      "tok_per_s": 156.18873775729568,
      "output_text": "Speculative decoding is a technique used in large language models to speed up inference by predicting multiple tokens ahead of time and validating them later. If the predictions are correct, the model saves computation; if not, it discards the speculative tokens and continues from the last confirmed one."
    },
    {
      "wall_s": 0.35724863699942944,
      "prompt_tokens": 17,
      "generated_tokens": 56,
      "tok_per_s": 156.75357216293435,
      "output_text": "Speculative decoding is a technique used in large language models to speed up inference by predicting multiple tokens ahead of time and validating them later. If the predictions are correct, the model saves computation; if not, it discards the speculative tokens and continues from the last confirmed one."
    },
    {
      "wall_s": 0.35704978499779827,
      "prompt_tokens": 17,
      "generated_tokens": 56,
      "tok_per_s": 156.8408730461645,
      "output_text": "Speculative decoding is a technique used in large language models to speed up inference by predicting multiple tokens ahead of time and validating them later. If the predictions are correct, the model saves computation; if not, it discards the speculative tokens and continues from the last confirmed one."
    }
  ],
  "summary": {
    "median_tok_per_s": 156.75357216293435
  }
}
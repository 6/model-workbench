{
  "timestamp": "2025-12-07_210709",
  "model_id": "unsloth/Ministral-3-14B-Reasoning-2512-GGUF/Ministral-3-14B-Reasoning-2512-UD-Q4_K_XL.gguf",
  "model_ref": "/home/peter/models/unsloth/Ministral-3-14B-Reasoning-2512-GGUF/Ministral-3-14B-Reasoning-2512-UD-Q4_K_XL.gguf",
  "engine": "llama-server",
  "gpu_info": {
    "driver_version": "580.95.05",
    "gpus": [
      {
        "index": 0,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887
      },
      {
        "index": 1,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887
      }
    ]
  },
  "tag": "dual-gpu",
  "env": {
    "CUDA_VISIBLE_DEVICES": "",
    "engine": "llama-server",
    "llama_server_bin": "llama-server",
    "llama_host": "127.0.0.1",
    "llama_port": 8080,
    "llama_ctx": null,
    "llama_n_gpu_layers": 999,
    "llama_parallel": 1,
    "temperature": 0.0,
    "seed": 0
  },
  "bench": {
    "prompt_set": "long",
    "prompt": "Write a concise technical overview of KV cache and why it matters for long context.",
    "iterations": [
      {
        "wall_s": 4.0508470669992676,
        "max_tokens": 512,
        "prompt_tokens": 17,
        "generated_tokens": 512,
        "tok_per_s": 126.39331762758265,
        "temperature": 0.0,
        "seed": 0,
        "output_text": " Please include a number of key points and a brief conclusion.\n\n### KV Cache: A Technical Overview\n\n**Key Points:**\n\n1. **Definition and Purpose:**\n   - **KV Cache** (Key-Value Cache) is a mechanism used in transformer-based models, particularly in **Long-Context Transformers (LCTs)**, to store and reuse intermediate outputs (keys and values) during inference.\n   - It reduces computational overhead by avoiding redundant calculations for overlapping input sequences, which is crucial for handling **long contexts** (e.g., > 100K tokens).\n\n2. **How It Works:**\n   - During inference, the model processes a sequence in **chunks** (e.g., 1K tokens at a time).\n   - For each chunk, the model computes **attention keys and values** (K and V) and stores them in a cache.\n   - Subsequent chunks reuse the cached K and V for attention computations, only computing new K and V for the current chunk.\n\n3. **Computational Efficiency:**\n   - **Reduces Attention Computations:** Without KV cache, attention would recompute all keys and values for every chunk, leading to quadratic complexity (O(n\u00b2)). With KV cache, only new K and V are computed, reducing complexity to **O(n)** for long contexts.\n   - **Memory Savings:** KV cache avoids recalculating attention for the entire sequence, significantly improving inference speed and memory usage.\n\n4. **Challenges with Long Contexts:**\n   - **Memory Constraints:** Storing K and V for long sequences (e.g., 100K tokens) can consume **GBs of memory**, limiting scalability.\n   - **Attention Overhead:** Even with KV cache, attention computations for very long sequences (e.g., > 1M tokens) can become **prohibitively expensive** due to the number of tokens.\n\n5. **Solutions for Long Contexts:**\n   - **Recurrent KV Cache:** Processes the sequence **recurrently**, storing only the most recent K and V (e.g., 1K tokens) and discarding older ones. This reduces memory usage but may lose some context.\n   - **Sliding Window Attention:** Uses a **sliding window** (e.g., 1K tokens) to compute attention, discarding older tokens outside the window. This balances memory and context retention.\n   - **Memory-Efficient Attention:** Techniques like **memory compression** (e.g., using low"
      },
      {
        "wall_s": 4.048732575000031,
        "max_tokens": 512,
        "prompt_tokens": 17,
        "generated_tokens": 512,
        "tok_per_s": 126.45932783051153,
        "temperature": 0.0,
        "seed": 0,
        "output_text": " Please include a number of key points and a brief conclusion.\n\n### KV Cache: A Technical Overview\n\n**Key Points:**\n\n1. **Definition and Purpose:**\n   - **KV Cache** (Key-Value Cache) is a mechanism used in transformer-based models, particularly in **Long-Context Transformers (LCTs)**, to store and reuse intermediate outputs (keys and values) during inference.\n   - It reduces computational overhead by avoiding redundant calculations for overlapping input sequences, which is crucial for handling **long contexts** (e.g., > 100K tokens).\n\n2. **How It Works:**\n   - During inference, the model processes a sequence in **chunks** (e.g., 1K tokens at a time).\n   - For each chunk, the model computes **attention keys and values** (K and V) and stores them in a cache.\n   - Subsequent chunks reuse the cached K and V for attention computations, only computing new K and V for the current chunk.\n\n3. **Computational Efficiency:**\n   - **Reduces Attention Computations:** Without KV cache, attention would recompute all keys and values for every chunk, leading to quadratic complexity (O(n\u00b2)). With KV cache, only new K and V are computed, reducing complexity to **O(n)** for long contexts.\n   - **Memory Savings:** KV cache avoids recalculating attention for the entire sequence, significantly improving inference speed and memory usage.\n\n4. **Challenges with Long Contexts:**\n   - **Memory Constraints:** Storing K and V for long sequences (e.g., 100K tokens) can consume **GBs of memory**, limiting scalability.\n   - **Attention Overhead:** Even with KV cache, attention computations for very long sequences (e.g., > 1M tokens) can become **prohibitively expensive** due to the number of tokens.\n\n5. **Solutions for Long Contexts:**\n   - **Recurrent KV Cache:** Processes the sequence **recurrently**, storing only the most recent K and V (e.g., 1K tokens) and discarding older ones. This reduces memory usage but may lose some context.\n   - **Sliding Window Attention:** Uses a **sliding window** (e.g., 1K tokens) to compute attention, discarding older tokens outside the window. This balances memory and context retention.\n   - **Memory-Efficient Attention:** Techniques like **memory compression** (e.g., using low"
      },
      {
        "wall_s": 4.02757284099971,
        "max_tokens": 512,
        "prompt_tokens": 17,
        "generated_tokens": 512,
        "tok_per_s": 127.1237095423737,
        "temperature": 0.0,
        "seed": 0,
        "output_text": " Please include a number of key points and a brief conclusion.\n\n### KV Cache: A Technical Overview\n\n**Key Points:**\n\n1. **Definition and Purpose:**\n   - **KV Cache** (Key-Value Cache) is a mechanism used in transformer-based models, particularly in **Long-Context Transformers (LCTs)**, to store and reuse intermediate outputs (keys and values) during inference.\n   - It reduces computational overhead by avoiding redundant calculations for overlapping input sequences, which is crucial for handling **long contexts** (e.g., > 100K tokens).\n\n2. **How It Works:**\n   - During inference, the model processes a sequence in **chunks** (e.g., 1K tokens at a time).\n   - For each chunk, the model computes **attention keys and values** (K and V) and stores them in a cache.\n   - Subsequent chunks reuse the cached K and V for attention computations, only computing new K and V for the current chunk.\n\n3. **Computational Efficiency:**\n   - **Reduces Attention Computations:** Without KV cache, attention would recompute all keys and values for every chunk, leading to quadratic complexity (O(n\u00b2)). With KV cache, only new K and V are computed, reducing complexity to **O(n)** for long contexts.\n   - **Memory Savings:** KV cache avoids recalculating attention for the entire sequence, significantly improving inference speed and memory usage.\n\n4. **Challenges with Long Contexts:**\n   - **Memory Constraints:** Storing K and V for long sequences (e.g., 100K tokens) can consume **GBs of memory**, limiting scalability.\n   - **Attention Overhead:** Even with KV cache, attention computations for very long sequences (e.g., > 1M tokens) can become **prohibitively expensive** due to the number of tokens.\n\n5. **Solutions for Long Contexts:**\n   - **Recurrent KV Cache:** Processes the sequence **recurrently**, storing only the most recent K and V (e.g., 1K tokens) and discarding older ones. This reduces memory usage but may lose some context.\n   - **Sliding Window Attention:** Uses a **sliding window** (e.g., 1K tokens) to compute attention, discarding older tokens outside the window. This balances memory and context retention.\n   - **Memory-Efficient Attention:** Techniques like **memory compression** (e.g., using low"
      }
    ],
    "summary": {
      "iterations": 3,
      "median_wall_s": 4.048732575000031,
      "median_generated_tokens": 512,
      "median_tok_per_s": 126.45932783051153
    }
  }
}
{
  "timestamp": "2025-12-07_172706",
  "model_id": "unsloth/Qwen3-235B-A22B-Thinking-2507-GGUF/UD-Q4_K_XL",
  "model_ref": "/home/peter/models/unsloth/Qwen3-235B-A22B-Thinking-2507-GGUF/UD-Q4_K_XL/Qwen3-235B-A22B-Thinking-2507-UD-Q4_K_XL-00001-of-00003.gguf",
  "engine": "llama-server",
  "gpu_info": {
    "driver_version": "580.95.05",
    "gpus": [
      {
        "index": 0,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887
      },
      {
        "index": 1,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887
      }
    ]
  },
  "tag": "dual-gpu",
  "env": {
    "CUDA_VISIBLE_DEVICES": "",
    "engine": "llama-server",
    "llama_server_bin": "llama-server",
    "llama_host": "127.0.0.1",
    "llama_port": 8080,
    "llama_ctx": null,
    "llama_n_gpu_layers": 999,
    "llama_parallel": 1,
    "temperature": 0.0,
    "seed": 0
  },
  "bench": {
    "prompt_set": "long",
    "prompt": "Write a concise technical overview of KV cache and why it matters for long context.",
    "iterations": [
      {
        "wall_s": 6.679032559000007,
        "max_tokens": 512,
        "prompt_tokens": 16,
        "generated_tokens": 512,
        "tok_per_s": 76.65780866872392,
        "temperature": 0.0,
        "seed": 0,
        "output_text": " Include the following keywords: attention mechanism, transformer architecture, memory bandwidth, computational efficiency, sequence length. Write in 3 paragraphs.\n\nWe are writing a technical overview of KV cache and its importance for long context.\n Required keywords: attention mechanism, transformer architecture, memory bandwidth, computational efficiency, sequence length.\n Structure: 3 paragraphs.\n\n Outline:\n Paragraph 1: Introduce KV cache in the context of the transformer architecture and attention mechanism, explaining what it is and how it works.\n Paragraph 2: Discuss the problem with long sequence length and how KV cache helps with computational efficiency and memory bandwidth.\n Paragraph 3: Conclude by emphasizing why KV cache matters for long context, tying together the keywords.\n\n Let's write accordingly.\nWe are writing a technical overview of KV cache and its importance for long context.\n Required keywords: attention mechanism, transformer architecture, memory bandwidth, computational efficiency, sequence length.\n Structure: 3 paragraphs.\n\n Outline:\n Paragraph 1: Introduce KV cache in the context of the transformer architecture and attention mechanism, explaining what it is and how it works.\n Paragraph 2: Discuss the problem with long sequence length and how KV cache helps with computational efficiency and memory bandwidth.\n Paragraph 3: Conclude by emphasizing why KV cache matters for long context, tying together the keywords.\n\n Let's write accordingly.\nWe are writing a technical overview of KV cache and its importance for long context.\n Required keywords: attention mechanism, transformer architecture, memory bandwidth, computational efficiency, sequence length.\n Structure: 3 paragraphs.\n\n Outline:\n Paragraph 1: Introduce KV cache in the context of the transformer architecture and attention mechanism, explaining what it is and how it works.\n Paragraph 2: Discuss the problem with long sequence length and how KV cache helps with computational efficiency and memory bandwidth.\n Paragraph 3: Conclude by emphasizing why KV cache matters for long context, tying together the keywords.\n\n Let's write accordingly.\nWe are writing a technical overview of KV cache and its importance for long context.\n Required keywords: attention mechanism, transformer architecture, memory bandwidth, computational efficiency, sequence length.\n Structure: 3 paragraphs.\n\n Outline:\n Paragraph 1: Introduce KV cache in the context of the transformer architecture and attention mechanism, explaining what it is and how it works.\n Paragraph 2: Discuss the problem with long sequence length and how KV cache helps with computational efficiency and memory bandwidth.\n Paragraph 3: Conclude by emphasizing why KV cache matters for long context, tying together the keywords.\n\n Let's write accordingly.\nWe are writing a technical overview of KV cache and"
      },
      {
        "wall_s": 6.650227997000002,
        "max_tokens": 512,
        "prompt_tokens": 16,
        "generated_tokens": 512,
        "tok_per_s": 76.98984158602823,
        "temperature": 0.0,
        "seed": 0,
        "output_text": " Include the following keywords: attention mechanism, transformer architecture, memory bandwidth, computational efficiency, sequence length. Write in 3 paragraphs.\n\nWe are writing a technical overview of KV cache and its importance for long context.\n Required keywords: attention mechanism, transformer architecture, memory bandwidth, computational efficiency, sequence length.\n Structure: 3 paragraphs.\n\n Outline:\n Paragraph 1: Introduce KV cache in the context of the transformer architecture and attention mechanism, explaining what it is and how it works.\n Paragraph 2: Discuss the problem with long sequence length and how KV cache helps with computational efficiency and memory bandwidth.\n Paragraph 3: Conclude by emphasizing why KV cache matters for long context, tying together the keywords.\n\n Let's write accordingly.\nWe are writing a technical overview of KV cache and its importance for long context.\n Required keywords: attention mechanism, transformer architecture, memory bandwidth, computational efficiency, sequence length.\n Structure: 3 paragraphs.\n\n Outline:\n Paragraph 1: Introduce KV cache in the context of the transformer architecture and attention mechanism, explaining what it is and how it works.\n Paragraph 2: Discuss the problem with long sequence length and how KV cache helps with computational efficiency and memory bandwidth.\n Paragraph 3: Conclude by emphasizing why KV cache matters for long context, tying together the keywords.\n\n Let's write accordingly.\nWe are writing a technical overview of KV cache and its importance for long context.\n Required keywords: attention mechanism, transformer architecture, memory bandwidth, computational efficiency, sequence length.\n Structure: 3 paragraphs.\n\n Outline:\n Paragraph 1: Introduce KV cache in the context of the transformer architecture and attention mechanism, explaining what it is and how it works.\n Paragraph 2: Discuss the problem with long sequence length and how KV cache helps with computational efficiency and memory bandwidth.\n Paragraph 3: Conclude by emphasizing why KV cache matters for long context, tying together the keywords.\n\n Let's write accordingly.\nWe are writing a technical overview of KV cache and its importance for long context.\n Required keywords: attention mechanism, transformer architecture, memory bandwidth, computational efficiency, sequence length.\n Structure: 3 paragraphs.\n\n Outline:\n Paragraph 1: Introduce KV cache in the context of the transformer architecture and attention mechanism, explaining what it is and how it works.\n Paragraph 2: Discuss the problem with long sequence length and how KV cache helps with computational efficiency and memory bandwidth.\n Paragraph 3: Conclude by emphasizing why KV cache matters for long context, tying together the keywords.\n\n Let's write accordingly.\nWe are writing a technical overview of KV cache and"
      },
      {
        "wall_s": 6.625460620000013,
        "max_tokens": 512,
        "prompt_tokens": 16,
        "generated_tokens": 512,
        "tok_per_s": 77.27764594275092,
        "temperature": 0.0,
        "seed": 0,
        "output_text": " Include the following keywords: attention mechanism, transformer architecture, memory bandwidth, computational efficiency, sequence length. Write in 3 paragraphs.\n\nWe are writing a technical overview of KV cache and its importance for long context.\n Required keywords: attention mechanism, transformer architecture, memory bandwidth, computational efficiency, sequence length.\n Structure: 3 paragraphs.\n\n Outline:\n Paragraph 1: Introduce KV cache in the context of the transformer architecture and attention mechanism, explaining what it is and how it works.\n Paragraph 2: Discuss the problem with long sequence length and how KV cache helps with computational efficiency and memory bandwidth.\n Paragraph 3: Conclude by emphasizing why KV cache matters for long context, tying together the keywords.\n\n Let's write accordingly.\nWe are writing a technical overview of KV cache and its importance for long context.\n Required keywords: attention mechanism, transformer architecture, memory bandwidth, computational efficiency, sequence length.\n Structure: 3 paragraphs.\n\n Outline:\n Paragraph 1: Introduce KV cache in the context of the transformer architecture and attention mechanism, explaining what it is and how it works.\n Paragraph 2: Discuss the problem with long sequence length and how KV cache helps with computational efficiency and memory bandwidth.\n Paragraph 3: Conclude by emphasizing why KV cache matters for long context, tying together the keywords.\n\n Let's write accordingly.\nWe are writing a technical overview of KV cache and its importance for long context.\n Required keywords: attention mechanism, transformer architecture, memory bandwidth, computational efficiency, sequence length.\n Structure: 3 paragraphs.\n\n Outline:\n Paragraph 1: Introduce KV cache in the context of the transformer architecture and attention mechanism, explaining what it is and how it works.\n Paragraph 2: Discuss the problem with long sequence length and how KV cache helps with computational efficiency and memory bandwidth.\n Paragraph 3: Conclude by emphasizing why KV cache matters for long context, tying together the keywords.\n\n Let's write accordingly.\nWe are writing a technical overview of KV cache and its importance for long context.\n Required keywords: attention mechanism, transformer architecture, memory bandwidth, computational efficiency, sequence length.\n Structure: 3 paragraphs.\n\n Outline:\n Paragraph 1: Introduce KV cache in the context of the transformer architecture and attention mechanism, explaining what it is and how it works.\n Paragraph 2: Discuss the problem with long sequence length and how KV cache helps with computational efficiency and memory bandwidth.\n Paragraph 3: Conclude by emphasizing why KV cache matters for long context, tying together the keywords.\n\n Let's write accordingly.\nWe are writing a technical overview of KV cache and"
      }
    ],
    "summary": {
      "iterations": 3,
      "median_wall_s": 6.650227997000002,
      "median_generated_tokens": 512,
      "median_tok_per_s": 76.98984158602823
    }
  }
}
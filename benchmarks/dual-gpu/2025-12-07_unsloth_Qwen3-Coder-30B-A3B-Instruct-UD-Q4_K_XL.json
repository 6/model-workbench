{
  "timestamp": "2025-12-07_211313",
  "model_id": "unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/Qwen3-Coder-30B-A3B-Instruct-UD-Q4_K_XL.gguf",
  "model_ref": "/home/peter/models/unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF/Qwen3-Coder-30B-A3B-Instruct-UD-Q4_K_XL.gguf",
  "engine": "llama-server",
  "gpu_info": {
    "driver_version": "580.95.05",
    "gpus": [
      {
        "index": 0,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887
      },
      {
        "index": 1,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887
      }
    ]
  },
  "tag": "dual-gpu",
  "env": {
    "CUDA_VISIBLE_DEVICES": "",
    "engine": "llama-server",
    "llama_server_bin": "llama-server",
    "llama_host": "127.0.0.1",
    "llama_port": 8080,
    "llama_ctx": null,
    "llama_n_gpu_layers": 999,
    "llama_parallel": 1,
    "temperature": 0.0,
    "seed": 0
  },
  "bench": {
    "prompt_set": "long",
    "prompt": "Write a concise technical overview of KV cache and why it matters for long context.",
    "iterations": [
      {
        "wall_s": 1.9277815980003652,
        "max_tokens": 512,
        "prompt_tokens": 16,
        "generated_tokens": 512,
        "tok_per_s": 265.59025178530777,
        "temperature": 0.0,
        "seed": 0,
        "output_text": "  KV cache is a technique used in transformer models to store key and value vectors computed during attention operations. During inference, these cached vectors are reused to avoid recomputing attention for previously seen tokens, significantly improving efficiency. For long context scenarios, KV cache becomes critical because the memory overhead of storing all key-value pairs grows linearly with sequence length. Techniques like sliding window attention, prefix caching, and selective caching are used to manage this memory burden while maintaining performance. KV cache is essential for enabling efficient long-context inference in transformer models, particularly for applications like chatbots, document understanding, and code generation where long sequences are common.\n\nKV cache is a technique used in transformer models to store key and value vectors computed during attention operations. During inference, these cached vectors are reused to avoid recomputing attention for previously seen tokens, significantly improving efficiency. For long context scenarios, KV cache becomes critical because the memory overhead of storing all key-value pairs grows linearly with sequence length. Techniques like sliding window attention, prefix caching, and selective caching are used to manage this memory burden while maintaining performance. KV cache is essential for enabling efficient long-context inference in transformer models, particularly for applications like chatbots, document understanding, and code generation where long sequences are common.\n\nKV cache is a technique used in transformer models to store key and value vectors computed during attention operations. During inference, these cached vectors are reused to avoid recomputing attention for previously seen tokens, significantly improving efficiency. For long context scenarios, KV cache becomes critical because the memory overhead of storing all key-value pairs grows linearly with sequence length. Techniques like sliding window attention, prefix caching, and selective caching are used to manage this memory burden while maintaining performance. KV cache is essential for enabling efficient long-context inference in transformer models, particularly for applications like chatbots, document understanding, and code generation where long sequences are common.\n\nKV cache is a technique used in transformer models to store key and value vectors computed during attention operations. During inference, these cached vectors are reused to avoid recomputing attention for previously seen tokens, significantly improving efficiency. For long context scenarios, KV cache becomes critical because the memory overhead of storing all key-value pairs grows linearly with sequence length. Techniques like sliding window attention, prefix caching, and selective caching are used to manage this memory burden while maintaining performance. KV cache is essential for enabling efficient long-context inference in transformer models, particularly for applications like chatbots, document understanding, and code generation where long sequences are common.\n\nKV cache is a technique used in transformer models to store"
      },
      {
        "wall_s": 1.930366642000081,
        "max_tokens": 512,
        "prompt_tokens": 16,
        "generated_tokens": 512,
        "tok_per_s": 265.2345874924099,
        "temperature": 0.0,
        "seed": 0,
        "output_text": "  KV cache is a technique used in transformer models to store key and value vectors computed during attention operations. During inference, these cached vectors are reused to avoid recomputing attention for previously seen tokens, significantly improving efficiency. For long context scenarios, KV cache becomes critical because the memory overhead of storing all key-value pairs grows linearly with sequence length. Techniques like sliding window attention, prefix caching, and selective caching are used to manage this memory burden while maintaining performance. KV cache is essential for enabling efficient long-context inference in transformer models, particularly for applications like chatbots, document understanding, and code generation where long sequences are common.\n\nKV cache is a technique used in transformer models to store key and value vectors computed during attention operations. During inference, these cached vectors are reused to avoid recomputing attention for previously seen tokens, significantly improving efficiency. For long context scenarios, KV cache becomes critical because the memory overhead of storing all key-value pairs grows linearly with sequence length. Techniques like sliding window attention, prefix caching, and selective caching are used to manage this memory burden while maintaining performance. KV cache is essential for enabling efficient long-context inference in transformer models, particularly for applications like chatbots, document understanding, and code generation where long sequences are common.\n\nKV cache is a technique used in transformer models to store key and value vectors computed during attention operations. During inference, these cached vectors are reused to avoid recomputing attention for previously seen tokens, significantly improving efficiency. For long context scenarios, KV cache becomes critical because the memory overhead of storing all key-value pairs grows linearly with sequence length. Techniques like sliding window attention, prefix caching, and selective caching are used to manage this memory burden while maintaining performance. KV cache is essential for enabling efficient long-context inference in transformer models, particularly for applications like chatbots, document understanding, and code generation where long sequences are common.\n\nKV cache is a technique used in transformer models to store key and value vectors computed during attention operations. During inference, these cached vectors are reused to avoid recomputing attention for previously seen tokens, significantly improving efficiency. For long context scenarios, KV cache becomes critical because the memory overhead of storing all key-value pairs grows linearly with sequence length. Techniques like sliding window attention, prefix caching, and selective caching are used to manage this memory burden while maintaining performance. KV cache is essential for enabling efficient long-context inference in transformer models, particularly for applications like chatbots, document understanding, and code generation where long sequences are common.\n\nKV cache is a technique used in transformer models to store"
      },
      {
        "wall_s": 1.9162543290003669,
        "max_tokens": 512,
        "prompt_tokens": 16,
        "generated_tokens": 512,
        "tok_per_s": 267.1879156390947,
        "temperature": 0.0,
        "seed": 0,
        "output_text": "  KV cache is a technique used in transformer models to store key and value vectors computed during attention operations. During inference, these cached vectors are reused to avoid recomputing attention for previously seen tokens, significantly improving efficiency. For long context scenarios, KV cache becomes critical because the memory overhead of storing all key-value pairs grows linearly with sequence length. Techniques like sliding window attention, prefix caching, and selective caching are used to manage this memory burden while maintaining performance. KV cache is essential for enabling efficient long-context inference in transformer models, particularly for applications like chatbots, document understanding, and code generation where long sequences are common.\n\nKV cache is a technique used in transformer models to store key and value vectors computed during attention operations. During inference, these cached vectors are reused to avoid recomputing attention for previously seen tokens, significantly improving efficiency. For long context scenarios, KV cache becomes critical because the memory overhead of storing all key-value pairs grows linearly with sequence length. Techniques like sliding window attention, prefix caching, and selective caching are used to manage this memory burden while maintaining performance. KV cache is essential for enabling efficient long-context inference in transformer models, particularly for applications like chatbots, document understanding, and code generation where long sequences are common.\n\nKV cache is a technique used in transformer models to store key and value vectors computed during attention operations. During inference, these cached vectors are reused to avoid recomputing attention for previously seen tokens, significantly improving efficiency. For long context scenarios, KV cache becomes critical because the memory overhead of storing all key-value pairs grows linearly with sequence length. Techniques like sliding window attention, prefix caching, and selective caching are used to manage this memory burden while maintaining performance. KV cache is essential for enabling efficient long-context inference in transformer models, particularly for applications like chatbots, document understanding, and code generation where long sequences are common.\n\nKV cache is a technique used in transformer models to store key and value vectors computed during attention operations. During inference, these cached vectors are reused to avoid recomputing attention for previously seen tokens, significantly improving efficiency. For long context scenarios, KV cache becomes critical because the memory overhead of storing all key-value pairs grows linearly with sequence length. Techniques like sliding window attention, prefix caching, and selective caching are used to manage this memory burden while maintaining performance. KV cache is essential for enabling efficient long-context inference in transformer models, particularly for applications like chatbots, document understanding, and code generation where long sequences are common.\n\nKV cache is a technique used in transformer models to store"
      }
    ],
    "summary": {
      "iterations": 3,
      "median_wall_s": 1.9277815980003652,
      "median_generated_tokens": 512,
      "median_tok_per_s": 265.59025178530777
    }
  }
}
{
  "timestamp": "2025-12-07_211142",
  "model_id": "unsloth/Qwen3-VL-2B-Instruct-1M-GGUF/Qwen3-VL-2B-Instruct-1M-UD-Q4_K_XL.gguf",
  "model_ref": "/home/peter/models/unsloth/Qwen3-VL-2B-Instruct-1M-GGUF/Qwen3-VL-2B-Instruct-1M-UD-Q4_K_XL.gguf",
  "engine": "llama-server",
  "gpu_info": {
    "driver_version": "580.95.05",
    "gpus": [
      {
        "index": 0,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887
      },
      {
        "index": 1,
        "name": "NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition",
        "memory_total_mib": 97887
      }
    ]
  },
  "tag": "dual-gpu",
  "env": {
    "CUDA_VISIBLE_DEVICES": "",
    "engine": "llama-server",
    "llama_server_bin": "llama-server",
    "llama_host": "127.0.0.1",
    "llama_port": 8080,
    "llama_ctx": null,
    "llama_n_gpu_layers": 999,
    "llama_parallel": 1,
    "temperature": 0.0,
    "seed": 0
  },
  "bench": {
    "prompt_set": "long",
    "prompt": "Write a concise technical overview of KV cache and why it matters for long context.",
    "iterations": [
      {
        "wall_s": 0.6077633780005272,
        "max_tokens": 512,
        "prompt_tokens": 16,
        "generated_tokens": 208,
        "tok_per_s": 342.23845583505954,
        "temperature": 0.0,
        "seed": 0,
        "output_text": " In 100 words or less.\n\nOkay, here's a concise technical overview of KV cache and its importance for long context:\n\nKV cache (Key-Value Cache) is a memory-based storage system that stores intermediate results of model computations. It stores key-value pairs, where keys are model states (e.g., hidden states, attention weights) and values are the corresponding outputs. This cache is crucial for long-context applications because it allows the model to efficiently reuse previously computed information, reducing redundant calculations. By storing and retrieving these cached values, the model can maintain its context over long sequences without needing to recompute everything from scratch. This significantly improves efficiency and enables the model to handle longer input lengths, which is essential for tasks like long-form text generation, dialogue systems, and video processing. The cache is typically managed by the model's attention mechanism, which identifies relevant states to store and retrieve. This approach is fundamental to modern large language models (LLMs) and enables them to process longer inputs effectively.\n\n(99 words)"
      },
      {
        "wall_s": 0.6020906030007609,
        "max_tokens": 512,
        "prompt_tokens": 16,
        "generated_tokens": 208,
        "tok_per_s": 345.462956842954,
        "temperature": 0.0,
        "seed": 0,
        "output_text": " In 100 words or less.\n\nOkay, here's a concise technical overview of KV cache and its importance for long context:\n\nKV cache (Key-Value Cache) is a memory-based storage system that stores intermediate results of model computations. It stores key-value pairs, where keys are model states (e.g., hidden states, attention weights) and values are the corresponding outputs. This cache is crucial for long-context applications because it allows the model to efficiently reuse previously computed information, reducing redundant calculations. By storing and retrieving these cached values, the model can maintain its context over long sequences without needing to recompute everything from scratch. This significantly improves efficiency and enables the model to handle longer input lengths, which is essential for tasks like long-form text generation, dialogue systems, and video processing. The cache is typically managed by the model's attention mechanism, which identifies relevant states to store and retrieve. This approach is fundamental to modern large language models (LLMs) and enables them to process longer inputs effectively.\n\n(99 words)"
      },
      {
        "wall_s": 0.5962158079983055,
        "max_tokens": 512,
        "prompt_tokens": 16,
        "generated_tokens": 208,
        "tok_per_s": 348.8669659704681,
        "temperature": 0.0,
        "seed": 0,
        "output_text": " In 100 words or less.\n\nOkay, here's a concise technical overview of KV cache and its importance for long context:\n\nKV cache (Key-Value Cache) is a memory-based storage system that stores intermediate results of model computations. It stores key-value pairs, where keys are model states (e.g., hidden states, attention weights) and values are the corresponding outputs. This cache is crucial for long-context applications because it allows the model to efficiently reuse previously computed information, reducing redundant calculations. By storing and retrieving these cached values, the model can maintain its context over long sequences without needing to recompute everything from scratch. This significantly improves efficiency and enables the model to handle longer input lengths, which is essential for tasks like long-form text generation, dialogue systems, and video processing. The cache is typically managed by the model's attention mechanism, which identifies relevant states to store and retrieve. This approach is fundamental to modern large language models (LLMs) and enables them to process longer inputs effectively.\n\n(99 words)"
      }
    ],
    "summary": {
      "iterations": 3,
      "median_wall_s": 0.6020906030007609,
      "median_generated_tokens": 208,
      "median_tok_per_s": 345.462956842954
    }
  }
}
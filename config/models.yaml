# Backend version defaults
# Models without explicit backend_version use these
defaults:
  vllm_version: v0.8.0        # Default vLLM version for safetensors models
  vllm_image_type: prebuilt   # Image type: "prebuilt" (official Docker Hub) or "build" (from source)
  llama_version: b4521        # Default llama.cpp version for GGUF models
  # ik_llama_version: null    # Optional: ik_llama.cpp version (falls back to llama_version if not set)
  trtllm_version: "0.18.0"    # Default TensorRT-LLM version (NGC image tag)

models:
  # Devstral 123B GGUF (from main)
  - source: hf
    repo_id: unsloth/Devstral-2-123B-Instruct-2512-GGUF
    include:
      - "UD-Q3_K_XL/*.gguf"
      - "UD-Q4_K_XL/*.gguf"
      - "UD-Q5_K_XL/*.gguf"
      - "UD-Q6_K_XL/*.gguf"

  # vLLM models (safetensors) - use default vllm_version unless overridden
  - source: hf
    repo_id: mistralai/Devstral-Small-2-24B-Instruct-2512
    # backend_version: abc123def  # Uncomment to pin to specific commit

  # GGUF models - use default llama_version unless overridden
  - source: hf
    repo_id: bartowski/mistralai_Devstral-Small-2-24B-Instruct-2512-GGUF
    include:
      - "mistralai_Devstral-Small-2-24B-Instruct-2512-Q4_K_L.gguf"

  - source: hf
    repo_id: zai-org/GLM-4.6V-FP8
    # backend_version: v0.8.1  # Can override to specific version if needed

  - source: hf
    repo_id: unsloth/MiniMax-M2-GGUF
    include:
      - "UD-Q4_K_XL/*.gguf"
      - "UD-Q5_K_XL/*.gguf"

  # Seems buggy
  #- source: hf
  #  repo_id: bartowski/cerebras_MiniMax-M2-REAP-139B-A10B-GGUF
  #  include:
  #    - "cerebras_MiniMax-M2-REAP-139B-A10B-Q4_K_M/*.gguf"

  #- source: hf
  #  repo_id: bartowski/cerebras_MiniMax-M2-REAP-162B-A10B-GGUF
  #  include:
  #    - "cerebras_MiniMax-M2-REAP-162B-A10B-IQ4_XS/*.gguf"

  - source: hf
    repo_id: bartowski/cerebras_MiniMax-M2-REAP-172B-A10B-GGUF
    include:
      - "cerebras_MiniMax-M2-REAP-172B-A10B-IQ4_XS/*.gguf"

  - source: hf
    repo_id: unsloth/Qwen3-235B-A22B-Thinking-2507-GGUF
    include:
      - "UD-Q4_K_XL/*.gguf"
      - "UD-Q5_K_XL/*.gguf"

  #- source: hf
  #  repo_id: unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF
  #  include:
  #    - "UD-Q4_K_XL/*.gguf"

  - source: hf
    repo_id: unsloth/Qwen3-VL-235B-A22B-Instruct-GGUF
    include:
      - "UD-Q4_K_XL/*.gguf"
      - "mmproj-F16.gguf"

  #- source: hf
  #  repo_id: unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF
  #  include:
  #    - "UD-Q2_K_XL/*.gguf"
  #    #- "UD-Q4_K_XL/*.gguf"

  - source: hf
    repo_id: unsloth/gpt-oss-20b-GGUF
    include:
      - "gpt-oss-20b-F16.gguf"

  - source: hf
    repo_id: unsloth/gpt-oss-120b-GGUF
    include:
      - "gpt-oss-120b-F16.gguf"

  - source: hf
    repo_id: Qwen/Qwen3-Omni-30B-A3B-Instruct

  - source: hf
    repo_id: Qwen/Qwen-Image-Edit-2509

  - source: hf
    repo_id: Qwen/Qwen3-30B-A3B-Instruct-2507-FP8

  #- source: hf
  #  repo_id: Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8

  #- source: hf
  #  repo_id: Qwen/Qwen3-Next-80B-A3B-Thinking-FP8

  - source: hf
    repo_id: Qwen/Qwen3-VL-8B-Instruct-FP8

  # Small model to trial evals
  - source: hf
    repo_id: Qwen/Qwen3-VL-2B-Instruct

  #- source: hf
  #  repo_id: Qwen/Qwen3-VL-4B-Instruct

  - source: hf
    repo_id: allenai/Olmo-3-32B-Think

  - source: hf
    repo_id: unsloth/Olmo-3-32B-Think-GGUF
    include:
      - "Olmo-3-32B-Think-UD-Q4_K_XL.gguf"

  - source: hf
    repo_id: allenai/Olmo-3-7B-Think

  #- source: hf
  #  repo_id: mistralai/Ministral-3-14B-Instruct-2512

  #- source: hf
  #  repo_id: mistralai/Ministral-3-8B-Instruct-2512

  - source: hf
    repo_id: openai/gpt-oss-20b

  - source: hf
    repo_id: openai/gpt-oss-safeguard-20b

  - source: hf
    repo_id: openai/gpt-oss-120b

  - source: hf
    repo_id: unsloth/GLM-4.5-Air-GGUF
    include:
      - "UD-Q4_K_XL/*.gguf"

  #- source: hf
  #  repo_id: zai-org/GLM-4.5-Air-FP8

  - source: hf
    repo_id: unsloth/GLM-4.6-GGUF
    include:
      - "UD-Q3_K_XL/*.gguf"
      - "IQ4_XS/*.gguf"
      - "UD-Q4_K_XL/*.gguf"
      - "UD-Q5_K_XL/*.gguf"

  - source: hf
    repo_id: unsloth/GLM-4.6-REAP-268B-A32B-GGUF
    include:
      - "UD-Q4_K_XL/*.gguf"
      - "UD-Q5_K_XL/*.gguf"

  - source: hf
    repo_id: unsloth/Ministral-3-14B-Instruct-2512-GGUF
    include:
      - "Ministral-3-14B-Instruct-2512-UD-Q4_K_XL.gguf"

  - source: hf
    repo_id: unsloth/Ministral-3-14B-Reasoning-2512-GGUF
    include:
      - "Ministral-3-14B-Reasoning-2512-UD-Q4_K_XL.gguf"

  - source: hf
    repo_id: unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF
    include:
      - "Qwen3-Next-80B-A3B-Thinking-UD-Q4_K_XL.gguf"

  - source: hf
    repo_id: unsloth/Qwen3-VL-32B-Instruct-1M-GGUF
    include:
      - "Qwen3-VL-32B-Instruct-1M-UD-Q4_K_XL.gguf"
      - "mmproj-F16.gguf"

  - source: hf
    repo_id: unsloth/Qwen3-VL-30B-A3B-Instruct-GGUF
    include:
      - "Qwen3-VL-30B-A3B-Instruct-UD-Q4_K_XL.gguf"
      - "mmproj-F16.gguf"

  - source: hf
    repo_id: unsloth/Qwen3-VL-8B-Instruct-1M-GGUF
    include:
      - "Qwen3-VL-8B-Instruct-1M-UD-Q4_K_XL.gguf"
      - "mmproj-F16.gguf"

  - source: hf
    repo_id: unsloth/Qwen3-VL-2B-Instruct-1M-GGUF
    include:
      - "Qwen3-VL-2B-Instruct-1M-UD-Q4_K_XL.gguf"
      - "mmproj-F16.gguf"

  - source: hf
    repo_id: unsloth/Qwen3-VL-2B-Thinking-1M-GGUF
    include:
      - "Qwen3-VL-2B-Thinking-1M-UD-Q4_K_XL.gguf"
      - "mmproj-F16.gguf"

  - source: hf
    repo_id: unsloth/Qwen3-0.6B-GGUF
    include:
      - "Qwen3-0.6B-UD-Q4_K_XL.gguf"
      - "Qwen3-0.6B-UD-Q6_K_XL.gguf"

  - source: hf
    repo_id: unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF
    include:
      - "Qwen3-Coder-30B-A3B-Instruct-UD-Q4_K_XL.gguf"

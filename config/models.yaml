# Backend configuration
# Models without explicit overrides use these defaults
defaults:
  # Storage paths by model format
  storage:
    primary: ~/models              # Default for safetensors/EXL3/MLX
    gguf: /data/storage2/models    # GGUF models (slower NVMe, more space)

  backends:
    # https://github.com/vllm-project/vllm/releases
    vllm:
      version: v0.13.0
      image_type: prebuilt    # "prebuilt" (Docker Hub) or "build" (from source)
      args:
        gpu_memory_utilization: 0.98  # Dedicated inference GPUs, no display
        max_model_len: 65536
      # Model-specific CLI args (pattern is regex matched against model path)
      model_patterns:
        - pattern: "glm.*v"  # GLM vision models
          args:
            - "--enable-expert-parallel"
            - "--allowed-local-media-path"
            - "/"
            - "--mm-encoder-tp-mode"
            - "data"
            - "--mm_processor_cache_type"
            - "shm"
        - pattern: "mistral|devstral|ministral"
          args:
            - "--tokenizer_mode"
            - "mistral"
            - "--config_format"
            - "mistral"
            - "--load_format"
            - "mistral"
    # https://github.com/ggerganov/llama.cpp/releases
    # Note: b7538 introduced Blackwell changes that break CUDA graphs on SM120
    # Staying on b7531 (last version before that change) for stable performance
    llama:
      version: b7531
      image_type: build
      args:
        n_gpu_layers: 999
    # https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tensorrt-llm/containers/release
    trtllm:
      version: 1.2.0rc5
      image_type: prebuilt    # Uses NGC images
    # https://hub.docker.com/r/lmsysorg/sglang/tags
    sglang:
      version: nightly-dev-20251221-1d90b194   # Dec 21 nightly with MiMo support
      image_type: prebuilt
      args:
        mem_fraction_static: 0.98  # Dedicated inference GPUs, no display
        max_model_len: 65536
      model_patterns:
        - pattern: "mimo"
          args:
            - "--attention-backend"
            - "triton"
            - "--reasoning-parser"
            - "qwen3"
            - "--tool-call-parser"
            - "mimo"
    # https://github.com/turboderp-org/exllamav3/releases
    # https://github.com/theroyallab/tabbyAPI (commit SHA, no releases)
    exl:
      version: v0.0.18
      tabby_version: 84bb1ce   # TabbyAPI commit (pinned in Dockerfile default)
      image_type: build
      args:
        cache_size: 32768
        gpu_split_auto: true   # Automatically split model across GPUs
        # gpu_split: [24, 24]  # Explicit split in GB per GPU (overrides gpu_split_auto)
    # https://github.com/kvcache-ai/ktransformers
    # CPU-GPU hybrid inference for models that don't fit in GPU memory
    ktransformers:
      version: main
      image_type: build
      args:
        kt_method: FP8          # Use native FP8 weights (for M2.1, etc.)
        cpu_threads: 16         # Physical CPU cores for inference
        numa_nodes: 1           # NUMA node count (1 for desktop, 2+ for servers)
        cache_lens: 32768       # KV cache length in tokens

models:
  - repo_id: MiniMaxAI/MiniMax-M2.1
    backends:
      ktransformers:
        args:
          kt_method: FP8        # M2.1 uses native FP8 weights

  - repo_id: cyankiwi/MiniMax-M2.1-AWQ-4bit

  - repo_id: unsloth/MiniMax-M2.1-GGUF
    include:
      - "UD-Q4_K_XL/*.gguf"
      - "UD-Q5_K_XL/*.gguf"
      - "Q6_K/*.gguf"
    backends:
      llama:
        args:
          repeat_penalty: 1.15      # Prevent repetition loops in thinking
          repeat_last_n: 128        # Consider last 128 tokens for penalty

  - repo_id: QuantTrio/GLM-4.7-AWQ
    backends:
      vllm:
        args:
          cpu_offload_gb: 15
          max_model_len: 8192

  # DOES NOT FIT: GLM-4.7 Compressed-Tensors W4A16 (189GB) on 2x RTX PRO 6000 (192GB)
  # - Model uses 88+ GiB per GPU (~93% of 94.97 GiB VRAM)
  # - cpu_offload_gb doesn't work with MoE models (vllm-project/vllm#15196)
  # - No vLLM memory optimization can free enough headroom for KV cache
  # Use EXL3 or GGUF quantizations instead (see below)
  #- repo_id: TheHouseOfTheDude/GLM-4.7_Compressed-Tensors
  #  revisions:
  #    - W4A16_GS128
  #  backends:
  #    vllm:
  #      docker_image: vllm-ct:v0.13.0
  #      args:
  #        gpu_memory_utilization: 0.99
  #        max_model_len: 1024

  - repo_id: mratsim/GLM-4.7-EXL3
    revisions:
      - 3.84bpw-tuned
      - 4bpw_H6
    backend: exl

  - repo_id: AesSedai/GLM-4.7-GGUF
    include:
      - "GLM-4.7-IQ4_XS/*.gguf"

  - repo_id: unsloth/GLM-4.7-GGUF
    include:
      - "UD-Q3_K_XL/*.gguf"
      - "UD-Q4_K_XL/*.gguf"
      - "UD-Q5_K_XL/*.gguf"
      - "UD-Q6_K_XL/*.gguf"

  - repo_id: ubergarm/GLM-4.7-GGUF
    include:
      - "IQ3_KS/*.gguf"
      - "big-IQ3_KS/*.gguf"

  - repo_id: cyankiwi/MiMo-V2-Flash-AWQ-4bit
    backend: sglang           # Only SGLang supports this model
    backends:
      sglang:
        args:
          frequency_penalty: 0.2   # Prevent repetition loops with AWQ quant

  - repo_id: cyankiwi/Devstral-2-123B-Instruct-2512-AWQ-4bit

  - repo_id: unsloth/Devstral-2-123B-Instruct-2512-GGUF
    include:
      #- "UD-Q3_K_XL/*.gguf"
      - "UD-Q4_K_XL/*.gguf"
      #- "UD-Q5_K_XL/*.gguf"
      #- "UD-Q6_K_XL/*.gguf"

  - repo_id: unsloth/Devstral-Small-2-24B-Instruct-2512-GGUF
    include:
      #- "Devstral-Small-2-24B-Instruct-2512-UD-Q4_K_XL.gguf"
      - "Devstral-Small-2-24B-Instruct-2512-UD-Q6_K_XL.gguf"

  - repo_id: mistralai/Devstral-Small-2-24B-Instruct-2512
    exclude:
      - "model-*.safetensors" # use consolidated

  - repo_id: mistralai/Devstral-2-123B-Instruct-2512
    exclude:
      - "model-*.safetensors" # use consolidated

  # ExLlamaV3 test model (EXL3 4.0bpw quant from non-main branch)
  - repo_id: turboderp/Qwen3-0.6B-exl3
    revisions:
      - 4.0bpw
    backend: exl

  #- repo_id: bartowski/mistralai_Devstral-Small-2-24B-Instruct-2512-GGUF
  #  include:
  #    - "mistralai_Devstral-Small-2-24B-Instruct-2512-Q4_K_L.gguf"

  - repo_id: google/gemma-3-1b-it

  - repo_id: Qwen/Qwen3-0.6B

  #- repo_id: zai-org/GLM-4.6V-FP8

  #- repo_id: unsloth/MiniMax-M2-GGUF
  #  include:
  #    #- "UD-Q4_K_XL/*.gguf"
  #    - "UD-Q5_K_XL/*.gguf"

  # Seems buggy
  #- repo_id: bartowski/cerebras_MiniMax-M2-REAP-139B-A10B-GGUF
  #  include:
  #    - "cerebras_MiniMax-M2-REAP-139B-A10B-Q4_K_M/*.gguf"

  #- repo_id: bartowski/cerebras_MiniMax-M2-REAP-162B-A10B-GGUF
  #  include:
  #    - "cerebras_MiniMax-M2-REAP-162B-A10B-IQ4_XS/*.gguf"

  #- repo_id: bartowski/cerebras_MiniMax-M2-REAP-172B-A10B-GGUF
  #  include:
  #    - "cerebras_MiniMax-M2-REAP-172B-A10B-IQ4_XS/*.gguf"

  - repo_id: unsloth/Qwen3-235B-A22B-Thinking-2507-GGUF
    include:
      #- "UD-Q4_K_XL/*.gguf"
      - "UD-Q5_K_XL/*.gguf"

  #- repo_id: unsloth/Qwen3-235B-A22B-Instruct-2507-GGUF
  #  include:
  #    - "UD-Q4_K_XL/*.gguf"

  - repo_id: unsloth/Qwen3-VL-235B-A22B-Instruct-GGUF
    include:
      - "UD-Q4_K_XL/*.gguf"
      - "mmproj-F16.gguf"

  #- repo_id: unsloth/Qwen3-Coder-480B-A35B-Instruct-GGUF
  #  include:
  #    - "UD-Q2_K_XL/*.gguf"
  #    #- "UD-Q4_K_XL/*.gguf"

  - repo_id: unsloth/gpt-oss-20b-GGUF
    include:
      - "gpt-oss-20b-F16.gguf"

  - repo_id: unsloth/gpt-oss-120b-GGUF
    include:
      - "gpt-oss-120b-F16.gguf"

  - repo_id: Qwen/Qwen3-Omni-30B-A3B-Instruct

  - repo_id: Qwen/Qwen-Image-Edit-2509

  #- repo_id: Qwen/Qwen3-30B-A3B-Instruct-2507-FP8

  #- repo_id: Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8

  #- repo_id: Qwen/Qwen3-Next-80B-A3B-Thinking-FP8

  #- repo_id: Qwen/Qwen3-VL-8B-Instruct-FP8

  # Small model to trial evals
  - repo_id: Qwen/Qwen3-VL-2B-Instruct

  #- repo_id: Qwen/Qwen3-VL-4B-Instruct

  - repo_id: allenai/Olmo-3-32B-Think

  #- repo_id: unsloth/Olmo-3-32B-Think-GGUF
  #  include:
  #    - "Olmo-3-32B-Think-UD-Q4_K_XL.gguf"

  #- repo_id: allenai/Olmo-3-7B-Think

  #- repo_id: mistralai/Ministral-3-14B-Instruct-2512

  #- repo_id: mistralai/Ministral-3-8B-Instruct-2512

  - repo_id: openai/gpt-oss-20b

  - repo_id: openai/gpt-oss-safeguard-20b

  - repo_id: openai/gpt-oss-120b

  #- repo_id: unsloth/GLM-4.5-Air-GGUF
  #  include:
  #    - "UD-Q4_K_XL/*.gguf"

  #- repo_id: zai-org/GLM-4.5-Air-FP8

  #- repo_id: unsloth/GLM-4.6-GGUF
  #  include:
  #    - "UD-Q3_K_XL/*.gguf"
  #    #- "IQ4_XS/*.gguf"
  #    #- "UD-Q4_K_XL/*.gguf"
  #    #- "UD-Q5_K_XL/*.gguf"

  #- repo_id: unsloth/GLM-4.6-REAP-268B-A32B-GGUF
  #  include:
  #    - "UD-Q4_K_XL/*.gguf"
  #    - "UD-Q5_K_XL/*.gguf"

  #- repo_id: unsloth/Ministral-3-14B-Instruct-2512-GGUF
  #  include:
  #    - "Ministral-3-14B-Instruct-2512-UD-Q4_K_XL.gguf"

  #- repo_id: unsloth/Ministral-3-14B-Reasoning-2512-GGUF
  #  include:
  #    - "Ministral-3-14B-Reasoning-2512-UD-Q4_K_XL.gguf"

  - repo_id: unsloth/Qwen3-Next-80B-A3B-Thinking-GGUF
    include:
      - "Qwen3-Next-80B-A3B-Thinking-UD-Q4_K_XL.gguf"

  - repo_id: unsloth/Qwen3-VL-32B-Instruct-1M-GGUF
    include:
      - "Qwen3-VL-32B-Instruct-1M-UD-Q4_K_XL.gguf"
      - "mmproj-F16.gguf"

  #- repo_id: unsloth/Qwen3-VL-30B-A3B-Instruct-GGUF
  #  include:
  #    - "Qwen3-VL-30B-A3B-Instruct-UD-Q4_K_XL.gguf"
  #    - "mmproj-F16.gguf"

  - repo_id: unsloth/Qwen3-VL-8B-Instruct-1M-GGUF
    include:
      - "Qwen3-VL-8B-Instruct-1M-UD-Q4_K_XL.gguf"
      - "mmproj-F16.gguf"

  - repo_id: unsloth/Qwen3-VL-2B-Instruct-1M-GGUF
    include:
      - "Qwen3-VL-2B-Instruct-1M-UD-Q4_K_XL.gguf"
      - "mmproj-F16.gguf"

  - repo_id: unsloth/Qwen3-VL-2B-Thinking-1M-GGUF
    include:
      - "Qwen3-VL-2B-Thinking-1M-UD-Q4_K_XL.gguf"
      - "mmproj-F16.gguf"

  - repo_id: unsloth/Qwen3-0.6B-GGUF
    include:
      - "Qwen3-0.6B-UD-Q4_K_XL.gguf"
      - "Qwen3-0.6B-UD-Q6_K_XL.gguf"

  #- repo_id: unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF
  #  include:
  #    - "Qwen3-Coder-30B-A3B-Instruct-UD-Q4_K_XL.gguf"

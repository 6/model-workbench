# TensorRT-LLM config for Nemotron-3-Nano models (NemotronH architecture)
# Source: https://github.com/NVIDIA/TensorRT-LLM/blob/main/examples/auto_deploy/nano_v3.yaml
runtime: trtllm
compile_backend: torch-cudagraph
max_batch_size: 384
max_seq_len: 65536  # tunable
enable_chunked_prefill: true
attn_backend: flashinfer
model_factory: AutoModelForCausalLM
skip_loading_weights: false
# TODO: https://github.com/NVIDIA/TensorRT-LLM/issues/9884
free_mem_ratio: 0.88
cuda_graph_batch_sizes: [1, 2, 4, 8, 16, 24, 32, 64, 128, 256, 320, 384]
# Note: kv_cache_config.enable_block_reuse=False is set internally by trtllm-serve
# for hybrid/SSM models, so we don't specify it here to avoid dict/object mismatch
transforms:
  detect_sharding:
    allreduce_strategy: SYMM_MEM
    sharding_dims: ['ep', 'bmm']
  manual_config:
    head_dim: 128
    tp_plan:
      # mamba SSM layer
      "in_proj": "mamba"
      "out_proj": "rowwise"
      # attention layer
      "q_proj": "colwise"
      "k_proj": "colwise"
      "v_proj": "colwise"
      "o_proj": "rowwise"
      # moe layer: SHARED experts
      "up_proj": "colwise"
      "down_proj": "rowwise"
      # MoLE: latent projections
      "fc1_latent_proj": "gather"
      "fc2_latent_proj": "gather"
  multi_stream_moe:
    stage: compile
    enabled: true
  insert_cached_ssm_attention:
    cache_config:
      mamba_dtype: null
  gather_logits_before_lm_head:
    enabled: true
  fuse_mamba_a_log:
    stage: post_load_fusion
    enabled: true

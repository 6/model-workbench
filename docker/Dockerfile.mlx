# MLX CUDA Server Docker Image
# Build: docker build -f docker/Dockerfile.mlx --build-arg VERSION=0.30.1 -t model-bench-mlx:0.30.1 .
#
# VERSION: MLX package version (e.g., 0.30.1)
#
# NOTE: MLX CUDA only supports BF16/FP32 models (no quantization)
# NOTE: MLX CUDA only supports single-GPU (no tensor parallelism)

ARG CUDA_VERSION=12.8.0
FROM nvidia/cuda:${CUDA_VERSION}-runtime-ubuntu24.04

# Required: MLX version to install
ARG VERSION
RUN test -n "$VERSION" || (echo "ERROR: VERSION build-arg is required" && exit 1)

# Prevent interactive prompts
ENV DEBIAN_FRONTEND=noninteractive

# Install Python and dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    python3 \
    python3-pip \
    python3-venv \
    && rm -rf /var/lib/apt/lists/*

# Install MLX with CUDA 12 support and mlx-lm
RUN pip3 install --break-system-packages "mlx[cuda12]==${VERSION}" mlx-lm

# Expose default mlx_lm.server port
EXPOSE 8080

# Working directory for model mounts
WORKDIR /workspace

# Entry point: mlx_lm.server
ENTRYPOINT ["python3", "-m", "mlx_lm.server"]

# Default arguments (can be overridden)
CMD ["--host", "0.0.0.0", "--port", "8080"]

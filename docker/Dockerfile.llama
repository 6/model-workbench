# llama.cpp Server Docker Image
# Build with: docker build -f docker/Dockerfile.llama --build-arg VERSION=b7349 -t model-bench-llama:b7349 .
#
# VERSION can be:
#   - Release tag: b7349, b4521, b4500
#   - Commit SHA: abc123def (7+ hex chars)

ARG CUDA_VERSION=12.8.0
FROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu24.04

# Required: version to checkout (release tag or commit SHA)
ARG VERSION
RUN test -n "$VERSION" || (echo "ERROR: VERSION build-arg is required" && exit 1)

# Prevent interactive prompts
ENV DEBIAN_FRONTEND=noninteractive

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
    git \
    build-essential \
    cmake \
    ninja-build \
    ccache \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Clone llama.cpp repository
WORKDIR /opt
RUN git clone https://github.com/ggerganov/llama.cpp.git

# Checkout specific version
WORKDIR /opt/llama.cpp
RUN git checkout ${VERSION}

# Build llama.cpp with CUDA support
# Using Release build for optimal performance
RUN cmake -B build \
    -DGGML_CUDA=ON \
    -DCMAKE_BUILD_TYPE=Release \
    -DLLAMA_CURL=OFF \
    -G Ninja

# Use CUDA stubs for build-time linking (real libcuda.so.1 injected at runtime by nvidia-container-toolkit)
# Create libcuda.so.1 symlink that linker expects
RUN ln -s /usr/local/cuda/lib64/stubs/libcuda.so /usr/local/cuda/lib64/stubs/libcuda.so.1 && \
    ldconfig

RUN cmake --build build --config Release -j$(nproc)

# Verify llama-server was built
RUN test -f /opt/llama.cpp/build/bin/llama-server || \
    (echo "ERROR: llama-server not found after build" && exit 1)

# Create symlink for easier access
RUN ln -sf /opt/llama.cpp/build/bin/llama-server /usr/local/bin/llama-server

# Expose default port
EXPOSE 8080

# Set working directory
WORKDIR /workspace

# Default entrypoint: llama-server
ENTRYPOINT ["/opt/llama.cpp/build/bin/llama-server"]

# Default arguments (can be overridden)
CMD ["--host", "0.0.0.0", "--port", "8080"]
